[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "llm-clients"
version = "0.1.0"
description = "UNIX-style command line utilities for working with Large Language Models"
readme = "README.md"
requires-python = ">=3.8"
license = "MIT"
keywords = ["llm", "clients", "ai", "prompt", "tools", "unix"]
authors = [
    { name = "PsychArch", email = "psycharch@github.com" }
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Environment :: Console",
    "Topic :: Utilities",
]
dependencies = [
    "click>=8.0.0",
    "anthropic>=0.5.0",
    "rich>=13.0.0",
    "pyyaml>=6.0.0",
    "xmltodict>=0.13.0",  # For XML processing
    "aiohttp>=3.8.0",    # For async HTTP requests
    "google-genai",  # For Gemini models
    "openai>=1.0.0",  # For OpenAI and Deepseek models
    "tencentcloud-sdk-python>=3.0.1000",  # For Hunyuan models
    "tencentcloud-sdk-python-lkeap>=3.0.1000",  # For LKEAP models
    "tencentcloud-sdk-python-common>=3.0.1000"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.23.0",
]

[project.scripts]
llm = "llm_cli.cli:main"
llm-prompt = "llm_cli.prompt:main"
llm-query = "llm_cli.query:main"
llm-execute = "llm_cli.execute:main"
llm-md-render = "llm_cli.utils.markdown_renderer:main"

[tool.black]
line-length = 88
target-version = ["py38"]

[tool.isort]
profile = "black"
multi_line_output = 3

[tool.ruff]
select = ["E", "F", "I", "N", "W", "B"]
ignore = []
line-length = 88
target-version = "py38"

[tool.hatch.build.targets.wheel]
packages = ["llm_cli"]
