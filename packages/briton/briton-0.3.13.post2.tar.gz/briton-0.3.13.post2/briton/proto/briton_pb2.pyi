"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _DataType:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _DataTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    DT_INVALID: _DataType.ValueType  # 0
    DT_INT4: _DataType.ValueType  # 1
    DT_INT8: _DataType.ValueType  # 2
    DT_UINT8: _DataType.ValueType  # 3
    DT_INT32: _DataType.ValueType  # 4
    DT_INT64: _DataType.ValueType  # 5
    DT_FLOAT16: _DataType.ValueType  # 10
    """kHALF"""
    DT_BFLOAT16: _DataType.ValueType  # 11
    """kBF16"""
    DT_FLOAT32: _DataType.ValueType  # 12
    """kFLOAT"""
    DT_FP8: _DataType.ValueType  # 13
    DT_BOOL: _DataType.ValueType  # 20

class DataType(_DataType, metaclass=_DataTypeEnumTypeWrapper):
    """These correspond to nvinfer1::DataType"""

DT_INVALID: DataType.ValueType  # 0
DT_INT4: DataType.ValueType  # 1
DT_INT8: DataType.ValueType  # 2
DT_UINT8: DataType.ValueType  # 3
DT_INT32: DataType.ValueType  # 4
DT_INT64: DataType.ValueType  # 5
DT_FLOAT16: DataType.ValueType  # 10
"""kHALF"""
DT_BFLOAT16: DataType.ValueType  # 11
"""kBF16"""
DT_FLOAT32: DataType.ValueType  # 12
"""kFLOAT"""
DT_FP8: DataType.ValueType  # 13
DT_BOOL: DataType.ValueType  # 20
global___DataType = DataType

@typing.final
class Tensor(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class Shape(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        DIM_FIELD_NUMBER: builtins.int
        @property
        def dim(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
        def __init__(
            self,
            *,
            dim: collections.abc.Iterable[builtins.int] | None = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["dim", b"dim"]) -> None: ...

    SHAPE_FIELD_NUMBER: builtins.int
    DTYPE_FIELD_NUMBER: builtins.int
    DATA_FIELD_NUMBER: builtins.int
    dtype: global___DataType.ValueType
    data: builtins.bytes
    @property
    def shape(self) -> global___Tensor.Shape: ...
    def __init__(
        self,
        *,
        shape: global___Tensor.Shape | None = ...,
        dtype: global___DataType.ValueType = ...,
        data: builtins.bytes = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["shape", b"shape"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["data", b"data", "dtype", b"dtype", "shape", b"shape"]) -> None: ...

global___Tensor = Tensor

@typing.final
class Batch(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_IDS_FIELD_NUMBER: builtins.int
    @property
    def request_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    def __init__(
        self,
        *,
        request_ids: collections.abc.Iterable[builtins.int] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["request_ids", b"request_ids"]) -> None: ...

global___Batch = Batch

@typing.final
class InferenceRequest(google.protobuf.message.Message):
    """The request message containing the user's name."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_ID_FIELD_NUMBER: builtins.int
    INPUT_TEXT_FIELD_NUMBER: builtins.int
    INPUT_IDS_FIELD_NUMBER: builtins.int
    REQUEST_OUTPUT_LEN_FIELD_NUMBER: builtins.int
    END_ID_FIELD_NUMBER: builtins.int
    PAD_ID_FIELD_NUMBER: builtins.int
    BEAM_WIDTH_FIELD_NUMBER: builtins.int
    TEMPERATURE_FIELD_NUMBER: builtins.int
    RUNTIME_TOP_K_FIELD_NUMBER: builtins.int
    RUNTIME_TOP_P_FIELD_NUMBER: builtins.int
    LEN_PENALTY_FIELD_NUMBER: builtins.int
    REPETITION_PENALTY_FIELD_NUMBER: builtins.int
    PRESENCE_PENALTY_FIELD_NUMBER: builtins.int
    BAD_WORDS_FIELD_NUMBER: builtins.int
    STOP_WORDS_FIELD_NUMBER: builtins.int
    LORA_TASK_ID_FIELD_NUMBER: builtins.int
    LORA_WEIGHTS_FIELD_NUMBER: builtins.int
    LORA_CONFIG_FIELD_NUMBER: builtins.int
    RANDOM_SEED_FIELD_NUMBER: builtins.int
    DRAFT_INPUT_IDS_FIELD_NUMBER: builtins.int
    OUTPUT_SCHEMA_HASH_FIELD_NUMBER: builtins.int
    FSM_STATE_FIELD_NUMBER: builtins.int
    FORCE_TOOLS_FIELD_NUMBER: builtins.int
    WRITE_OUTPUT_STATES_FIELD_NUMBER: builtins.int
    BATCH_FIELD_NUMBER: builtins.int
    request_id: builtins.int
    input_text: builtins.str
    """One of input_text or input_ids should be provided.
    If input_ids is provided then output_ids are returned.
    """
    request_output_len: builtins.int
    end_id: builtins.int
    pad_id: builtins.int
    beam_width: builtins.int
    """Logit selection related"""
    temperature: builtins.float
    runtime_top_k: builtins.int
    runtime_top_p: builtins.float
    len_penalty: builtins.float
    repetition_penalty: builtins.float
    presence_penalty: builtins.float
    lora_task_id: builtins.int
    random_seed: builtins.int
    output_schema_hash: builtins.str
    """Structured output"""
    fsm_state: builtins.int
    force_tools: builtins.bool
    write_output_states: builtins.bool
    @property
    def input_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """TODO(pankaj) Implement this. Not supported now, input_text
        should be provided.
        """

    @property
    def bad_words(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    @property
    def stop_words(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]: ...
    @property
    def lora_weights(self) -> global___Tensor: ...
    @property
    def lora_config(self) -> global___Tensor: ...
    @property
    def draft_input_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def batch(self) -> global___Batch:
        """If request is part of batch, then try to wait for other requests"""

    def __init__(
        self,
        *,
        request_id: builtins.int = ...,
        input_text: builtins.str | None = ...,
        input_ids: collections.abc.Iterable[builtins.int] | None = ...,
        request_output_len: builtins.int | None = ...,
        end_id: builtins.int | None = ...,
        pad_id: builtins.int | None = ...,
        beam_width: builtins.int | None = ...,
        temperature: builtins.float | None = ...,
        runtime_top_k: builtins.int | None = ...,
        runtime_top_p: builtins.float | None = ...,
        len_penalty: builtins.float | None = ...,
        repetition_penalty: builtins.float | None = ...,
        presence_penalty: builtins.float | None = ...,
        bad_words: collections.abc.Iterable[builtins.str] | None = ...,
        stop_words: collections.abc.Iterable[builtins.str] | None = ...,
        lora_task_id: builtins.int | None = ...,
        lora_weights: global___Tensor | None = ...,
        lora_config: global___Tensor | None = ...,
        random_seed: builtins.int | None = ...,
        draft_input_ids: collections.abc.Iterable[builtins.int] | None = ...,
        output_schema_hash: builtins.str | None = ...,
        fsm_state: builtins.int | None = ...,
        force_tools: builtins.bool | None = ...,
        write_output_states: builtins.bool | None = ...,
        batch: global___Batch | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_batch", b"_batch", "_beam_width", b"_beam_width", "_end_id", b"_end_id", "_force_tools", b"_force_tools", "_fsm_state", b"_fsm_state", "_input_text", b"_input_text", "_len_penalty", b"_len_penalty", "_lora_config", b"_lora_config", "_lora_task_id", b"_lora_task_id", "_lora_weights", b"_lora_weights", "_output_schema_hash", b"_output_schema_hash", "_pad_id", b"_pad_id", "_presence_penalty", b"_presence_penalty", "_random_seed", b"_random_seed", "_repetition_penalty", b"_repetition_penalty", "_request_output_len", b"_request_output_len", "_runtime_top_k", b"_runtime_top_k", "_runtime_top_p", b"_runtime_top_p", "_temperature", b"_temperature", "_write_output_states", b"_write_output_states", "batch", b"batch", "beam_width", b"beam_width", "end_id", b"end_id", "force_tools", b"force_tools", "fsm_state", b"fsm_state", "input_text", b"input_text", "len_penalty", b"len_penalty", "lora_config", b"lora_config", "lora_task_id", b"lora_task_id", "lora_weights", b"lora_weights", "output_schema_hash", b"output_schema_hash", "pad_id", b"pad_id", "presence_penalty", b"presence_penalty", "random_seed", b"random_seed", "repetition_penalty", b"repetition_penalty", "request_output_len", b"request_output_len", "runtime_top_k", b"runtime_top_k", "runtime_top_p", b"runtime_top_p", "temperature", b"temperature", "write_output_states", b"write_output_states"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_batch", b"_batch", "_beam_width", b"_beam_width", "_end_id", b"_end_id", "_force_tools", b"_force_tools", "_fsm_state", b"_fsm_state", "_input_text", b"_input_text", "_len_penalty", b"_len_penalty", "_lora_config", b"_lora_config", "_lora_task_id", b"_lora_task_id", "_lora_weights", b"_lora_weights", "_output_schema_hash", b"_output_schema_hash", "_pad_id", b"_pad_id", "_presence_penalty", b"_presence_penalty", "_random_seed", b"_random_seed", "_repetition_penalty", b"_repetition_penalty", "_request_output_len", b"_request_output_len", "_runtime_top_k", b"_runtime_top_k", "_runtime_top_p", b"_runtime_top_p", "_temperature", b"_temperature", "_write_output_states", b"_write_output_states", "bad_words", b"bad_words", "batch", b"batch", "beam_width", b"beam_width", "draft_input_ids", b"draft_input_ids", "end_id", b"end_id", "force_tools", b"force_tools", "fsm_state", b"fsm_state", "input_ids", b"input_ids", "input_text", b"input_text", "len_penalty", b"len_penalty", "lora_config", b"lora_config", "lora_task_id", b"lora_task_id", "lora_weights", b"lora_weights", "output_schema_hash", b"output_schema_hash", "pad_id", b"pad_id", "presence_penalty", b"presence_penalty", "random_seed", b"random_seed", "repetition_penalty", b"repetition_penalty", "request_id", b"request_id", "request_output_len", b"request_output_len", "runtime_top_k", b"runtime_top_k", "runtime_top_p", b"runtime_top_p", "stop_words", b"stop_words", "temperature", b"temperature", "write_output_states", b"write_output_states"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_batch", b"_batch"]) -> typing.Literal["batch"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_beam_width", b"_beam_width"]) -> typing.Literal["beam_width"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_end_id", b"_end_id"]) -> typing.Literal["end_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_force_tools", b"_force_tools"]) -> typing.Literal["force_tools"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_fsm_state", b"_fsm_state"]) -> typing.Literal["fsm_state"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_input_text", b"_input_text"]) -> typing.Literal["input_text"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_len_penalty", b"_len_penalty"]) -> typing.Literal["len_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_config", b"_lora_config"]) -> typing.Literal["lora_config"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_task_id", b"_lora_task_id"]) -> typing.Literal["lora_task_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_weights", b"_lora_weights"]) -> typing.Literal["lora_weights"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_output_schema_hash", b"_output_schema_hash"]) -> typing.Literal["output_schema_hash"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_pad_id", b"_pad_id"]) -> typing.Literal["pad_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_presence_penalty", b"_presence_penalty"]) -> typing.Literal["presence_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_random_seed", b"_random_seed"]) -> typing.Literal["random_seed"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_repetition_penalty", b"_repetition_penalty"]) -> typing.Literal["repetition_penalty"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_request_output_len", b"_request_output_len"]) -> typing.Literal["request_output_len"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_runtime_top_k", b"_runtime_top_k"]) -> typing.Literal["runtime_top_k"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_runtime_top_p", b"_runtime_top_p"]) -> typing.Literal["runtime_top_p"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_temperature", b"_temperature"]) -> typing.Literal["temperature"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_write_output_states", b"_write_output_states"]) -> typing.Literal["write_output_states"] | None: ...

global___InferenceRequest = InferenceRequest

@typing.final
class InferenceAnswerPart(google.protobuf.message.Message):
    """The response message containing the greetings."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    REQUEST_ID_FIELD_NUMBER: builtins.int
    OUTPUT_TEXT_FIELD_NUMBER: builtins.int
    OUTPUT_IDS_FIELD_NUMBER: builtins.int
    OUTPUT_STATES_FIELD_NUMBER: builtins.int
    request_id: builtins.int
    output_text: builtins.str
    @property
    def output_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    @property
    def output_states(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
    def __init__(
        self,
        *,
        request_id: builtins.int = ...,
        output_text: builtins.str = ...,
        output_ids: collections.abc.Iterable[builtins.int] | None = ...,
        output_states: collections.abc.Iterable[builtins.int] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["output_ids", b"output_ids", "output_states", b"output_states", "output_text", b"output_text", "request_id", b"request_id"]) -> None: ...

global___InferenceAnswerPart = InferenceAnswerPart

@typing.final
class AddedToken(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    CONTENT_FIELD_NUMBER: builtins.int
    SINGLE_WORD_FIELD_NUMBER: builtins.int
    LSTRIP_FIELD_NUMBER: builtins.int
    RSTRIP_FIELD_NUMBER: builtins.int
    NORMALIZED_FIELD_NUMBER: builtins.int
    SPECIAL_FIELD_NUMBER: builtins.int
    content: builtins.str
    single_word: builtins.bool
    lstrip: builtins.bool
    rstrip: builtins.bool
    normalized: builtins.bool
    special: builtins.bool
    def __init__(
        self,
        *,
        content: builtins.str | None = ...,
        single_word: builtins.bool | None = ...,
        lstrip: builtins.bool | None = ...,
        rstrip: builtins.bool | None = ...,
        normalized: builtins.bool | None = ...,
        special: builtins.bool | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_content", b"_content", "_lstrip", b"_lstrip", "_normalized", b"_normalized", "_rstrip", b"_rstrip", "_single_word", b"_single_word", "_special", b"_special", "content", b"content", "lstrip", b"lstrip", "normalized", b"normalized", "rstrip", b"rstrip", "single_word", b"single_word", "special", b"special"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_content", b"_content", "_lstrip", b"_lstrip", "_normalized", b"_normalized", "_rstrip", b"_rstrip", "_single_word", b"_single_word", "_special", b"_special", "content", b"content", "lstrip", b"lstrip", "normalized", b"normalized", "rstrip", b"rstrip", "single_word", b"single_word", "special", b"special"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_content", b"_content"]) -> typing.Literal["content"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lstrip", b"_lstrip"]) -> typing.Literal["lstrip"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_normalized", b"_normalized"]) -> typing.Literal["normalized"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_rstrip", b"_rstrip"]) -> typing.Literal["rstrip"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_single_word", b"_single_word"]) -> typing.Literal["single_word"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_special", b"_special"]) -> typing.Literal["special"] | None: ...

global___AddedToken = AddedToken

@typing.final
class AddedTokens(google.protobuf.message.Message):
    """These mirror the added tokens concept in HF PreTrainedTokenizerFast. Most
    tokens are defined in tokenizer.json and are automatically picked up by the
    rust tokenizer. But those defined outside, as in tokenizer_config.json and
    special_tokens_map.json, need to be added manually. Those additional tokens
    are supplied at startup here.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    TOKENS_FIELD_NUMBER: builtins.int
    @property
    def tokens(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___AddedToken]: ...
    def __init__(
        self,
        *,
        tokens: collections.abc.Iterable[global___AddedToken] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["tokens", b"tokens"]) -> None: ...

global___AddedTokens = AddedTokens

@typing.final
class BritonConfig(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _BatchSchedulerPolicy:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _BatchSchedulerPolicyEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[BritonConfig._BatchSchedulerPolicy.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        MAX_UTILIZATION: BritonConfig._BatchSchedulerPolicy.ValueType  # 0
        GUARANTEED_NO_EVICT: BritonConfig._BatchSchedulerPolicy.ValueType  # 1

    class BatchSchedulerPolicy(_BatchSchedulerPolicy, metaclass=_BatchSchedulerPolicyEnumTypeWrapper): ...
    MAX_UTILIZATION: BritonConfig.BatchSchedulerPolicy.ValueType  # 0
    GUARANTEED_NO_EVICT: BritonConfig.BatchSchedulerPolicy.ValueType  # 1

    ENGINE_PATH_FIELD_NUMBER: builtins.int
    HF_TOKENIZER_FIELD_NUMBER: builtins.int
    PORT_FIELD_NUMBER: builtins.int
    BATCH_SCHEDULER_POLICY_FIELD_NUMBER: builtins.int
    ENABLE_TRT_OVERLAP_FIELD_NUMBER: builtins.int
    MAX_TOKENS_IN_PAGED_KV_CACHE_FIELD_NUMBER: builtins.int
    KV_CACHE_FREE_GPU_MEM_FRACTION_FIELD_NUMBER: builtins.int
    MEDUSA_DECODING_MODE_FIELD_NUMBER: builtins.int
    ENABLE_CHUNKED_CONTEXT_FIELD_NUMBER: builtins.int
    ENABLE_KV_CACHE_REUSE_FIELD_NUMBER: builtins.int
    KV_CACHE_HOST_MEMORY_BYTES_FIELD_NUMBER: builtins.int
    LORA_CACHE_MAX_ADAPTER_SIZE_FIELD_NUMBER: builtins.int
    LORA_CACHE_OPTIMAL_ADAPTER_SIZE_FIELD_NUMBER: builtins.int
    LORA_CACHE_GPU_MEMORY_FRACTION_FIELD_NUMBER: builtins.int
    LORA_CACHE_HOST_MEMORY_BYTES_FIELD_NUMBER: builtins.int
    FSM_CACHE_DIR_FIELD_NUMBER: builtins.int
    MAX_BATCH_WAIT_MS_FIELD_NUMBER: builtins.int
    ADDED_TOKENS_FIELD_NUMBER: builtins.int
    MAX_NUM_TOKENS_FIELD_NUMBER: builtins.int
    engine_path: builtins.str
    hf_tokenizer: builtins.str
    port: builtins.int
    batch_scheduler_policy: global___BritonConfig.BatchSchedulerPolicy.ValueType
    enable_trt_overlap: builtins.bool
    max_tokens_in_paged_kv_cache: builtins.int
    kv_cache_free_gpu_mem_fraction: builtins.float
    medusa_decoding_mode: builtins.bool
    enable_chunked_context: builtins.bool
    enable_kv_cache_reuse: builtins.bool
    kv_cache_host_memory_bytes: builtins.int
    lora_cache_max_adapter_size: builtins.int
    lora_cache_optimal_adapter_size: builtins.int
    lora_cache_gpu_memory_fraction: builtins.float
    lora_cache_host_memory_bytes: builtins.int
    fsm_cache_dir: builtins.str
    """Path to cache dir with StatesToTokens"""
    max_batch_wait_ms: builtins.int
    """TODO(pankaj) This field is a placeholder for future use. Currently it is
    not used.

    Wait up to this time for all batch requests to arrive. Normally requests
    should arrive pretty quickly, this is to tackle exceptional cases. So this
    value can be relatively large, say a few ms.
    """
    max_num_tokens: builtins.int
    @property
    def added_tokens(self) -> global___AddedTokens:
        """Added tokens to the tokenizer"""

    def __init__(
        self,
        *,
        engine_path: builtins.str = ...,
        hf_tokenizer: builtins.str = ...,
        port: builtins.int | None = ...,
        batch_scheduler_policy: global___BritonConfig.BatchSchedulerPolicy.ValueType | None = ...,
        enable_trt_overlap: builtins.bool | None = ...,
        max_tokens_in_paged_kv_cache: builtins.int | None = ...,
        kv_cache_free_gpu_mem_fraction: builtins.float | None = ...,
        medusa_decoding_mode: builtins.bool | None = ...,
        enable_chunked_context: builtins.bool | None = ...,
        enable_kv_cache_reuse: builtins.bool | None = ...,
        kv_cache_host_memory_bytes: builtins.int | None = ...,
        lora_cache_max_adapter_size: builtins.int | None = ...,
        lora_cache_optimal_adapter_size: builtins.int | None = ...,
        lora_cache_gpu_memory_fraction: builtins.float | None = ...,
        lora_cache_host_memory_bytes: builtins.int | None = ...,
        fsm_cache_dir: builtins.str | None = ...,
        max_batch_wait_ms: builtins.int | None = ...,
        added_tokens: global___AddedTokens | None = ...,
        max_num_tokens: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_added_tokens", b"_added_tokens", "_batch_scheduler_policy", b"_batch_scheduler_policy", "_enable_chunked_context", b"_enable_chunked_context", "_enable_kv_cache_reuse", b"_enable_kv_cache_reuse", "_enable_trt_overlap", b"_enable_trt_overlap", "_fsm_cache_dir", b"_fsm_cache_dir", "_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction", "_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes", "_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction", "_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes", "_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size", "_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size", "_max_batch_wait_ms", b"_max_batch_wait_ms", "_max_num_tokens", b"_max_num_tokens", "_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache", "_medusa_decoding_mode", b"_medusa_decoding_mode", "_port", b"_port", "added_tokens", b"added_tokens", "batch_scheduler_policy", b"batch_scheduler_policy", "enable_chunked_context", b"enable_chunked_context", "enable_kv_cache_reuse", b"enable_kv_cache_reuse", "enable_trt_overlap", b"enable_trt_overlap", "fsm_cache_dir", b"fsm_cache_dir", "kv_cache_free_gpu_mem_fraction", b"kv_cache_free_gpu_mem_fraction", "kv_cache_host_memory_bytes", b"kv_cache_host_memory_bytes", "lora_cache_gpu_memory_fraction", b"lora_cache_gpu_memory_fraction", "lora_cache_host_memory_bytes", b"lora_cache_host_memory_bytes", "lora_cache_max_adapter_size", b"lora_cache_max_adapter_size", "lora_cache_optimal_adapter_size", b"lora_cache_optimal_adapter_size", "max_batch_wait_ms", b"max_batch_wait_ms", "max_num_tokens", b"max_num_tokens", "max_tokens_in_paged_kv_cache", b"max_tokens_in_paged_kv_cache", "medusa_decoding_mode", b"medusa_decoding_mode", "port", b"port"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_added_tokens", b"_added_tokens", "_batch_scheduler_policy", b"_batch_scheduler_policy", "_enable_chunked_context", b"_enable_chunked_context", "_enable_kv_cache_reuse", b"_enable_kv_cache_reuse", "_enable_trt_overlap", b"_enable_trt_overlap", "_fsm_cache_dir", b"_fsm_cache_dir", "_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction", "_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes", "_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction", "_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes", "_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size", "_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size", "_max_batch_wait_ms", b"_max_batch_wait_ms", "_max_num_tokens", b"_max_num_tokens", "_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache", "_medusa_decoding_mode", b"_medusa_decoding_mode", "_port", b"_port", "added_tokens", b"added_tokens", "batch_scheduler_policy", b"batch_scheduler_policy", "enable_chunked_context", b"enable_chunked_context", "enable_kv_cache_reuse", b"enable_kv_cache_reuse", "enable_trt_overlap", b"enable_trt_overlap", "engine_path", b"engine_path", "fsm_cache_dir", b"fsm_cache_dir", "hf_tokenizer", b"hf_tokenizer", "kv_cache_free_gpu_mem_fraction", b"kv_cache_free_gpu_mem_fraction", "kv_cache_host_memory_bytes", b"kv_cache_host_memory_bytes", "lora_cache_gpu_memory_fraction", b"lora_cache_gpu_memory_fraction", "lora_cache_host_memory_bytes", b"lora_cache_host_memory_bytes", "lora_cache_max_adapter_size", b"lora_cache_max_adapter_size", "lora_cache_optimal_adapter_size", b"lora_cache_optimal_adapter_size", "max_batch_wait_ms", b"max_batch_wait_ms", "max_num_tokens", b"max_num_tokens", "max_tokens_in_paged_kv_cache", b"max_tokens_in_paged_kv_cache", "medusa_decoding_mode", b"medusa_decoding_mode", "port", b"port"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_added_tokens", b"_added_tokens"]) -> typing.Literal["added_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_batch_scheduler_policy", b"_batch_scheduler_policy"]) -> typing.Literal["batch_scheduler_policy"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_chunked_context", b"_enable_chunked_context"]) -> typing.Literal["enable_chunked_context"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_kv_cache_reuse", b"_enable_kv_cache_reuse"]) -> typing.Literal["enable_kv_cache_reuse"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_enable_trt_overlap", b"_enable_trt_overlap"]) -> typing.Literal["enable_trt_overlap"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_fsm_cache_dir", b"_fsm_cache_dir"]) -> typing.Literal["fsm_cache_dir"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_kv_cache_free_gpu_mem_fraction", b"_kv_cache_free_gpu_mem_fraction"]) -> typing.Literal["kv_cache_free_gpu_mem_fraction"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_kv_cache_host_memory_bytes", b"_kv_cache_host_memory_bytes"]) -> typing.Literal["kv_cache_host_memory_bytes"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_gpu_memory_fraction", b"_lora_cache_gpu_memory_fraction"]) -> typing.Literal["lora_cache_gpu_memory_fraction"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_host_memory_bytes", b"_lora_cache_host_memory_bytes"]) -> typing.Literal["lora_cache_host_memory_bytes"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_max_adapter_size", b"_lora_cache_max_adapter_size"]) -> typing.Literal["lora_cache_max_adapter_size"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_lora_cache_optimal_adapter_size", b"_lora_cache_optimal_adapter_size"]) -> typing.Literal["lora_cache_optimal_adapter_size"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_batch_wait_ms", b"_max_batch_wait_ms"]) -> typing.Literal["max_batch_wait_ms"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_num_tokens", b"_max_num_tokens"]) -> typing.Literal["max_num_tokens"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_max_tokens_in_paged_kv_cache", b"_max_tokens_in_paged_kv_cache"]) -> typing.Literal["max_tokens_in_paged_kv_cache"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_medusa_decoding_mode", b"_medusa_decoding_mode"]) -> typing.Literal["medusa_decoding_mode"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing.Literal["_port", b"_port"]) -> typing.Literal["port"] | None: ...

global___BritonConfig = BritonConfig

@typing.final
class TokenToNextState(google.protobuf.message.Message):
    """FSM"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class TokenToNextStateEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.int
        value: builtins.int
        def __init__(
            self,
            *,
            key: builtins.int = ...,
            value: builtins.int = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["key", b"key", "value", b"value"]) -> None: ...

    TOKEN_TO_NEXT_STATE_FIELD_NUMBER: builtins.int
    @property
    def token_to_next_state(self) -> google.protobuf.internal.containers.ScalarMap[builtins.int, builtins.int]: ...
    def __init__(
        self,
        *,
        token_to_next_state: collections.abc.Mapping[builtins.int, builtins.int] | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["token_to_next_state", b"token_to_next_state"]) -> None: ...

global___TokenToNextState = TokenToNextState

@typing.final
class StatesToTokens(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    @typing.final
    class StatesToTokensEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.int
        @property
        def value(self) -> global___TokenToNextState: ...
        def __init__(
            self,
            *,
            key: builtins.int = ...,
            value: global___TokenToNextState | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["value", b"value"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["key", b"key", "value", b"value"]) -> None: ...

    STATES_TO_TOKENS_FIELD_NUMBER: builtins.int
    VOCAB_SIZE_FIELD_NUMBER: builtins.int
    EOS_TOKEN_ID_FIELD_NUMBER: builtins.int
    TOOLS_ID_FIELD_NUMBER: builtins.int
    vocab_size: builtins.int
    eos_token_id: builtins.int
    tools_id: builtins.int
    @property
    def states_to_tokens(self) -> google.protobuf.internal.containers.MessageMap[builtins.int, global___TokenToNextState]: ...
    def __init__(
        self,
        *,
        states_to_tokens: collections.abc.Mapping[builtins.int, global___TokenToNextState] | None = ...,
        vocab_size: builtins.int = ...,
        eos_token_id: builtins.int = ...,
        tools_id: builtins.int | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["_tools_id", b"_tools_id", "tools_id", b"tools_id"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["_tools_id", b"_tools_id", "eos_token_id", b"eos_token_id", "states_to_tokens", b"states_to_tokens", "tools_id", b"tools_id", "vocab_size", b"vocab_size"]) -> None: ...
    def WhichOneof(self, oneof_group: typing.Literal["_tools_id", b"_tools_id"]) -> typing.Literal["tools_id"] | None: ...

global___StatesToTokens = StatesToTokens
