{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import random\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "\n",
    "df = pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk1.csv')\n",
    "df = rt.columnsAreTimestamps(df, 'parsedDate')\n",
    "df = df.rename({'TimeSeconds':                '_del1_',\n",
    "                'parsedDate':                 'timestamp',\n",
    "                'dateTimeStr':                '_del2_',\n",
    "                'ipLayerProtocol':            'pro',\n",
    "                'ipLayerProtocolCode':        '_del3_',\n",
    "                'firstSeenSrcIp':             'sip',\n",
    "                'firstSeenDestIp':            'dip',\n",
    "                'firstSeenSrcPort':           'spt',\n",
    "                'firstSeenDestPort':          'dpt',\n",
    "                'moreFragments':              '_del4_',\n",
    "                'contFragments':              '_del5_',\n",
    "                'durationSeconds':            'dur',\n",
    "                'firstSeenSrcPayloadBytes':   '_del6_',\n",
    "                'firstSeenDestPayloadBytes':  '_del7_',\n",
    "                'firstSeenSrcTotalBytes':     'soct',\n",
    "                'firstSeenDestTotalBytes':    'doct',\n",
    "                'firstSeenSrcPacketCount':    'spkt',\n",
    "                'firstSeenDestPacketCount':   'dpkt',\n",
    "                'recordForceOut':             '_del8_'})\n",
    "df = df.drop(['_del1_', '_del2_', '_del3_', '_del4_', '_del5_', '_del6_', '_del7_', '_del8_'])\n",
    "# df = df.sample(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# spreadLines() - attempt to implement this visualization\n",
    "#\n",
    "# Based on:\n",
    "#\n",
    "# @misc{kuo2024spreadlinevisualizingegocentricdynamic,\n",
    "#       title={SpreadLine: Visualizing Egocentric Dynamic Influence}, \n",
    "#       author={Yun-Hsin Kuo and Dongyu Liu and Kwan-Liu Ma},\n",
    "#       year={2024},\n",
    "#       eprint={2408.08992},\n",
    "#       archivePrefix={arXiv},\n",
    "#       primaryClass={cs.HC},\n",
    "#       url={https://arxiv.org/abs/2408.08992}, \n",
    "# }\n",
    "# \n",
    "def spreadLines(rt_self,\n",
    "                df,\n",
    "                relationships,\n",
    "                node_focus,\n",
    "                ts_field        = None,  # Will attempt to guess based on datatypes\n",
    "                every           = '1d',  # \"the every field for the group_by_dynamic\" ... 1d, 1h, 1m\n",
    "                color_by        = None,\n",
    "                count_by        = None,\n",
    "                count_by_set    = False,\n",
    "                widget_id       = None,\n",
    "                w               = 1024,\n",
    "                h               = 512,\n",
    "                x_ins           = 32,\n",
    "                y_ins           = 8,\n",
    "                txt_h           = 12):\n",
    "    if rt_self.isPolars(df) == False: raise Exception('spreadLines() - only supports polars dataframe')\n",
    "    return SpreadLines(**locals())\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class SpreadLines(object):\n",
    "    #\n",
    "    # transform all fields (if they area t-field)\n",
    "    # - replace those fields w/ the new versions (i actually don't think the names change...)\n",
    "    #\n",
    "    def __transformFields__(self):\n",
    "        # Gather up all of the fields that are going to be used\n",
    "        _all_columns_ = [self.ts_field]\n",
    "        if self.color_by is not None: _all_columns_.append(self.color_by)\n",
    "        if self.count_by is not None: _all_columns_.append(self.count_by)\n",
    "        for _relationship_ in self.relationships:\n",
    "            _fm_, _to_ = _relationship_[0], _relationship_[1]\n",
    "            if   type(_fm_) is str: _all_columns_.append(_fm_)\n",
    "            elif type(_fm_) is tuple:\n",
    "                for i in range(len(_fm_)): _all_columns_.append(_fm_[i])\n",
    "            if   type(_to_) is str: _all_columns_.append(_to_)\n",
    "            elif type(_to_) is tuple:\n",
    "                for i in range(len(_to_)): _all_columns_.append(_to_[i])\n",
    "        # Transform the fields\n",
    "        self.df, _new_columns_ = self.rt_self.transformFieldListAndDataFrame(self.df, _all_columns_)\n",
    "        # Remap them\n",
    "        col_i = 0\n",
    "        self.ts_field        = _new_columns_[col_i]\n",
    "        col_i += 1\n",
    "        if self.color_by is not None: \n",
    "            self.color_by = _new_columns_[col_i]\n",
    "            col_i += 1\n",
    "        if self.count_by is not None:\n",
    "            self.count_by = _new_columns_[col_i]\n",
    "            col_i += 1\n",
    "        _new_relationships_ = []\n",
    "        for _relationship_ in self.relationships:\n",
    "            _fm_, _to_ = _relationship_[0], _relationship_[1]\n",
    "            if   type(_fm_) is str: \n",
    "                _fm_ = _new_columns_[col_i]\n",
    "                col_i += 1\n",
    "            elif type(_fm_) is tuple:\n",
    "                as_list = []\n",
    "                for i in range(len(_fm_)):\n",
    "                    as_list.append(_new_columns_[col_i])                    \n",
    "                    col_i += 1\n",
    "                _fm_ = tuple(as_list)\n",
    "            if   type(_to_) is str: \n",
    "                _to_ = _new_columns_[col_i]\n",
    "                col_i += 1\n",
    "            elif type(_to_) is tuple:\n",
    "                as_list = []\n",
    "                for i in range(len(_to_)): \n",
    "                    as_list.append(_new_columns_[col_i])\n",
    "                    col_i += 1\n",
    "                _to_ = tuple(as_list)\n",
    "            _new_relationships_.append((_fm_, _to_))\n",
    "        self.relationships = _new_relationships_\n",
    "\n",
    "\n",
    "    #\n",
    "    # __consolidateRelationships__() - simplify the relationship fields into a single field\n",
    "    # ... and use standard naming\n",
    "    # ... replaces the \"relationships\" field w/ the consolidated field names\n",
    "    # ... use (__fm0__, __to0__),( __fm1__, __to1__), etc.\n",
    "    #\n",
    "    def __consolidateRelationships__(self):\n",
    "        new_relationships = []\n",
    "        for i in range(len(self.relationships)):\n",
    "            _fm_, _to_ = self.relationships[i]\n",
    "            new_fm = f'__fm{i}__'\n",
    "            new_to = f'__to{i}__'\n",
    "            if type(_fm_) is str: self.df = self.df.with_columns(pl.col(_fm_).alias(new_fm))\n",
    "            else:                 self.df = self.rt_self.createConcatColumn(self.df, _fm_, new_fm)\n",
    "            if type(_to_) is str: self.df = self.df.with_columns(pl.col(_to_).alias(new_to))\n",
    "            else:                 self.df = self.rt_self.createConcatColumn(self.df, _to_, new_to)\n",
    "            new_relationships.append((new_fm, new_to))\n",
    "        self.relationships = new_relationships\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    def __init__(self, rt_self, **kwargs):\n",
    "        self.rt_self       = rt_self\n",
    "        self.df            = rt_self.copyDataFrame(kwargs['df'])\n",
    "        self.relationships = kwargs['relationships']\n",
    "        self.node_focus    = kwargs['node_focus']\n",
    "        self.ts_field      = self.rt_self.guessTimestampField(self.df) if kwargs['ts_field'] is None else kwargs['ts_field']\n",
    "        self.every         = kwargs['every']\n",
    "        self.color_by      = kwargs['color_by']\n",
    "        self.count_by      = kwargs['count_by']\n",
    "        self.count_by_set  = kwargs['count_by_set']\n",
    "        self.widget_id     = f'spreadlines_{random.randint(0,65535)}' if kwargs['widget_id'] is None else kwargs['widget_id']\n",
    "        self.w             = kwargs['w']\n",
    "        self.h             = kwargs['h']\n",
    "        self.x_ins         = kwargs['x_ins']\n",
    "        self.y_ins         = kwargs['y_ins']\n",
    "        self.txt_h         = kwargs['txt_h']\n",
    "        # Unwrap any fields w/ the appropriate transforms\n",
    "        self.__transformFields__()\n",
    "        # Consolidate the fm's and to's into a simple field (__fm0__, __to0__),( __fm1__, __to1__), etc.\n",
    "        self.__consolidateRelationships__()\n",
    "        # How many bins?  And what's in those bins for nodes next to the focus?\n",
    "        self.df = self.df.sort(self.ts_field)\n",
    "        _bin_                    = 0\n",
    "        _dfs_containing_focus_   = [] # focus  -> alter1 or alter1 -> focus\n",
    "        _dfs_containing_alter2s_ = [] # alter1 -> alter2 or alter2 -> alter1  ... note does not include focus or alter1 <-> alter1\n",
    "        self.bin_to_timestamps   = {}\n",
    "        self.bin_to_alter1s      = {}\n",
    "        self.bin_to_alter2s      = {}\n",
    "        for k, k_df in self.df.group_by_dynamic(self.ts_field, every=self.every):\n",
    "            _timestamp_     = k[0]\n",
    "            _found_matches_ = False\n",
    "            # find the first alters\n",
    "            for i in range(len(self.relationships)):\n",
    "                _fm_, _to_ = self.relationships[i]\n",
    "                \n",
    "                # From Is Focus\n",
    "                _df_fm_is_focus_ = k_df.filter(pl.col(_fm_) == self.node_focus)\n",
    "                _df_fm_is_focus_ = _df_fm_is_focus_.with_columns(pl.lit(_fm_).alias('__focus_col__'), pl.lit(_to_).alias('__alter_col__'), pl.lit(1).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                if len(_df_fm_is_focus_) > 0: \n",
    "                    _dfs_containing_focus_.append(_df_fm_is_focus_)\n",
    "                    if _bin_ not in self.bin_to_alter1s:        self.bin_to_alter1s[_bin_]       = {}\n",
    "                    if 'to'  not in self.bin_to_alter1s[_bin_]: self.bin_to_alter1s[_bin_]['to'] = set()\n",
    "                    self.bin_to_alter1s[_bin_]['to'] |= set(_df_fm_is_focus_[_to_])\n",
    "                    _found_matches_ = True\n",
    "\n",
    "                # To Is Focus\n",
    "                _df_to_is_focus_ = k_df.filter(pl.col(_to_) == self.node_focus)\n",
    "                _df_to_is_focus_ = _df_to_is_focus_.with_columns(pl.lit(_to_).alias('__focus_col__'), pl.lit(_fm_).alias('__alter_col__'), pl.lit(1).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                if len(_df_to_is_focus_) > 0:\n",
    "                    _dfs_containing_focus_.append(_df_to_is_focus_)\n",
    "                    if _bin_ not in self.bin_to_alter1s:        self.bin_to_alter1s[_bin_]       = {}\n",
    "                    if 'fm'  not in self.bin_to_alter1s[_bin_]: self.bin_to_alter1s[_bin_]['fm'] = set()\n",
    "                    self.bin_to_alter1s[_bin_]['fm'] |= set(_df_to_is_focus_[_fm_])\n",
    "                    _found_matches_ = True\n",
    "\n",
    "                # For any shared nodes between the two sides, keep them on the 'fm' side\n",
    "                if _bin_ in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[_bin_] and 'to' in self.bin_to_alter1s[_bin_]:\n",
    "                    _shared_nodes_ = self.bin_to_alter1s[_bin_]['fm'] & self.bin_to_alter1s[_bin_]['to']\n",
    "                    if len(_shared_nodes_) > 0: self.bin_to_alter1s[_bin_]['to'] -= _shared_nodes_\n",
    "\n",
    "            # find the second alters\n",
    "            if _found_matches_:\n",
    "                _all_alter1s_ = set()\n",
    "                if 'fm' in self.bin_to_alter1s[_bin_]: _all_alter1s_ |= self.bin_to_alter1s[_bin_]['fm']\n",
    "                if 'to' in self.bin_to_alter1s[_bin_]: _all_alter1s_ |= self.bin_to_alter1s[_bin_]['to']\n",
    "                # Go through all the relationships\n",
    "                for i in range(len(self.relationships)):\n",
    "                    _fm_, _to_ = self.relationships[i]\n",
    "                    if 'fm' in self.bin_to_alter1s[_bin_]:\n",
    "                        _df_          = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['fm']) | pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['fm']))\n",
    "                        _set_alter2s_ = (set(_df_[_fm_]) | set(_df_[_to_])) - (_all_alter1s_ | set([self.node_focus]))\n",
    "                        if len(_set_alter2s_) > 0:\n",
    "                            if _bin_ not in self.bin_to_alter2s:        self.bin_to_alter2s[_bin_]       = {}\n",
    "                            if 'fm'  not in self.bin_to_alter2s[_bin_]: self.bin_to_alter2s[_bin_]['fm'] = set()\n",
    "                            self.bin_to_alter2s[_bin_]['fm'] |= _set_alter2s_\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['fm']) & pl.col(_to_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_fm_).alias('__alter1_col__'), pl.lit(_to_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['fm']) & pl.col(_fm_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_to_).alias('__alter1_col__'), pl.lit(_fm_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('fm').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                    if 'to' in self.bin_to_alter1s[_bin_]:\n",
    "                        _df_          = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['to']) | pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['to']))\n",
    "                        _set_alter2s_ = (set(_df_[_fm_]) | set(_df_[_to_])) - (_all_alter1s_ | set([self.node_focus]))\n",
    "                        if len(_set_alter2s_) > 0:\n",
    "                            if _bin_ not in self.bin_to_alter2s:        self.bin_to_alter2s[_bin_]       = {}\n",
    "                            if 'to'  not in self.bin_to_alter2s[_bin_]: self.bin_to_alter2s[_bin_]['to'] = set()\n",
    "                            self.bin_to_alter2s[_bin_]['to'] |= _set_alter2s_\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_fm_).is_in(self.bin_to_alter1s[_bin_]['to']) & pl.col(_to_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_fm_).alias('__alter1_col__'), pl.lit(_to_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                            _df_ = k_df.filter(pl.col(_to_).is_in(self.bin_to_alter1s[_bin_]['to']) & pl.col(_fm_).is_in(_set_alter2s_))\n",
    "                            _df_ = _df_.with_columns(pl.lit(_to_).alias('__alter1_col__'), pl.lit(_fm_).alias('__alter2_col__'), pl.lit(2).alias('__alter_level__'), pl.lit(_bin_).alias('__bin__'), pl.lit(_timestamp_).alias('__bin_ts__'), pl.lit('to').alias('__alter_side__'))\n",
    "                            _dfs_containing_alter2s_.append(_df_)\n",
    "\n",
    "                # For any shared nodes between the two sides, keep them on the 'fm' side\n",
    "                if _bin_ in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[_bin_] and 'to' in self.bin_to_alter2s[_bin_]:\n",
    "                    _shared_nodes_ = self.bin_to_alter2s[_bin_]['fm'] & self.bin_to_alter2s[_bin_]['to']\n",
    "                    if len(_shared_nodes_) > 0: self.bin_to_alter2s[_bin_]['to'] -= _shared_nodes_\n",
    "\n",
    "            if _found_matches_: \n",
    "                self.bin_to_timestamps[_bin_] = _timestamp_\n",
    "                _bin_ += 1\n",
    "        \n",
    "        # Concatenate the pieces and parts\n",
    "        if len(_dfs_containing_focus_) > 0:   self.df_alter1s = pl.concat(_dfs_containing_focus_).unique()    # unique because we may have duplicate rows on the two sides\n",
    "        else:                                 self.df_alter1s = pl.DataFrame()\n",
    "        if len(_dfs_containing_alter2s_) > 0: self.df_alter2s = pl.concat(_dfs_containing_alter2s_).unique()  # unique because we may have duplicate rows on the two sides\n",
    "        else:                                 self.df_alter2s = pl.DataFrame()\n",
    "\n",
    "    #\n",
    "    # svgSketch() - produce a basic sketch of how the visualization would look\n",
    "    #\n",
    "    def svgSketch(self):\n",
    "        w_usable, h_usable = self.w - 2*self.x_ins, self.h - 2*self.y_ins\n",
    "        y_mid              = self.y_ins + h_usable/2\n",
    "        bin_to_x           = {}\n",
    "        bin_inter_dist     = w_usable/(len(self.bin_to_alter1s) - 1)\n",
    "        for _bin_ in self.bin_to_alter1s: bin_to_x[_bin_] = self.x_ins + _bin_*bin_inter_dist\n",
    "        _y_diff_alter1s_, _y_diff_alter2s_ = h_usable/8, 2*h_usable/8\n",
    "\n",
    "        svg = [f'<svg x=\"0\" y=\"0\" width=\"{self.w}\" height=\"{self.h}\">']\n",
    "        svg.append(f'<rect x=\"0\" y=\"0\" width=\"{self.w}\" height=\"{self.h}\" fill=\"{self.rt_self.co_mgr.getTVColor(\"background\",\"default\")}\" />')\n",
    "\n",
    "        svg.append(f'<line x1=\"{self.x_ins}\" y1=\"{y_mid}\" x2=\"{self.x_ins+w_usable}\" y2=\"{y_mid}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"major\")}\" stroke-width=\"4\" />')        \n",
    "        for _bin_ in bin_to_x:\n",
    "            _x_ = bin_to_x[_bin_]\n",
    "            svg.append(f'<line x1=\"{_x_}\" y1=\"{self.y_ins}\" x2=\"{_x_}\" y2=\"{self.y_ins + h_usable}\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"minor\")}\" stroke-width=\"1.0\" />')\n",
    "            svg.append(f'<circle cx=\"{_x_}\" cy=\"{y_mid}\" r=\"5\" stroke=\"{self.rt_self.co_mgr.getTVColor(\"axis\",\"minor\")}\" stroke-width=\"1.0\" fill=\"{self.rt_self.co_mgr.getTVColor('data','default')}\" />')\n",
    "            _date_str_ = self.bin_to_timestamps[_bin_].strftime(self.__dateFormat__())\n",
    "            svg.append(self.rt_self.svgText(_date_str_, _x_-2, self.y_ins + h_usable + 4, rt.co_mgr.getTVColor('axis','minor'), anchor='begin', rotation=270))\n",
    "            if _bin_ in self.bin_to_alter1s and 'fm' in self.bin_to_alter1s[_bin_]:\n",
    "                _y_         = y_mid - _y_diff_alter1s_\n",
    "                _num_nodes_ = len(self.bin_to_alter1s[_bin_]['fm'])\n",
    "                svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "                if _bin_ in self.bin_to_alter2s and 'fm' in self.bin_to_alter2s[_bin_]:\n",
    "                    _y_         = y_mid - _y_diff_alter2s_\n",
    "                    _num_nodes_ = len(self.bin_to_alter2s[_bin_]['fm'])\n",
    "                    svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "            if _bin_ in self.bin_to_alter1s and 'to' in self.bin_to_alter1s[_bin_]:\n",
    "                _y_         = y_mid + _y_diff_alter1s_\n",
    "                _num_nodes_ = len(self.bin_to_alter1s[_bin_]['to'])\n",
    "                svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "                if _bin_ in self.bin_to_alter2s and 'to' in self.bin_to_alter2s[_bin_]:\n",
    "                    _y_         = y_mid + _y_diff_alter2s_\n",
    "                    _num_nodes_ = len(self.bin_to_alter2s[_bin_]['to'])\n",
    "                    svg.append(self.rt_self.svgText(str(_num_nodes_), _x_+2, _y_ + 4, 'black', anchor='begin', rotation=90))\n",
    "\n",
    "        svg.append('</svg>')\n",
    "        return ''.join(svg)\n",
    "\n",
    "    def __dateFormat__(self):\n",
    "        if   'd' in self.every: return '%Y-%m-%d'\n",
    "        elif 'h' in self.every: return '%Y-%m-%d %H'\n",
    "        else:                   return '%Y-%m-%d %H:%M'\n",
    "\n",
    "#\n",
    "# spreadLines()\n",
    "#\n",
    "sl = spreadLines(rt, df, [('sip','dip')], '172.30.0.4', every=\"1h\", h=384)\n",
    "rt.tile([sl.svgSketch()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.histogram(df, bin_by='sip', count_by='dip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = spreadLines(rt, df, [('sip','dip')], '172.10.0.6', every=\"1h\", h=384) # highest out-degree node\n",
    "rt.tile([sl.svgSketch()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.histogram(df, bin_by='dip', count_by='sip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = spreadLines(rt, df, [('sip','dip')], '172.0.0.1', every=\"1h\", h=384) # highest in-degree node\n",
    "rt.tile([sl.svgSketch()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
