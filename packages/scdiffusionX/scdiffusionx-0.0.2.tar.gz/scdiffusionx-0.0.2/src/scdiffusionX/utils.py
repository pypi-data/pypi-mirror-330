import anndata as ad
import scanpy as sc
import matplotlib.pyplot as plt
import numpy as np
import torch

from tqdm import tqdm
import scib
from sklearn.model_selection import  train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import *
from torch.autograd import Variable
import argparse

def parse_segment(segment):
    chrom, positions = segment.split('-')[0], '-'.join(segment.split('-')[1:])
    start, end = map(int, positions.split('-'))
    return chrom, start, end

def find_enhancer_overlaps(df, segment):
    chrom, start, end = parse_segment(segment)

    filtered = df[df['chrom'] == chrom]
    # æ£€æŸ¥é‡å ï¼šç‰‡æ®µèµ·å§‹ä½ç½®å°äºç­‰äº peak ç»“æŸï¼Œä¸”ç‰‡æ®µç»“æŸä½ç½®å¤§äºç­‰äº peak èµ·å§‹
    # overlaps = filtered[((filtered['start'] <= end) & (filtered['end'] >= start)) | ((filtered['start2'] <= end) & (filtered['end2'] >= start))]
    overlaps = filtered[(filtered['start'] <= end) & (filtered['end'] >= start)]
    return overlaps

def find_enhancer_overlaps_loop(df, segment):
    chrom, start, end = parse_segment(segment)

    filtered = df[df['chrom'] == chrom]
    # æ£€æŸ¥é‡å ï¼šç‰‡æ®µèµ·å§‹ä½ç½®å°äºç­‰äº peak ç»“æŸï¼Œä¸”ç‰‡æ®µç»“æŸä½ç½®å¤§äºç­‰äº peak èµ·å§‹
    overlaps = filtered[((filtered['start'] <= end+4000) & (filtered['end'] >= start-4000)) | ((filtered['start2'] <= end+4000) & (filtered['end2'] >= start-4000))]
    # overlaps = filtered[(filtered['start'] <= end) & (filtered['end'] >= start)]
    return overlaps

# æ‰¾overlapçš„åŸºå› 
def find_nearest_gene(final_df,segment):
    chrom, positions = segment.split('-')[0], '-'.join(segment.split('-')[1:])
    start, end = map(int, positions.split('-'))

    # ç­›é€‰ä¸ç‰‡æ®µé‡å çš„åŸºå› 
    relevant_genes = final_df[(final_df['seqname'] == chrom) &
                               (final_df['start'] <= end) & 
                               (final_df['end'] >= start) &
                               (final_df['feature'] == 'gene')] 

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŸºå› ï¼Œè¿”å› None
    if relevant_genes.empty:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŸºå› ï¼Œå¯»æ‰¾è·ç¦»æœ€è¿‘çš„åŸºå› 
        nearest_genes = final_df[(final_df['seqname'] == chrom) & 
                                (final_df['feature'] == 'gene')]

        # è®¡ç®—åˆ°ç‰‡æ®µçš„å¼€å§‹å’Œç»“æŸä½ç½®çš„è·ç¦»
        nearest_genes['distance_start'] = (nearest_genes['start'] - start).abs()
        nearest_genes['distance_end'] = (nearest_genes['end'] - end).abs()

        # æ‰¾åˆ°æœ€è¿‘çš„åŸºå› ï¼ˆæœ€å°è·ç¦»ï¼‰
        relevant_genes = nearest_genes.loc[nearest_genes[['distance_start', 'distance_end']].min(axis=1).idxmin()]
        return [relevant_genes['gene_name']], relevant_genes['feature']
    return list(relevant_genes['gene_name']), relevant_genes['feature']

def check_tss_overlap(df, chrom_segment):
    # è§£ææŸ“è‰²ä½“ç‰‡æ®µ
    chrom, positions = chrom_segment.split('-')[0], '-'.join(chrom_segment.split('-')[1:])
    start_pos, end_pos = map(int, positions.split('-'))

    # è¿‡æ»¤å‡ºç›¸å…³æŸ“è‰²ä½“çš„åŸºå› 
    relevant_genes = df[(df['seqname'] == chrom) & (df['feature'] == 'gene')]

    # æ ¹æ®é“¾è®¡ç®— TSS ä½ç½®
    relevant_genes['tss_position_start'] = relevant_genes.apply(
        lambda gene: gene['start']-3000 if gene['strand'] == '+' else gene['end'], axis=1
    )
    relevant_genes['tss_position_end'] = relevant_genes.apply(
        lambda gene: gene['start'] if gene['strand'] == '+' else gene['end'] + 3000, axis=1
    )

    # æ£€æŸ¥ç»™å®šç‰‡æ®µæ˜¯å¦ä¸ TSS é‡å 
    overlaps = relevant_genes[
        (start_pos <= relevant_genes['tss_position_end']) &
         (relevant_genes['tss_position_start'] <= end_pos)
    ]

    # åˆ›å»ºé‡å ç»“æœ
    result = overlaps['gene_name'].copy().values

    return result

def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
    '''
    å°†æºåŸŸæ•°æ®å’Œç›®æ ‡åŸŸæ•°æ®è½¬åŒ–ä¸ºæ ¸çŸ©é˜µ, å³ä¸Šæ–‡ä¸­çš„K
    Params: 
	    source: æºåŸŸæ•°æ®(n * len(x))
	    target: ç›®æ ‡åŸŸæ•°æ®(m * len(y))
	    kernel_mul: 
	    kernel_num: å–ä¸åŒé«˜æ–¯æ ¸çš„æ•°é‡
	    fix_sigma: ä¸åŒé«˜æ–¯æ ¸çš„sigmaå€¼
	Return:
		sum(kernel_val): å¤šä¸ªæ ¸çŸ©é˜µä¹‹å’Œ
    '''
    n_samples = int(source.size()[0])+int(target.size()[0])# æ±‚çŸ©é˜µçš„è¡Œæ•°ï¼Œä¸€èˆ¬sourceå’Œtargetçš„å°ºåº¦æ˜¯ä¸€æ ·çš„ï¼Œè¿™æ ·ä¾¿äºè®¡ç®—
    total = torch.cat([source, target], dim=0)#å°†source,targetæŒ‰åˆ—æ–¹å‘åˆå¹¶
    #å°†totalå¤åˆ¶ï¼ˆn+mï¼‰ä»½
    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    #å°†totalçš„æ¯ä¸€è¡Œéƒ½å¤åˆ¶æˆï¼ˆn+mï¼‰è¡Œï¼Œå³æ¯ä¸ªæ•°æ®éƒ½æ‰©å±•æˆï¼ˆn+mï¼‰ä»½
    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    #æ±‚ä»»æ„ä¸¤ä¸ªæ•°æ®ä¹‹é—´çš„å’Œï¼Œå¾—åˆ°çš„çŸ©é˜µä¸­åæ ‡ï¼ˆi,jï¼‰ä»£è¡¨totalä¸­ç¬¬iè¡Œæ•°æ®å’Œç¬¬jè¡Œæ•°æ®ä¹‹é—´çš„l2 distance(i==jæ—¶ä¸º0ï¼‰
    
    batch_size = 200
    num_window = int(total0.shape[0]/batch_size)+1
    L2_dis = []
    for i in tqdm(range(num_window)):
        diff = (total0[i*batch_size:(i+1)*batch_size].cuda()-total1[i*batch_size:(i+1)*batch_size].cuda())
        diff.square_()
        L2_dis.append(diff.sum(2).cpu())
    L2_distance = torch.concatenate(L2_dis,dim=0)

    # L2_distance = ((total0-total1)**2).sum(2) 

    #è°ƒæ•´é«˜æ–¯æ ¸å‡½æ•°çš„sigmaå€¼
    if fix_sigma:
        bandwidth = fix_sigma
    else:
        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)
    #ä»¥fix_sigmaä¸ºä¸­å€¼ï¼Œä»¥kernel_mulä¸ºå€æ•°å–kernel_numä¸ªbandwidthå€¼ï¼ˆæ¯”å¦‚fix_sigmaä¸º1æ—¶ï¼Œå¾—åˆ°[0.25,0.5,1,2,4]
    bandwidth /= kernel_mul ** (kernel_num // 2)
    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
    #é«˜æ–¯æ ¸å‡½æ•°çš„æ•°å­¦è¡¨è¾¾å¼
    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
    #å¾—åˆ°æœ€ç»ˆçš„æ ¸çŸ©é˜µ
    return sum(kernel_val)#/len(kernel_val)

def mmd_rbf(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
    '''
    è®¡ç®—æºåŸŸæ•°æ®å’Œç›®æ ‡åŸŸæ•°æ®çš„MMDè·ç¦»
    Params: 
	    source: æºåŸŸæ•°æ®(n * len(x))
	    target: ç›®æ ‡åŸŸæ•°æ®(m * len(y))
	    kernel_mul: 
	    kernel_num: å–ä¸åŒé«˜æ–¯æ ¸çš„æ•°é‡
	    fix_sigma: ä¸åŒé«˜æ–¯æ ¸çš„sigmaå€¼
	Return:
		loss: MMD loss
    '''
    batch_size = int(source.size()[0])#ä¸€èˆ¬é»˜è®¤ä¸ºæºåŸŸå’Œç›®æ ‡åŸŸçš„batchsizeç›¸åŒ
    kernels = guassian_kernel(source, target,
        kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)
    #æ ¹æ®å¼ï¼ˆ3ï¼‰å°†æ ¸çŸ©é˜µåˆ†æˆ4éƒ¨åˆ†
    XX = kernels[:batch_size, :batch_size]
    YY = kernels[batch_size:, batch_size:]
    XY = kernels[:batch_size, batch_size:]
    YX = kernels[batch_size:, :batch_size]
    loss = torch.mean(XX + YY - XY -YX)
    return loss#å› ä¸ºä¸€èˆ¬éƒ½æ˜¯n==mï¼Œæ‰€ä»¥LçŸ©é˜µä¸€èˆ¬ä¸åŠ å…¥è®¡ç®—


def MMD(adata):
    real = adata[adata.obs_names=='true_Cell'].obsm['X_pca']
    gen = adata[adata.obs_names=='gen_Cell'].obsm['X_pca']
    X = torch.Tensor(real)
    Y = torch.Tensor(gen)
    X,Y = Variable(X), Variable(Y)
    return mmd_rbf(X,Y)


def LISI(adata):
    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20)
    lisi = scib.me.ilisi_graph(adata, batch_key="batch", type_="knn")
    return lisi


def random_forest(adata, return_roc = False):
    real = adata[adata.obs_names=='true_Cell'].obsm['X_pca']
    sim = adata[adata.obs_names=='gen_Cell'].obsm['X_pca']

    data = np.concatenate((real,sim),axis=0)
    label = np.concatenate((np.ones((real.shape[0])),np.zeros((sim.shape[0]))))

    ##å°†è®­ç»ƒé›†åˆ‡åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train,X_val,y_train,y_val = train_test_split(data, label,
                                                test_size = 0.25,random_state = 1)

    ## ä½¿ç”¨éšæœºæ£®æ—å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»
    rfc1 = RandomForestClassifier(n_estimators = 1000, # æ ‘çš„æ•°é‡
                                max_depth= 5,       # å­æ ‘æœ€å¤§æ·±åº¦
                                oob_score=True,
                                class_weight = "balanced",
                                random_state=1)
    rfc1.fit(X_train,y_train)

    ## å¯è§†åŒ–åœ¨éªŒè¯é›†ä¸Šçš„Rocæ›²çº¿
    pre_y = rfc1.predict_proba(X_val)[:, 1]
    fpr_Nb, tpr_Nb, _ = roc_curve(y_val, pre_y)
    aucval = auc(fpr_Nb, tpr_Nb)    # è®¡ç®—aucçš„å–å€¼
    if return_roc:
        return aucval, fpr_Nb, tpr_Nb
    return aucval

def norm_total(array, target_sum = 1e4):        
    current_sum = np.sum(array,axis=1)[:,None] if len(array.shape)>1 else np.sum(array)
    normalization_factor = target_sum / current_sum  
    normalized_array = array * normalization_factor  
    return normalized_array


def plot_mse_curves(mse_values,legend, x_list, layer, metric):
    # è·å–ç¬¬ä¸€ä¸ªç»´åº¦å’Œç¬¬äºŒä¸ªç»´åº¦çš„å¤§å°
    n_curves, n_points = mse_values.shape
    
    # åˆ›å»ºä¸€ä¸ªç»˜å›¾å¯¹è±¡
    plt.rcParams['pdf.fonttype'] = 42
    plt.rcParams['ps.fonttype'] = 42
    plt.figure(figsize=(12, 7))
    
    # ä¾æ¬¡ç»˜åˆ¶æ¯ä¸€æ¡æ›²çº¿
    for i in range(n_curves):
        plt.plot(x_list, mse_values[i], label=legend[i])  # æ·»åŠ æ ‡ç­¾ä»¥ä¾¿è¯†åˆ«æ¯æ¡æ›²çº¿
    
    # æ·»åŠ å›¾ä¾‹ã€æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title(f"{metric} Curves layer {layer}")
    plt.xlabel("Time step / 100")
    plt.ylabel(f"{metric} Value with prev step")
    # è®¾ç½®å›¾ä¾‹åœ¨å›¾çš„å¤–éƒ¨
    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
    
    # è°ƒæ•´å¸ƒå±€ï¼Œä½¿å¾—å›¾ä¾‹ä¸é®æŒ¡å›¾
    plt.tight_layout(rect=[0, 0, 0.85, 1])
    
    # æ˜¾ç¤ºå›¾å½¢
    # plt.show()
    plt.savefig(f'/stor/lep/workspace/multi_diffusion/MM-Diffusion/evaluate_script/figures/attention/map_info/{metric}_{layer}.pdf')


def calculate_mse(array):
    # å¾—åˆ°æ•°ç»„çš„ç¬¬ä¸€ä¸ªç»´åº¦å¤§å°
    n, h, w, d = array.shape
    
    # åˆå§‹åŒ–å­˜å‚¨ç»“æœçš„æ•°ç»„
    mse_values = np.zeros((n - 1, h))
    
    # é€å¯¹è®¡ç®—å‰åå¼ é‡çš„MSE
    for i in range(1, n):
        # è®¡ç®—ç›¸é‚»ä¸¤ä¸ªå¼ é‡ä¹‹é—´æ¯ä¸ªä½ç½®çš„MSE
        mse = np.mean((array[i] - array[i - 1]) ** 2, axis=(1, 2))
        mse_values[i - 1] = mse
    
    return mse_values

def find_max_index(matrix, topk):
    # å±•å¹³çŸ©é˜µ
    flattened = matrix.flatten()

    # è·å–æ’åºåçš„ç´¢å¼•å¹¶å–åäº”ä¸ª
    top_indices_flat = np.argsort(flattened)[-topk:]

    # å°†ä¸€ç»´ç´¢å¼•è½¬æ¢ä¸ºäºŒç»´ç´¢å¼•
    index_rna = np.unravel_index(top_indices_flat, matrix.shape)

    return index_rna

def calculate_entropy(matrix):
    # Flatten the matrix to calculate probabilities
    flat_matrix = matrix.flatten()
    
    # Normalize the matrix to create a probability distribution
    probabilities = flat_matrix / np.sum(flat_matrix)
    
    # Filter out zero probabilities to avoid log2(0)
    probabilities = probabilities[probabilities > 0]
    
    # Calculate entropy
    entropy = -np.sum(probabilities * np.log2(probabilities))
    
    return entropy

def calculate_entropies(array):
    # Initialize a 9x22 array to store entropies
    entropies = np.zeros((array.shape[0], array.shape[1]))
    
    # Iterate over each 128x128 matrix
    for i in range(array.shape[0]):
        for j in range(array.shape[1]):
            entropies[i, j] = calculate_entropy(array[i, j])
    
    return entropies


def parse_args():
    parser = argparse.ArgumentParser(description="Simple example of a training script.")
    parser.add_argument(
        "--input_perturbation", type=float, default=0, help="The scale of input perturbation. Recommended 0.1."
    )
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        default=None,
        # required=True,
        help="Path to pretrained model or model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--pretrained_vae_path",
        type=str,
        default=None,
        # required=True,
        help="Path to pretrained vae.",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        required=False,
        help="Revision of pretrained model identifier from huggingface.co/models.",
    )
    parser.add_argument(
        "--variant",
        type=str,
        default=None,
        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
    )
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help=(
            "The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,"
            " dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,"
            " or to a folder containing files that ğŸ¤— Datasets can understand."
        ),
    )
    parser.add_argument(
        "--dataset_config_name",
        type=str,
        default=None,
        help="The config of the Dataset, leave as None if there's only one config.",
    )
    parser.add_argument(
        "--train_data_dir",
        type=str,
        default=None,
        help=(
            "A folder containing the training data. Folder contents must follow the structure described in"
            " https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file"
            " must exist to provide the captions for the images. Ignored if `dataset_name` is specified."
        ),
    )
    parser.add_argument(
        "--image_column", type=str, default="image", help="The column of the dataset containing an image."
    )
    parser.add_argument(
        "--caption_column",
        type=str,
        default="text",
        help="The column of the dataset containing a caption or a list of captions.",
    )
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=None,
        help=(
            "For debugging purposes or quicker training, truncate the number of training examples to this "
            "value if set."
        ),
    )
    parser.add_argument(
        "--validation_prompts",
        type=str,
        default=None,
        nargs="+",
        help=("A set of prompts evaluated every `--validation_epochs` and logged to `--report_to`."),
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="sd-model-finetuned",
        help="The output directory where the model predictions and checkpoints will be written.",
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default=None,
        help="The directory where the downloaded models and datasets will be stored.",
    )
    parser.add_argument("--seed", type=int, default=None, help="A seed for reproducible training.")
    parser.add_argument(
        "--resolution",
        type=int,
        default=512,
        help=(
            "The resolution for input images, all the images in the train/validation dataset will be resized to this"
            " resolution"
        ),
    )
    parser.add_argument(
        "--center_crop",
        default=False,
        action="store_true",
        help=(
            "Whether to center crop the input images to the resolution. If not set, the images will be randomly"
            " cropped. The images will be resized to the resolution first before cropping."
        ),
    )
    parser.add_argument(
        "--random_flip",
        action="store_true",
        help="whether to randomly flip images horizontally",
    )
    parser.add_argument(
        "--train_batch_size", type=int, default=16, help="Batch size (per device) for the training dataloader."
    )
    parser.add_argument("--num_train_epochs", type=int, default=100)
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="Total number of training steps to perform.  If provided, overrides num_train_epochs.",
    )
    parser.add_argument(
        "--gradient_accumulation_steps",
        type=int,
        default=1,
        help="Number of updates steps to accumulate before performing a backward/update pass.",
    )
    parser.add_argument(
        "--gradient_checkpointing",
        action="store_true",
        help="Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.",
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="Initial learning rate (after the potential warmup period) to use.",
    )
    parser.add_argument(
        "--scale_lr",
        action="store_true",
        default=False,
        help="Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.",
    )
    parser.add_argument(
        "--lr_scheduler",
        type=str,
        default="constant",
        help=(
            'The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
            ' "constant", "constant_with_warmup"]'
        ),
    )
    parser.add_argument(
        "--lr_warmup_steps", type=int, default=500, help="Number of steps for the warmup in the lr scheduler."
    )
    parser.add_argument(
        "--snr_gamma",
        type=float,
        default=None,
        help="SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. "
        "More details here: https://arxiv.org/abs/2303.09556.",
    )
    parser.add_argument(
        "--dream_training",
        action="store_true",
        help=(
            "Use the DREAM training method, which makes training more efficient and accurate at the ",
            "expense of doing an extra forward pass. See: https://arxiv.org/abs/2312.00210",
        ),
    )
    parser.add_argument(
        "--dream_detail_preservation",
        type=float,
        default=1.0,
        help="Dream detail preservation factor p (should be greater than 0; default=1.0, as suggested in the paper)",
    )
    parser.add_argument(
        "--use_8bit_adam", action="store_true", help="Whether or not to use 8-bit Adam from bitsandbytes."
    )
    parser.add_argument(
        "--allow_tf32",
        action="store_true",
        help=(
            "Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see"
            " https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices"
        ),
    )
    parser.add_argument("--use_ema", action="store_true", help="Whether to use EMA model.")
    parser.add_argument("--offload_ema", action="store_true", help="Offload EMA model to CPU during training step.")
    parser.add_argument("--foreach_ema", action="store_true", help="Use faster foreach implementation of EMAModel.")
    parser.add_argument(
        "--non_ema_revision",
        type=str,
        default=None,
        required=False,
        help=(
            "Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or"
            " remote repository specified with --pretrained_model_name_or_path."
        ),
    )
    parser.add_argument(
        "--dataloader_num_workers",
        type=int,
        default=0,
        help=(
            "Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process."
        ),
    )
    parser.add_argument("--adam_beta1", type=float, default=0.9, help="The beta1 parameter for the Adam optimizer.")
    parser.add_argument("--adam_beta2", type=float, default=0.999, help="The beta2 parameter for the Adam optimizer.")
    parser.add_argument("--adam_weight_decay", type=float, default=1e-2, help="Weight decay to use.")
    parser.add_argument("--adam_epsilon", type=float, default=1e-08, help="Epsilon value for the Adam optimizer")
    parser.add_argument("--max_grad_norm", default=1.0, type=float, help="Max gradient norm.")
    parser.add_argument("--push_to_hub", action="store_true", help="Whether or not to push the model to the Hub.")
    parser.add_argument("--hub_token", type=str, default=None, help="The token to use to push to the Model Hub.")
    parser.add_argument(
        "--prediction_type",
        type=str,
        default=None,
        help="The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave `None`. If left to `None` the default prediction type of the scheduler: `noise_scheduler.config.prediction_type` is chosen.",
    )
    parser.add_argument(
        "--hub_model_id",
        type=str,
        default=None,
        help="The name of the repository to keep in sync with the local `output_dir`.",
    )
    parser.add_argument(
        "--logging_dir",
        type=str,
        default="logs",
        help=(
            "[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to"
            " *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***."
        ),
    )
    parser.add_argument(
        "--mixed_precision",
        type=str,
        default=None,
        choices=["no", "fp16", "bf16"],
        help=(
            "Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >="
            " 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the"
            " flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config."
        ),
    )
    parser.add_argument(
        "--report_to",
        type=str,
        default="tensorboard",
        help=(
            'The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
            ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
        ),
    )
    parser.add_argument("--local_rank", type=int, default=-1, help="For distributed training: local_rank")
    parser.add_argument(
        "--checkpointing_steps",
        type=int,
        default=500,
        help=(
            "Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming"
            " training using `--resume_from_checkpoint`."
        ),
    )
    parser.add_argument(
        "--checkpoints_total_limit",
        type=int,
        default=None,
        help=("Max number of checkpoints to store."),
    )
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help=(
            "Whether training should be resumed from a previous checkpoint. Use a path saved by"
            ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
        ),
    )
    parser.add_argument(
        "--enable_xformers_memory_efficient_attention", action="store_true", help="Whether or not to use xformers."
    )
    parser.add_argument("--noise_offset", type=float, default=0, help="The scale of noise offset.")
    parser.add_argument(
        "--validation_epochs",
        type=int,
        default=5,
        help="Run validation every X epochs.",
    )
    parser.add_argument(
        "--tracker_project_name",
        type=str,
        default="text2image-fine-tune",
        help=(
            "The `project_name` argument passed to Accelerator.init_trackers for"
            " more information see https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator"
        ),
    )

    args = parser.parse_args(args=[])

    return args