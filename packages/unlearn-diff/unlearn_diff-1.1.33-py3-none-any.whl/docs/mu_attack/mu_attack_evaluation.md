### Evaluation:

In this section, we assess the performance and robustness of the results generated by the attack algorithms

### Activate Environment 

You can either use default environment or mu_attack specific environment.

**Example**
```bash
create_env mu_attack

```

#### **Running the Evaluation Framework**

Create a file, eg, `evaluate.py` and use examples and modify your configs to run the file.  

**Example Code**

```python
from mu_attack.configs.evaluation import attack_evaluation_config
from mu_attack.execs.evaluator import MuAttackEvaluator

def main():
    config = attack_evaluation_config
    config = attack_evaluation_config
    config.asr.root = "results/hard_prompt_esd_nudity_P4D_abstractionism/P4d"
    config.asr.root_no_attack = "results/hard_prompt_esd_nudity_P4D_abstrc/NoAttackEsdNudity"
    config.clip.devices = "0"
    config.clip.image_path = "results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images"
    config.clip.log_path = "results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/log.json"
    config.fid.ref_batch_path = "results/hard_prompt_esd_nudity_P4D_abstractionism/P4d/images"
    config.fid.sample_batch_path = "data/i2p/nude"

    # Common output path
    config.output_path = "results/evaluation/results.json"

    evaluator = MuAttackEvaluator(config)
    
    # Run the evaluation (this will run ASR, CLIP, and FID evaluators)
    results = evaluator.run()
    
    print("Evaluation Results:",results)

if __name__ == "__main__":
    main()
```

**Running the Training Script in Offline Mode**

```bash
WANDB_MODE=offline python evaluate.py
```

**How It Works** 
* Default Values: The script first loads default values from the evluation config file as in configs section.

* Parameter Overrides: Any parameters passed directly to the algorithm, overrides these configs.

* Final Configuration: The script merges the configs and convert them into dictionary to proceed with the evaluation. 


**Evaluation Metrics:**

* Attack Succes Rate (ASR)

* Fr√©chet inception distance(FID): evaluate distributional quality of image generations, lower is better.

* CLIP score : measure contextual alignment with prompt descriptions, higher is better.


**Configuration File Structure for Evaluator**

* ASR Evaluator Configuration

    - root: Directory containing results with attack.
    - root-no-attack: Directory containing results without attack.

* Clip Evaluator Configuration

    - image_path: Path to the directory containing generated images to evaluate.
    - devices: Device ID(s) to use for evaluation. Example: "0" for the first GPU or "0,1" for multiple GPUs.
    - log_path: Path to the log file containing prompt for the generated images.
    - model_name_or_path: Path or model name for the pre-trained CLIP model. Default is "openai/clip-vit-base-patch32".

* FID Evaluator Configuration

    - ref_batch_path: Path to the directory containing reference images.
    - sample_batch_path: Path to the directory containing generated/sample images.

* Global Configuration

    - output_path: Path to save the evaluation results as a JSON file.


