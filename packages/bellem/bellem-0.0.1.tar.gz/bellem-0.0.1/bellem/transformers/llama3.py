# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/transformers/transformers.llama3.ipynb.

# %% auto 0
__all__ = ['prepare_llama3_for_training', 'prepare_llama3_for_inference']

# %% ../../nbs/transformers/transformers.llama3.ipynb 3
def prepare_llama3_for_training(tokenizer, model):
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training
    model.config.pretraining_tp = 1
    model.config.use_cache = False

def prepare_llama3_for_inference(tokenizer, model):
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "left" # During inference, the padding must be on the left for decoder-only models
