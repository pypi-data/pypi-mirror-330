# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from ... import _utilities
from . import outputs
from ._enums import *
from ._inputs import *

__all__ = ['ModelDeploymentMonitoringJobArgs', 'ModelDeploymentMonitoringJob']

@pulumi.input_type
class ModelDeploymentMonitoringJobArgs:
    def __init__(__self__, *,
                 display_name: pulumi.Input[str],
                 endpoint: pulumi.Input[str],
                 logging_sampling_strategy: pulumi.Input['GoogleCloudAiplatformV1beta1SamplingStrategyArgs'],
                 model_deployment_monitoring_objective_configs: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]],
                 model_deployment_monitoring_schedule_config: pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs'],
                 analysis_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 enable_monitoring_pipeline_logs: Optional[pulumi.Input[bool]] = None,
                 encryption_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 location: Optional[pulumi.Input[str]] = None,
                 log_ttl: Optional[pulumi.Input[str]] = None,
                 model_monitoring_alert_config: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']] = None,
                 predict_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 sample_predict_instance: Optional[Any] = None,
                 stats_anomalies_base_directory: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1GcsDestinationArgs']] = None):
        """
        The set of arguments for constructing a ModelDeploymentMonitoringJob resource.
        :param pulumi.Input[str] display_name: The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
        :param pulumi.Input[str] endpoint: Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        :param pulumi.Input['GoogleCloudAiplatformV1beta1SamplingStrategyArgs'] logging_sampling_strategy: Sample Strategy for logging.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]] model_deployment_monitoring_objective_configs: The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
        :param pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs'] model_deployment_monitoring_schedule_config: Schedule config for running the monitoring job.
        :param pulumi.Input[str] analysis_instance_schema_uri: YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
        :param pulumi.Input[bool] enable_monitoring_pipeline_logs: If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
        :param pulumi.Input['GoogleCloudAiplatformV1beta1EncryptionSpecArgs'] encryption_spec: Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        :param pulumi.Input[str] log_ttl: The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
        :param pulumi.Input['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs'] model_monitoring_alert_config: Alert config for model monitoring.
        :param pulumi.Input[str] predict_instance_schema_uri: YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
        :param Any sample_predict_instance: Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
        :param pulumi.Input['GoogleCloudAiplatformV1beta1GcsDestinationArgs'] stats_anomalies_base_directory: Stats anomalies base folder path.
        """
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "endpoint", endpoint)
        pulumi.set(__self__, "logging_sampling_strategy", logging_sampling_strategy)
        pulumi.set(__self__, "model_deployment_monitoring_objective_configs", model_deployment_monitoring_objective_configs)
        pulumi.set(__self__, "model_deployment_monitoring_schedule_config", model_deployment_monitoring_schedule_config)
        if analysis_instance_schema_uri is not None:
            pulumi.set(__self__, "analysis_instance_schema_uri", analysis_instance_schema_uri)
        if enable_monitoring_pipeline_logs is not None:
            pulumi.set(__self__, "enable_monitoring_pipeline_logs", enable_monitoring_pipeline_logs)
        if encryption_spec is not None:
            pulumi.set(__self__, "encryption_spec", encryption_spec)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if location is not None:
            pulumi.set(__self__, "location", location)
        if log_ttl is not None:
            pulumi.set(__self__, "log_ttl", log_ttl)
        if model_monitoring_alert_config is not None:
            pulumi.set(__self__, "model_monitoring_alert_config", model_monitoring_alert_config)
        if predict_instance_schema_uri is not None:
            pulumi.set(__self__, "predict_instance_schema_uri", predict_instance_schema_uri)
        if project is not None:
            pulumi.set(__self__, "project", project)
        if sample_predict_instance is not None:
            pulumi.set(__self__, "sample_predict_instance", sample_predict_instance)
        if stats_anomalies_base_directory is not None:
            pulumi.set(__self__, "stats_anomalies_base_directory", stats_anomalies_base_directory)

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> pulumi.Input[str]:
        """
        The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
        """
        return pulumi.get(self, "display_name")

    @display_name.setter
    def display_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "display_name", value)

    @property
    @pulumi.getter
    def endpoint(self) -> pulumi.Input[str]:
        """
        Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        """
        return pulumi.get(self, "endpoint")

    @endpoint.setter
    def endpoint(self, value: pulumi.Input[str]):
        pulumi.set(self, "endpoint", value)

    @property
    @pulumi.getter(name="loggingSamplingStrategy")
    def logging_sampling_strategy(self) -> pulumi.Input['GoogleCloudAiplatformV1beta1SamplingStrategyArgs']:
        """
        Sample Strategy for logging.
        """
        return pulumi.get(self, "logging_sampling_strategy")

    @logging_sampling_strategy.setter
    def logging_sampling_strategy(self, value: pulumi.Input['GoogleCloudAiplatformV1beta1SamplingStrategyArgs']):
        pulumi.set(self, "logging_sampling_strategy", value)

    @property
    @pulumi.getter(name="modelDeploymentMonitoringObjectiveConfigs")
    def model_deployment_monitoring_objective_configs(self) -> pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]]:
        """
        The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
        """
        return pulumi.get(self, "model_deployment_monitoring_objective_configs")

    @model_deployment_monitoring_objective_configs.setter
    def model_deployment_monitoring_objective_configs(self, value: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]]):
        pulumi.set(self, "model_deployment_monitoring_objective_configs", value)

    @property
    @pulumi.getter(name="modelDeploymentMonitoringScheduleConfig")
    def model_deployment_monitoring_schedule_config(self) -> pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs']:
        """
        Schedule config for running the monitoring job.
        """
        return pulumi.get(self, "model_deployment_monitoring_schedule_config")

    @model_deployment_monitoring_schedule_config.setter
    def model_deployment_monitoring_schedule_config(self, value: pulumi.Input['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs']):
        pulumi.set(self, "model_deployment_monitoring_schedule_config", value)

    @property
    @pulumi.getter(name="analysisInstanceSchemaUri")
    def analysis_instance_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
        """
        return pulumi.get(self, "analysis_instance_schema_uri")

    @analysis_instance_schema_uri.setter
    def analysis_instance_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "analysis_instance_schema_uri", value)

    @property
    @pulumi.getter(name="enableMonitoringPipelineLogs")
    def enable_monitoring_pipeline_logs(self) -> Optional[pulumi.Input[bool]]:
        """
        If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
        """
        return pulumi.get(self, "enable_monitoring_pipeline_logs")

    @enable_monitoring_pipeline_logs.setter
    def enable_monitoring_pipeline_logs(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_monitoring_pipeline_logs", value)

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']]:
        """
        Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @encryption_spec.setter
    def encryption_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']]):
        pulumi.set(self, "encryption_spec", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter
    def location(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "location")

    @location.setter
    def location(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "location", value)

    @property
    @pulumi.getter(name="logTtl")
    def log_ttl(self) -> Optional[pulumi.Input[str]]:
        """
        The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
        """
        return pulumi.get(self, "log_ttl")

    @log_ttl.setter
    def log_ttl(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "log_ttl", value)

    @property
    @pulumi.getter(name="modelMonitoringAlertConfig")
    def model_monitoring_alert_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']]:
        """
        Alert config for model monitoring.
        """
        return pulumi.get(self, "model_monitoring_alert_config")

    @model_monitoring_alert_config.setter
    def model_monitoring_alert_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']]):
        pulumi.set(self, "model_monitoring_alert_config", value)

    @property
    @pulumi.getter(name="predictInstanceSchemaUri")
    def predict_instance_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "predict_instance_schema_uri")

    @predict_instance_schema_uri.setter
    def predict_instance_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "predict_instance_schema_uri", value)

    @property
    @pulumi.getter
    def project(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "project")

    @project.setter
    def project(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "project", value)

    @property
    @pulumi.getter(name="samplePredictInstance")
    def sample_predict_instance(self) -> Optional[Any]:
        """
        Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "sample_predict_instance")

    @sample_predict_instance.setter
    def sample_predict_instance(self, value: Optional[Any]):
        pulumi.set(self, "sample_predict_instance", value)

    @property
    @pulumi.getter(name="statsAnomaliesBaseDirectory")
    def stats_anomalies_base_directory(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1beta1GcsDestinationArgs']]:
        """
        Stats anomalies base folder path.
        """
        return pulumi.get(self, "stats_anomalies_base_directory")

    @stats_anomalies_base_directory.setter
    def stats_anomalies_base_directory(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1beta1GcsDestinationArgs']]):
        pulumi.set(self, "stats_anomalies_base_directory", value)


class ModelDeploymentMonitoringJob(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 analysis_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 display_name: Optional[pulumi.Input[str]] = None,
                 enable_monitoring_pipeline_logs: Optional[pulumi.Input[bool]] = None,
                 encryption_spec: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']]] = None,
                 endpoint: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 location: Optional[pulumi.Input[str]] = None,
                 log_ttl: Optional[pulumi.Input[str]] = None,
                 logging_sampling_strategy: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1SamplingStrategyArgs']]] = None,
                 model_deployment_monitoring_objective_configs: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]]]] = None,
                 model_deployment_monitoring_schedule_config: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs']]] = None,
                 model_monitoring_alert_config: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']]] = None,
                 predict_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 sample_predict_instance: Optional[Any] = None,
                 stats_anomalies_base_directory: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1GcsDestinationArgs']]] = None,
                 __props__=None):
        """
        Creates a ModelDeploymentMonitoringJob. It will run periodically on a configured interval.
        Auto-naming is currently not supported for this resource.

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[str] analysis_instance_schema_uri: YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
        :param pulumi.Input[str] display_name: The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
        :param pulumi.Input[bool] enable_monitoring_pipeline_logs: If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
        :param pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']] encryption_spec: Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
        :param pulumi.Input[str] endpoint: Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        :param pulumi.Input[str] log_ttl: The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
        :param pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1SamplingStrategyArgs']] logging_sampling_strategy: Sample Strategy for logging.
        :param pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]]] model_deployment_monitoring_objective_configs: The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
        :param pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs']] model_deployment_monitoring_schedule_config: Schedule config for running the monitoring job.
        :param pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']] model_monitoring_alert_config: Alert config for model monitoring.
        :param pulumi.Input[str] predict_instance_schema_uri: YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
        :param Any sample_predict_instance: Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
        :param pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1GcsDestinationArgs']] stats_anomalies_base_directory: Stats anomalies base folder path.
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: ModelDeploymentMonitoringJobArgs,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        Creates a ModelDeploymentMonitoringJob. It will run periodically on a configured interval.
        Auto-naming is currently not supported for this resource.

        :param str resource_name: The name of the resource.
        :param ModelDeploymentMonitoringJobArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(ModelDeploymentMonitoringJobArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 analysis_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 display_name: Optional[pulumi.Input[str]] = None,
                 enable_monitoring_pipeline_logs: Optional[pulumi.Input[bool]] = None,
                 encryption_spec: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1EncryptionSpecArgs']]] = None,
                 endpoint: Optional[pulumi.Input[str]] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 location: Optional[pulumi.Input[str]] = None,
                 log_ttl: Optional[pulumi.Input[str]] = None,
                 logging_sampling_strategy: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1SamplingStrategyArgs']]] = None,
                 model_deployment_monitoring_objective_configs: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigArgs']]]]] = None,
                 model_deployment_monitoring_schedule_config: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigArgs']]] = None,
                 model_monitoring_alert_config: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigArgs']]] = None,
                 predict_instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 project: Optional[pulumi.Input[str]] = None,
                 sample_predict_instance: Optional[Any] = None,
                 stats_anomalies_base_directory: Optional[pulumi.Input[pulumi.InputType['GoogleCloudAiplatformV1beta1GcsDestinationArgs']]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = ModelDeploymentMonitoringJobArgs.__new__(ModelDeploymentMonitoringJobArgs)

            __props__.__dict__["analysis_instance_schema_uri"] = analysis_instance_schema_uri
            if display_name is None and not opts.urn:
                raise TypeError("Missing required property 'display_name'")
            __props__.__dict__["display_name"] = display_name
            __props__.__dict__["enable_monitoring_pipeline_logs"] = enable_monitoring_pipeline_logs
            __props__.__dict__["encryption_spec"] = encryption_spec
            if endpoint is None and not opts.urn:
                raise TypeError("Missing required property 'endpoint'")
            __props__.__dict__["endpoint"] = endpoint
            __props__.__dict__["labels"] = labels
            __props__.__dict__["location"] = location
            __props__.__dict__["log_ttl"] = log_ttl
            if logging_sampling_strategy is None and not opts.urn:
                raise TypeError("Missing required property 'logging_sampling_strategy'")
            __props__.__dict__["logging_sampling_strategy"] = logging_sampling_strategy
            if model_deployment_monitoring_objective_configs is None and not opts.urn:
                raise TypeError("Missing required property 'model_deployment_monitoring_objective_configs'")
            __props__.__dict__["model_deployment_monitoring_objective_configs"] = model_deployment_monitoring_objective_configs
            if model_deployment_monitoring_schedule_config is None and not opts.urn:
                raise TypeError("Missing required property 'model_deployment_monitoring_schedule_config'")
            __props__.__dict__["model_deployment_monitoring_schedule_config"] = model_deployment_monitoring_schedule_config
            __props__.__dict__["model_monitoring_alert_config"] = model_monitoring_alert_config
            __props__.__dict__["predict_instance_schema_uri"] = predict_instance_schema_uri
            __props__.__dict__["project"] = project
            __props__.__dict__["sample_predict_instance"] = sample_predict_instance
            __props__.__dict__["stats_anomalies_base_directory"] = stats_anomalies_base_directory
            __props__.__dict__["bigquery_tables"] = None
            __props__.__dict__["create_time"] = None
            __props__.__dict__["error"] = None
            __props__.__dict__["latest_monitoring_pipeline_metadata"] = None
            __props__.__dict__["name"] = None
            __props__.__dict__["next_schedule_time"] = None
            __props__.__dict__["schedule_state"] = None
            __props__.__dict__["state"] = None
            __props__.__dict__["update_time"] = None
        replace_on_changes = pulumi.ResourceOptions(replace_on_changes=["location", "project"])
        opts = pulumi.ResourceOptions.merge(opts, replace_on_changes)
        super(ModelDeploymentMonitoringJob, __self__).__init__(
            'google-native:aiplatform/v1beta1:ModelDeploymentMonitoringJob',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None) -> 'ModelDeploymentMonitoringJob':
        """
        Get an existing ModelDeploymentMonitoringJob resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = ModelDeploymentMonitoringJobArgs.__new__(ModelDeploymentMonitoringJobArgs)

        __props__.__dict__["analysis_instance_schema_uri"] = None
        __props__.__dict__["bigquery_tables"] = None
        __props__.__dict__["create_time"] = None
        __props__.__dict__["display_name"] = None
        __props__.__dict__["enable_monitoring_pipeline_logs"] = None
        __props__.__dict__["encryption_spec"] = None
        __props__.__dict__["endpoint"] = None
        __props__.__dict__["error"] = None
        __props__.__dict__["labels"] = None
        __props__.__dict__["latest_monitoring_pipeline_metadata"] = None
        __props__.__dict__["location"] = None
        __props__.__dict__["log_ttl"] = None
        __props__.__dict__["logging_sampling_strategy"] = None
        __props__.__dict__["model_deployment_monitoring_objective_configs"] = None
        __props__.__dict__["model_deployment_monitoring_schedule_config"] = None
        __props__.__dict__["model_monitoring_alert_config"] = None
        __props__.__dict__["name"] = None
        __props__.__dict__["next_schedule_time"] = None
        __props__.__dict__["predict_instance_schema_uri"] = None
        __props__.__dict__["project"] = None
        __props__.__dict__["sample_predict_instance"] = None
        __props__.__dict__["schedule_state"] = None
        __props__.__dict__["state"] = None
        __props__.__dict__["stats_anomalies_base_directory"] = None
        __props__.__dict__["update_time"] = None
        return ModelDeploymentMonitoringJob(resource_name, opts=opts, __props__=__props__)

    @property
    @pulumi.getter(name="analysisInstanceSchemaUri")
    def analysis_instance_schema_uri(self) -> pulumi.Output[str]:
        """
        YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
        """
        return pulumi.get(self, "analysis_instance_schema_uri")

    @property
    @pulumi.getter(name="bigqueryTables")
    def bigquery_tables(self) -> pulumi.Output[Sequence['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTableResponse']]:
        """
        The created bigquery tables for the job under customer project. Customer could do their own query & analysis. There could be 4 log tables in maximum: 1. Training data logging predict request/response 2. Serving data logging predict request/response
        """
        return pulumi.get(self, "bigquery_tables")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> pulumi.Output[str]:
        """
        Timestamp when this ModelDeploymentMonitoringJob was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> pulumi.Output[str]:
        """
        The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="enableMonitoringPipelineLogs")
    def enable_monitoring_pipeline_logs(self) -> pulumi.Output[bool]:
        """
        If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
        """
        return pulumi.get(self, "enable_monitoring_pipeline_logs")

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1EncryptionSpecResponse']:
        """
        Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @property
    @pulumi.getter
    def endpoint(self) -> pulumi.Output[str]:
        """
        Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        """
        return pulumi.get(self, "endpoint")

    @property
    @pulumi.getter
    def error(self) -> pulumi.Output['outputs.GoogleRpcStatusResponse']:
        """
        Only populated when the job's state is `JOB_STATE_FAILED` or `JOB_STATE_CANCELLED`.
        """
        return pulumi.get(self, "error")

    @property
    @pulumi.getter
    def labels(self) -> pulumi.Output[Mapping[str, str]]:
        """
        The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter(name="latestMonitoringPipelineMetadata")
    def latest_monitoring_pipeline_metadata(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse']:
        """
        Latest triggered monitoring pipeline metadata.
        """
        return pulumi.get(self, "latest_monitoring_pipeline_metadata")

    @property
    @pulumi.getter
    def location(self) -> pulumi.Output[str]:
        return pulumi.get(self, "location")

    @property
    @pulumi.getter(name="logTtl")
    def log_ttl(self) -> pulumi.Output[str]:
        """
        The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
        """
        return pulumi.get(self, "log_ttl")

    @property
    @pulumi.getter(name="loggingSamplingStrategy")
    def logging_sampling_strategy(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1SamplingStrategyResponse']:
        """
        Sample Strategy for logging.
        """
        return pulumi.get(self, "logging_sampling_strategy")

    @property
    @pulumi.getter(name="modelDeploymentMonitoringObjectiveConfigs")
    def model_deployment_monitoring_objective_configs(self) -> pulumi.Output[Sequence['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigResponse']]:
        """
        The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
        """
        return pulumi.get(self, "model_deployment_monitoring_objective_configs")

    @property
    @pulumi.getter(name="modelDeploymentMonitoringScheduleConfig")
    def model_deployment_monitoring_schedule_config(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigResponse']:
        """
        Schedule config for running the monitoring job.
        """
        return pulumi.get(self, "model_deployment_monitoring_schedule_config")

    @property
    @pulumi.getter(name="modelMonitoringAlertConfig")
    def model_monitoring_alert_config(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigResponse']:
        """
        Alert config for model monitoring.
        """
        return pulumi.get(self, "model_monitoring_alert_config")

    @property
    @pulumi.getter
    def name(self) -> pulumi.Output[str]:
        """
        Resource name of a ModelDeploymentMonitoringJob.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="nextScheduleTime")
    def next_schedule_time(self) -> pulumi.Output[str]:
        """
        Timestamp when this monitoring pipeline will be scheduled to run for the next round.
        """
        return pulumi.get(self, "next_schedule_time")

    @property
    @pulumi.getter(name="predictInstanceSchemaUri")
    def predict_instance_schema_uri(self) -> pulumi.Output[str]:
        """
        YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "predict_instance_schema_uri")

    @property
    @pulumi.getter
    def project(self) -> pulumi.Output[str]:
        return pulumi.get(self, "project")

    @property
    @pulumi.getter(name="samplePredictInstance")
    def sample_predict_instance(self) -> pulumi.Output[Any]:
        """
        Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "sample_predict_instance")

    @property
    @pulumi.getter(name="scheduleState")
    def schedule_state(self) -> pulumi.Output[str]:
        """
        Schedule state when the monitoring job is in Running state.
        """
        return pulumi.get(self, "schedule_state")

    @property
    @pulumi.getter
    def state(self) -> pulumi.Output[str]:
        """
        The detailed state of the monitoring job. When the job is still creating, the state will be 'PENDING'. Once the job is successfully created, the state will be 'RUNNING'. Pause the job, the state will be 'PAUSED'. Resume the job, the state will return to 'RUNNING'.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="statsAnomaliesBaseDirectory")
    def stats_anomalies_base_directory(self) -> pulumi.Output['outputs.GoogleCloudAiplatformV1beta1GcsDestinationResponse']:
        """
        Stats anomalies base folder path.
        """
        return pulumi.get(self, "stats_anomalies_base_directory")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> pulumi.Output[str]:
        """
        Timestamp when this ModelDeploymentMonitoringJob was updated most recently.
        """
        return pulumi.get(self, "update_time")

