# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from ... import _utilities
from . import outputs
from ._enums import *

__all__ = [
    'GoogleCloudAiplatformV1ActiveLearningConfigResponse',
    'GoogleCloudAiplatformV1ArtifactResponse',
    'GoogleCloudAiplatformV1AutomaticResourcesResponse',
    'GoogleCloudAiplatformV1AutoscalingMetricSpecResponse',
    'GoogleCloudAiplatformV1BatchDedicatedResourcesResponse',
    'GoogleCloudAiplatformV1BatchPredictionJobInputConfigResponse',
    'GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigResponse',
    'GoogleCloudAiplatformV1BatchPredictionJobOutputConfigResponse',
    'GoogleCloudAiplatformV1BatchPredictionJobOutputInfoResponse',
    'GoogleCloudAiplatformV1BigQueryDestinationResponse',
    'GoogleCloudAiplatformV1BigQuerySourceResponse',
    'GoogleCloudAiplatformV1BlurBaselineConfigResponse',
    'GoogleCloudAiplatformV1CompletionStatsResponse',
    'GoogleCloudAiplatformV1ContainerSpecResponse',
    'GoogleCloudAiplatformV1ContextResponse',
    'GoogleCloudAiplatformV1CreatePipelineJobRequestResponse',
    'GoogleCloudAiplatformV1CustomJobSpecResponse',
    'GoogleCloudAiplatformV1DedicatedResourcesResponse',
    'GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse',
    'GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse',
    'GoogleCloudAiplatformV1DeployedIndexRefResponse',
    'GoogleCloudAiplatformV1DeployedIndexResponse',
    'GoogleCloudAiplatformV1DeployedModelRefResponse',
    'GoogleCloudAiplatformV1DeployedModelResponse',
    'GoogleCloudAiplatformV1DiskSpecResponse',
    'GoogleCloudAiplatformV1EncryptionSpecResponse',
    'GoogleCloudAiplatformV1EnvVarResponse',
    'GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse',
    'GoogleCloudAiplatformV1ExamplesResponse',
    'GoogleCloudAiplatformV1ExecutionResponse',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse',
    'GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse',
    'GoogleCloudAiplatformV1ExplanationMetadataResponse',
    'GoogleCloudAiplatformV1ExplanationParametersResponse',
    'GoogleCloudAiplatformV1ExplanationSpecResponse',
    'GoogleCloudAiplatformV1FeatureGroupBigQueryResponse',
    'GoogleCloudAiplatformV1FeatureMonitoringStatsAnomalyResponse',
    'GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureResponse',
    'GoogleCloudAiplatformV1FeatureNoiseSigmaResponse',
    'GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse',
    'GoogleCloudAiplatformV1FeatureOnlineStoreBigtableResponse',
    'GoogleCloudAiplatformV1FeatureStatsAnomalyResponse',
    'GoogleCloudAiplatformV1FeatureViewBigQuerySourceResponse',
    'GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse',
    'GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceResponse',
    'GoogleCloudAiplatformV1FeatureViewSyncConfigResponse',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigResponse',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse',
    'GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigResponse',
    'GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse',
    'GoogleCloudAiplatformV1FilterSplitResponse',
    'GoogleCloudAiplatformV1FractionSplitResponse',
    'GoogleCloudAiplatformV1GcsDestinationResponse',
    'GoogleCloudAiplatformV1GcsSourceResponse',
    'GoogleCloudAiplatformV1IndexPrivateEndpointsResponse',
    'GoogleCloudAiplatformV1IndexStatsResponse',
    'GoogleCloudAiplatformV1InputDataConfigResponse',
    'GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse',
    'GoogleCloudAiplatformV1MachineSpecResponse',
    'GoogleCloudAiplatformV1ManualBatchTuningParametersResponse',
    'GoogleCloudAiplatformV1MeasurementMetricResponse',
    'GoogleCloudAiplatformV1MeasurementResponse',
    'GoogleCloudAiplatformV1MetadataStoreMetadataStoreStateResponse',
    'GoogleCloudAiplatformV1ModelContainerSpecResponse',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringBigQueryTableResponse',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigResponse',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigResponse',
    'GoogleCloudAiplatformV1ModelExportFormatResponse',
    'GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse',
    'GoogleCloudAiplatformV1ModelMonitoringAlertConfigResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse',
    'GoogleCloudAiplatformV1ModelOriginalModelInfoResponse',
    'GoogleCloudAiplatformV1ModelResponse',
    'GoogleCloudAiplatformV1ModelSourceInfoResponse',
    'GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse',
    'GoogleCloudAiplatformV1NasJobOutputResponse',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse',
    'GoogleCloudAiplatformV1NasJobSpecResponse',
    'GoogleCloudAiplatformV1NasTrialResponse',
    'GoogleCloudAiplatformV1NetworkSpecResponse',
    'GoogleCloudAiplatformV1NfsMountResponse',
    'GoogleCloudAiplatformV1NotebookEucConfigResponse',
    'GoogleCloudAiplatformV1NotebookIdleShutdownConfigResponse',
    'GoogleCloudAiplatformV1PersistentDiskSpecResponse',
    'GoogleCloudAiplatformV1PipelineJobDetailResponse',
    'GoogleCloudAiplatformV1PipelineJobResponse',
    'GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse',
    'GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse',
    'GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse',
    'GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse',
    'GoogleCloudAiplatformV1PipelineTaskDetailResponse',
    'GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse',
    'GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse',
    'GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse',
    'GoogleCloudAiplatformV1PipelineTemplateMetadataResponse',
    'GoogleCloudAiplatformV1PortResponse',
    'GoogleCloudAiplatformV1PredefinedSplitResponse',
    'GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigResponse',
    'GoogleCloudAiplatformV1PredictSchemataResponse',
    'GoogleCloudAiplatformV1PresetsResponse',
    'GoogleCloudAiplatformV1PrivateEndpointsResponse',
    'GoogleCloudAiplatformV1PrivateServiceConnectConfigResponse',
    'GoogleCloudAiplatformV1ProbeExecActionResponse',
    'GoogleCloudAiplatformV1ProbeResponse',
    'GoogleCloudAiplatformV1PythonPackageSpecResponse',
    'GoogleCloudAiplatformV1ResourcesConsumedResponse',
    'GoogleCloudAiplatformV1SampleConfigResponse',
    'GoogleCloudAiplatformV1SampledShapleyAttributionResponse',
    'GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse',
    'GoogleCloudAiplatformV1SamplingStrategyResponse',
    'GoogleCloudAiplatformV1SavedQueryResponse',
    'GoogleCloudAiplatformV1ScheduleRunResponseResponse',
    'GoogleCloudAiplatformV1SchedulingResponse',
    'GoogleCloudAiplatformV1SmoothGradConfigResponse',
    'GoogleCloudAiplatformV1StratifiedSplitResponse',
    'GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse',
    'GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse',
    'GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse',
    'GoogleCloudAiplatformV1StudySpecMetricSpecResponse',
    'GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse',
    'GoogleCloudAiplatformV1StudySpecParameterSpecResponse',
    'GoogleCloudAiplatformV1StudySpecResponse',
    'GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse',
    'GoogleCloudAiplatformV1StudyTimeConstraintResponse',
    'GoogleCloudAiplatformV1TensorboardTimeSeriesMetadataResponse',
    'GoogleCloudAiplatformV1ThresholdConfigResponse',
    'GoogleCloudAiplatformV1TimestampSplitResponse',
    'GoogleCloudAiplatformV1TrainingConfigResponse',
    'GoogleCloudAiplatformV1TrialParameterResponse',
    'GoogleCloudAiplatformV1TrialResponse',
    'GoogleCloudAiplatformV1UnmanagedContainerModelResponse',
    'GoogleCloudAiplatformV1ValueResponse',
    'GoogleCloudAiplatformV1WorkerPoolSpecResponse',
    'GoogleCloudAiplatformV1XraiAttributionResponse',
    'GoogleIamV1BindingResponse',
    'GoogleRpcStatusResponse',
    'GoogleTypeExprResponse',
    'GoogleTypeMoneyResponse',
]

@pulumi.output_type
class GoogleCloudAiplatformV1ActiveLearningConfigResponse(dict):
    """
    Parameters that configure the active learning pipeline. Active learning will label the data incrementally by several iterations. For every iteration, it will select a batch of data based on the sampling strategy.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxDataItemCount":
            suggest = "max_data_item_count"
        elif key == "maxDataItemPercentage":
            suggest = "max_data_item_percentage"
        elif key == "sampleConfig":
            suggest = "sample_config"
        elif key == "trainingConfig":
            suggest = "training_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ActiveLearningConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ActiveLearningConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ActiveLearningConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_data_item_count: str,
                 max_data_item_percentage: int,
                 sample_config: 'outputs.GoogleCloudAiplatformV1SampleConfigResponse',
                 training_config: 'outputs.GoogleCloudAiplatformV1TrainingConfigResponse'):
        """
        Parameters that configure the active learning pipeline. Active learning will label the data incrementally by several iterations. For every iteration, it will select a batch of data based on the sampling strategy.
        :param str max_data_item_count: Max number of human labeled DataItems.
        :param int max_data_item_percentage: Max percent of total DataItems for human labeling.
        :param 'GoogleCloudAiplatformV1SampleConfigResponse' sample_config: Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        :param 'GoogleCloudAiplatformV1TrainingConfigResponse' training_config: CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        """
        pulumi.set(__self__, "max_data_item_count", max_data_item_count)
        pulumi.set(__self__, "max_data_item_percentage", max_data_item_percentage)
        pulumi.set(__self__, "sample_config", sample_config)
        pulumi.set(__self__, "training_config", training_config)

    @property
    @pulumi.getter(name="maxDataItemCount")
    def max_data_item_count(self) -> str:
        """
        Max number of human labeled DataItems.
        """
        return pulumi.get(self, "max_data_item_count")

    @property
    @pulumi.getter(name="maxDataItemPercentage")
    def max_data_item_percentage(self) -> int:
        """
        Max percent of total DataItems for human labeling.
        """
        return pulumi.get(self, "max_data_item_percentage")

    @property
    @pulumi.getter(name="sampleConfig")
    def sample_config(self) -> 'outputs.GoogleCloudAiplatformV1SampleConfigResponse':
        """
        Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        """
        return pulumi.get(self, "sample_config")

    @property
    @pulumi.getter(name="trainingConfig")
    def training_config(self) -> 'outputs.GoogleCloudAiplatformV1TrainingConfigResponse':
        """
        CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        """
        return pulumi.get(self, "training_config")


@pulumi.output_type
class GoogleCloudAiplatformV1ArtifactResponse(dict):
    """
    Instance of a general artifact.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "createTime":
            suggest = "create_time"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "schemaTitle":
            suggest = "schema_title"
        elif key == "schemaVersion":
            suggest = "schema_version"
        elif key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ArtifactResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ArtifactResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ArtifactResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 create_time: str,
                 description: str,
                 display_name: str,
                 etag: str,
                 labels: Mapping[str, str],
                 metadata: Mapping[str, Any],
                 name: str,
                 schema_title: str,
                 schema_version: str,
                 state: str,
                 update_time: str,
                 uri: str):
        """
        Instance of a general artifact.
        :param str create_time: Timestamp when this Artifact was created.
        :param str description: Description of the Artifact
        :param str display_name: User provided display name of the Artifact. May be up to 128 Unicode characters.
        :param str etag: An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        :param Mapping[str, str] labels: The labels with user-defined metadata to organize your Artifacts. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Artifact (System labels are excluded).
        :param Mapping[str, Any] metadata: Properties of the Artifact. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        :param str name: The resource name of the Artifact.
        :param str schema_title: The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str schema_version: The version of the schema in schema_name to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str state: The state of this Artifact. This is a property of the Artifact, and does not imply or capture any ongoing process. This property is managed by clients (such as Vertex AI Pipelines), and the system does not prescribe or check the validity of state transitions.
        :param str update_time: Timestamp when this Artifact was last updated.
        :param str uri: The uniform resource identifier of the artifact file. May be empty if there is no actual artifact file.
        """
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "schema_title", schema_title)
        pulumi.set(__self__, "schema_version", schema_version)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "update_time", update_time)
        pulumi.set(__self__, "uri", uri)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this Artifact was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Description of the Artifact
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        User provided display name of the Artifact. May be up to 128 Unicode characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize your Artifacts. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Artifact (System labels are excluded).
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter
    def metadata(self) -> Mapping[str, Any]:
        """
        Properties of the Artifact. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The resource name of the Artifact.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="schemaTitle")
    def schema_title(self) -> str:
        """
        The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_title")

    @property
    @pulumi.getter(name="schemaVersion")
    def schema_version(self) -> str:
        """
        The version of the schema in schema_name to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_version")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The state of this Artifact. This is a property of the Artifact, and does not imply or capture any ongoing process. This property is managed by clients (such as Vertex AI Pipelines), and the system does not prescribe or check the validity of state transitions.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this Artifact was last updated.
        """
        return pulumi.get(self, "update_time")

    @property
    @pulumi.getter
    def uri(self) -> str:
        """
        The uniform resource identifier of the artifact file. May be empty if there is no actual artifact file.
        """
        return pulumi.get(self, "uri")


@pulumi.output_type
class GoogleCloudAiplatformV1AutomaticResourcesResponse(dict):
    """
    A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration. Each Model supporting these resources documents its specific guidelines.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1AutomaticResourcesResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1AutomaticResourcesResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1AutomaticResourcesResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_replica_count: int,
                 min_replica_count: int):
        """
        A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration. Each Model supporting these resources documents its specific guidelines.
        :param int max_replica_count: Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        :param int min_replica_count: Immutable. The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        pulumi.set(__self__, "max_replica_count", max_replica_count)
        pulumi.set(__self__, "min_replica_count", min_replica_count)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> int:
        """
        Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
        """
        return pulumi.get(self, "max_replica_count")

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> int:
        """
        Immutable. The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to max_replica_count, and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
        """
        return pulumi.get(self, "min_replica_count")


@pulumi.output_type
class GoogleCloudAiplatformV1AutoscalingMetricSpecResponse(dict):
    """
    The metric specification that defines the target resource utilization (CPU utilization, accelerator's duty cycle, and so on) for calculating the desired replica count.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricName":
            suggest = "metric_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1AutoscalingMetricSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1AutoscalingMetricSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1AutoscalingMetricSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric_name: str,
                 target: int):
        """
        The metric specification that defines the target resource utilization (CPU utilization, accelerator's duty cycle, and so on) for calculating the desired replica count.
        :param str metric_name: The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param int target: The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        pulumi.set(__self__, "metric_name", metric_name)
        pulumi.set(__self__, "target", target)

    @property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> str:
        """
        The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @property
    @pulumi.getter
    def target(self) -> int:
        """
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")


@pulumi.output_type
class GoogleCloudAiplatformV1BatchDedicatedResourcesResponse(dict):
    """
    A description of resources that are used for performing batch operations, are dedicated to a Model, and need manual configuration.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "machineSpec":
            suggest = "machine_spec"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "startingReplicaCount":
            suggest = "starting_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BatchDedicatedResourcesResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BatchDedicatedResourcesResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BatchDedicatedResourcesResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 machine_spec: 'outputs.GoogleCloudAiplatformV1MachineSpecResponse',
                 max_replica_count: int,
                 starting_replica_count: int):
        """
        A description of resources that are used for performing batch operations, are dedicated to a Model, and need manual configuration.
        :param 'GoogleCloudAiplatformV1MachineSpecResponse' machine_spec: Immutable. The specification of a single machine.
        :param int max_replica_count: Immutable. The maximum number of machine replicas the batch operation may be scaled to. The default value is 10.
        :param int starting_replica_count: Immutable. The number of machine replicas used at the start of the batch operation. If not set, Vertex AI decides starting number, not greater than max_replica_count
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "max_replica_count", max_replica_count)
        pulumi.set(__self__, "starting_replica_count", starting_replica_count)

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.GoogleCloudAiplatformV1MachineSpecResponse':
        """
        Immutable. The specification of a single machine.
        """
        return pulumi.get(self, "machine_spec")

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> int:
        """
        Immutable. The maximum number of machine replicas the batch operation may be scaled to. The default value is 10.
        """
        return pulumi.get(self, "max_replica_count")

    @property
    @pulumi.getter(name="startingReplicaCount")
    def starting_replica_count(self) -> int:
        """
        Immutable. The number of machine replicas used at the start of the batch operation. If not set, Vertex AI decides starting number, not greater than max_replica_count
        """
        return pulumi.get(self, "starting_replica_count")


@pulumi.output_type
class GoogleCloudAiplatformV1BatchPredictionJobInputConfigResponse(dict):
    """
    Configures the input to BatchPredictionJob. See Model.supported_input_storage_formats for Model's supported input formats, and how instances should be expressed via any of them.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigquerySource":
            suggest = "bigquery_source"
        elif key == "gcsSource":
            suggest = "gcs_source"
        elif key == "instancesFormat":
            suggest = "instances_format"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BatchPredictionJobInputConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobInputConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobInputConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_source: 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse',
                 gcs_source: 'outputs.GoogleCloudAiplatformV1GcsSourceResponse',
                 instances_format: str):
        """
        Configures the input to BatchPredictionJob. See Model.supported_input_storage_formats for Model's supported input formats, and how instances should be expressed via any of them.
        :param 'GoogleCloudAiplatformV1BigQuerySourceResponse' bigquery_source: The BigQuery location of the input table. The schema of the table should be in the format described by the given context OpenAPI Schema, if one is provided. The table may contain additional columns that are not described by the schema, and they will be ignored.
        :param 'GoogleCloudAiplatformV1GcsSourceResponse' gcs_source: The Cloud Storage location for the input instances.
        :param str instances_format: The format in which instances are given, must be one of the Model's supported_input_storage_formats.
        """
        pulumi.set(__self__, "bigquery_source", bigquery_source)
        pulumi.set(__self__, "gcs_source", gcs_source)
        pulumi.set(__self__, "instances_format", instances_format)

    @property
    @pulumi.getter(name="bigquerySource")
    def bigquery_source(self) -> 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse':
        """
        The BigQuery location of the input table. The schema of the table should be in the format described by the given context OpenAPI Schema, if one is provided. The table may contain additional columns that are not described by the schema, and they will be ignored.
        """
        return pulumi.get(self, "bigquery_source")

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> 'outputs.GoogleCloudAiplatformV1GcsSourceResponse':
        """
        The Cloud Storage location for the input instances.
        """
        return pulumi.get(self, "gcs_source")

    @property
    @pulumi.getter(name="instancesFormat")
    def instances_format(self) -> str:
        """
        The format in which instances are given, must be one of the Model's supported_input_storage_formats.
        """
        return pulumi.get(self, "instances_format")


@pulumi.output_type
class GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigResponse(dict):
    """
    Configuration defining how to transform batch prediction input instances to the instances that the Model accepts.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "excludedFields":
            suggest = "excluded_fields"
        elif key == "includedFields":
            suggest = "included_fields"
        elif key == "instanceType":
            suggest = "instance_type"
        elif key == "keyField":
            suggest = "key_field"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 excluded_fields: Sequence[str],
                 included_fields: Sequence[str],
                 instance_type: str,
                 key_field: str):
        """
        Configuration defining how to transform batch prediction input instances to the instances that the Model accepts.
        :param Sequence[str] excluded_fields: Fields that will be excluded in the prediction instance that is sent to the Model. Excluded will be attached to the batch prediction output if key_field is not specified. When excluded_fields is populated, included_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        :param Sequence[str] included_fields: Fields that will be included in the prediction instance that is sent to the Model. If instance_type is `array`, the order of field names in included_fields also determines the order of the values in the array. When included_fields is populated, excluded_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        :param str instance_type: The format of the instance that the Model accepts. Vertex AI will convert compatible batch prediction input instance formats to the specified format. Supported values are: * `object`: Each input is converted to JSON object format. * For `bigquery`, each row is converted to an object. * For `jsonl`, each line of the JSONL input must be an object. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. * `array`: Each input is converted to JSON array format. * For `bigquery`, each row is converted to an array. The order of columns is determined by the BigQuery column order, unless included_fields is populated. included_fields must be populated for specifying field orders. * For `jsonl`, if each line of the JSONL input is an object, included_fields must be populated for specifying field orders. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. If not specified, Vertex AI converts the batch prediction input as follows: * For `bigquery` and `csv`, the behavior is the same as `array`. The order of columns is the same as defined in the file or table, unless included_fields is populated. * For `jsonl`, the prediction instance format is determined by each line of the input. * For `tf-record`/`tf-record-gzip`, each record will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the record. * For `file-list`, each file in the list will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the file.
        :param str key_field: The name of the field that is considered as a key. The values identified by the key field is not included in the transformed instances that is sent to the Model. This is similar to specifying this name of the field in excluded_fields. In addition, the batch prediction output will not include the instances. Instead the output will only include the value of the key field, in a field named `key` in the output: * For `jsonl` output format, the output will have a `key` field instead of the `instance` field. * For `csv`/`bigquery` output format, the output will have have a `key` column instead of the instance feature columns. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        pulumi.set(__self__, "excluded_fields", excluded_fields)
        pulumi.set(__self__, "included_fields", included_fields)
        pulumi.set(__self__, "instance_type", instance_type)
        pulumi.set(__self__, "key_field", key_field)

    @property
    @pulumi.getter(name="excludedFields")
    def excluded_fields(self) -> Sequence[str]:
        """
        Fields that will be excluded in the prediction instance that is sent to the Model. Excluded will be attached to the batch prediction output if key_field is not specified. When excluded_fields is populated, included_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "excluded_fields")

    @property
    @pulumi.getter(name="includedFields")
    def included_fields(self) -> Sequence[str]:
        """
        Fields that will be included in the prediction instance that is sent to the Model. If instance_type is `array`, the order of field names in included_fields also determines the order of the values in the array. When included_fields is populated, excluded_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "included_fields")

    @property
    @pulumi.getter(name="instanceType")
    def instance_type(self) -> str:
        """
        The format of the instance that the Model accepts. Vertex AI will convert compatible batch prediction input instance formats to the specified format. Supported values are: * `object`: Each input is converted to JSON object format. * For `bigquery`, each row is converted to an object. * For `jsonl`, each line of the JSONL input must be an object. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. * `array`: Each input is converted to JSON array format. * For `bigquery`, each row is converted to an array. The order of columns is determined by the BigQuery column order, unless included_fields is populated. included_fields must be populated for specifying field orders. * For `jsonl`, if each line of the JSONL input is an object, included_fields must be populated for specifying field orders. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. If not specified, Vertex AI converts the batch prediction input as follows: * For `bigquery` and `csv`, the behavior is the same as `array`. The order of columns is the same as defined in the file or table, unless included_fields is populated. * For `jsonl`, the prediction instance format is determined by each line of the input. * For `tf-record`/`tf-record-gzip`, each record will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the record. * For `file-list`, each file in the list will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the file.
        """
        return pulumi.get(self, "instance_type")

    @property
    @pulumi.getter(name="keyField")
    def key_field(self) -> str:
        """
        The name of the field that is considered as a key. The values identified by the key field is not included in the transformed instances that is sent to the Model. This is similar to specifying this name of the field in excluded_fields. In addition, the batch prediction output will not include the instances. Instead the output will only include the value of the key field, in a field named `key` in the output: * For `jsonl` output format, the output will have a `key` field instead of the `instance` field. * For `csv`/`bigquery` output format, the output will have have a `key` column instead of the instance feature columns. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "key_field")


@pulumi.output_type
class GoogleCloudAiplatformV1BatchPredictionJobOutputConfigResponse(dict):
    """
    Configures the output of BatchPredictionJob. See Model.supported_output_storage_formats for supported output formats, and how predictions are expressed via any of them.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryDestination":
            suggest = "bigquery_destination"
        elif key == "gcsDestination":
            suggest = "gcs_destination"
        elif key == "predictionsFormat":
            suggest = "predictions_format"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BatchPredictionJobOutputConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobOutputConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobOutputConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_destination: 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse',
                 gcs_destination: 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse',
                 predictions_format: str):
        """
        Configures the output of BatchPredictionJob. See Model.supported_output_storage_formats for supported output formats, and how predictions are expressed via any of them.
        :param 'GoogleCloudAiplatformV1BigQueryDestinationResponse' bigquery_destination: The BigQuery project or dataset location where the output is to be written to. If project is provided, a new dataset is created with name `prediction__` where is made BigQuery-dataset-name compatible (for example, most special characters become underscores), and timestamp is in YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two tables will be created, `predictions`, and `errors`. If the Model has both instance and prediction schemata defined then the tables have columns as follows: The `predictions` table contains instances for which the prediction succeeded, it has columns as per a concatenation of the Model's instance and prediction schemata. The `errors` table contains rows for which the prediction has failed, it has instance columns, as per the instance schema, followed by a single "errors" column, which as values has google.rpc.Status represented as a STRUCT, and containing only `code` and `message`.
        :param 'GoogleCloudAiplatformV1GcsDestinationResponse' gcs_destination: The Cloud Storage location of the directory where the output is to be written to. In the given directory a new directory is created. Its name is `prediction--`, where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files `predictions_0001.`, `predictions_0002.`, ..., `predictions_N.` are created where `` depends on chosen predictions_format, and N may equal 0001 and depends on the total number of successfully predicted instances. If the Model has both instance and prediction schemata defined then each such file contains predictions as per the predictions_format. If prediction for any instance failed (partially or completely), then an additional `errors_0001.`, `errors_0002.`,..., `errors_N.` files are created (N depends on total number of failed predictions). These files contain the failed instances, as per their schema, followed by an additional `error` field which as value has google.rpc.Status containing only `code` and `message` fields.
        :param str predictions_format: The format in which Vertex AI gives the predictions, must be one of the Model's supported_output_storage_formats.
        """
        pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        pulumi.set(__self__, "gcs_destination", gcs_destination)
        pulumi.set(__self__, "predictions_format", predictions_format)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse':
        """
        The BigQuery project or dataset location where the output is to be written to. If project is provided, a new dataset is created with name `prediction__` where is made BigQuery-dataset-name compatible (for example, most special characters become underscores), and timestamp is in YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two tables will be created, `predictions`, and `errors`. If the Model has both instance and prediction schemata defined then the tables have columns as follows: The `predictions` table contains instances for which the prediction succeeded, it has columns as per a concatenation of the Model's instance and prediction schemata. The `errors` table contains rows for which the prediction has failed, it has instance columns, as per the instance schema, followed by a single "errors" column, which as values has google.rpc.Status represented as a STRUCT, and containing only `code` and `message`.
        """
        return pulumi.get(self, "bigquery_destination")

    @property
    @pulumi.getter(name="gcsDestination")
    def gcs_destination(self) -> 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse':
        """
        The Cloud Storage location of the directory where the output is to be written to. In the given directory a new directory is created. Its name is `prediction--`, where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files `predictions_0001.`, `predictions_0002.`, ..., `predictions_N.` are created where `` depends on chosen predictions_format, and N may equal 0001 and depends on the total number of successfully predicted instances. If the Model has both instance and prediction schemata defined then each such file contains predictions as per the predictions_format. If prediction for any instance failed (partially or completely), then an additional `errors_0001.`, `errors_0002.`,..., `errors_N.` files are created (N depends on total number of failed predictions). These files contain the failed instances, as per their schema, followed by an additional `error` field which as value has google.rpc.Status containing only `code` and `message` fields.
        """
        return pulumi.get(self, "gcs_destination")

    @property
    @pulumi.getter(name="predictionsFormat")
    def predictions_format(self) -> str:
        """
        The format in which Vertex AI gives the predictions, must be one of the Model's supported_output_storage_formats.
        """
        return pulumi.get(self, "predictions_format")


@pulumi.output_type
class GoogleCloudAiplatformV1BatchPredictionJobOutputInfoResponse(dict):
    """
    Further describes this job's output. Supplements output_config.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryOutputDataset":
            suggest = "bigquery_output_dataset"
        elif key == "bigqueryOutputTable":
            suggest = "bigquery_output_table"
        elif key == "gcsOutputDirectory":
            suggest = "gcs_output_directory"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BatchPredictionJobOutputInfoResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobOutputInfoResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BatchPredictionJobOutputInfoResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_output_dataset: str,
                 bigquery_output_table: str,
                 gcs_output_directory: str):
        """
        Further describes this job's output. Supplements output_config.
        :param str bigquery_output_dataset: The path of the BigQuery dataset created, in `bq://projectId.bqDatasetId` format, into which the prediction output is written.
        :param str bigquery_output_table: The name of the BigQuery table created, in `predictions_` format, into which the prediction output is written. Can be used by UI to generate the BigQuery output path, for example.
        :param str gcs_output_directory: The full path of the Cloud Storage directory created, into which the prediction output is written.
        """
        pulumi.set(__self__, "bigquery_output_dataset", bigquery_output_dataset)
        pulumi.set(__self__, "bigquery_output_table", bigquery_output_table)
        pulumi.set(__self__, "gcs_output_directory", gcs_output_directory)

    @property
    @pulumi.getter(name="bigqueryOutputDataset")
    def bigquery_output_dataset(self) -> str:
        """
        The path of the BigQuery dataset created, in `bq://projectId.bqDatasetId` format, into which the prediction output is written.
        """
        return pulumi.get(self, "bigquery_output_dataset")

    @property
    @pulumi.getter(name="bigqueryOutputTable")
    def bigquery_output_table(self) -> str:
        """
        The name of the BigQuery table created, in `predictions_` format, into which the prediction output is written. Can be used by UI to generate the BigQuery output path, for example.
        """
        return pulumi.get(self, "bigquery_output_table")

    @property
    @pulumi.getter(name="gcsOutputDirectory")
    def gcs_output_directory(self) -> str:
        """
        The full path of the Cloud Storage directory created, into which the prediction output is written.
        """
        return pulumi.get(self, "gcs_output_directory")


@pulumi.output_type
class GoogleCloudAiplatformV1BigQueryDestinationResponse(dict):
    """
    The BigQuery location for the output content.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "outputUri":
            suggest = "output_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BigQueryDestinationResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BigQueryDestinationResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BigQueryDestinationResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 output_uri: str):
        """
        The BigQuery location for the output content.
        :param str output_uri: BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        pulumi.set(__self__, "output_uri", output_uri)

    @property
    @pulumi.getter(name="outputUri")
    def output_uri(self) -> str:
        """
        BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        return pulumi.get(self, "output_uri")


@pulumi.output_type
class GoogleCloudAiplatformV1BigQuerySourceResponse(dict):
    """
    The BigQuery location for the input content.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "inputUri":
            suggest = "input_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BigQuerySourceResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BigQuerySourceResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BigQuerySourceResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 input_uri: str):
        """
        The BigQuery location for the input content.
        :param str input_uri: BigQuery URI to a table, up to 2000 characters long. Accepted forms: * BigQuery path. For example: `bq://projectId.bqDatasetId.bqTableId`.
        """
        pulumi.set(__self__, "input_uri", input_uri)

    @property
    @pulumi.getter(name="inputUri")
    def input_uri(self) -> str:
        """
        BigQuery URI to a table, up to 2000 characters long. Accepted forms: * BigQuery path. For example: `bq://projectId.bqDatasetId.bqTableId`.
        """
        return pulumi.get(self, "input_uri")


@pulumi.output_type
class GoogleCloudAiplatformV1BlurBaselineConfigResponse(dict):
    """
    Config for blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxBlurSigma":
            suggest = "max_blur_sigma"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1BlurBaselineConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1BlurBaselineConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1BlurBaselineConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_blur_sigma: float):
        """
        Config for blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param float max_blur_sigma: The standard deviation of the blur kernel for the blurred baseline. The same blurring parameter is used for both the height and the width dimension. If not set, the method defaults to the zero (i.e. black for images) baseline.
        """
        pulumi.set(__self__, "max_blur_sigma", max_blur_sigma)

    @property
    @pulumi.getter(name="maxBlurSigma")
    def max_blur_sigma(self) -> float:
        """
        The standard deviation of the blur kernel for the blurred baseline. The same blurring parameter is used for both the height and the width dimension. If not set, the method defaults to the zero (i.e. black for images) baseline.
        """
        return pulumi.get(self, "max_blur_sigma")


@pulumi.output_type
class GoogleCloudAiplatformV1CompletionStatsResponse(dict):
    """
    Success and error statistics of processing multiple entities (for example, DataItems or structured data rows) in batch.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "failedCount":
            suggest = "failed_count"
        elif key == "incompleteCount":
            suggest = "incomplete_count"
        elif key == "successfulCount":
            suggest = "successful_count"
        elif key == "successfulForecastPointCount":
            suggest = "successful_forecast_point_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1CompletionStatsResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1CompletionStatsResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1CompletionStatsResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 failed_count: str,
                 incomplete_count: str,
                 successful_count: str,
                 successful_forecast_point_count: str):
        """
        Success and error statistics of processing multiple entities (for example, DataItems or structured data rows) in batch.
        :param str failed_count: The number of entities for which any error was encountered.
        :param str incomplete_count: In cases when enough errors are encountered a job, pipeline, or operation may be failed as a whole. Below is the number of entities for which the processing had not been finished (either in successful or failed state). Set to -1 if the number is unknown (for example, the operation failed before the total entity number could be collected).
        :param str successful_count: The number of entities that had been processed successfully.
        :param str successful_forecast_point_count: The number of the successful forecast points that are generated by the forecasting model. This is ONLY used by the forecasting batch prediction.
        """
        pulumi.set(__self__, "failed_count", failed_count)
        pulumi.set(__self__, "incomplete_count", incomplete_count)
        pulumi.set(__self__, "successful_count", successful_count)
        pulumi.set(__self__, "successful_forecast_point_count", successful_forecast_point_count)

    @property
    @pulumi.getter(name="failedCount")
    def failed_count(self) -> str:
        """
        The number of entities for which any error was encountered.
        """
        return pulumi.get(self, "failed_count")

    @property
    @pulumi.getter(name="incompleteCount")
    def incomplete_count(self) -> str:
        """
        In cases when enough errors are encountered a job, pipeline, or operation may be failed as a whole. Below is the number of entities for which the processing had not been finished (either in successful or failed state). Set to -1 if the number is unknown (for example, the operation failed before the total entity number could be collected).
        """
        return pulumi.get(self, "incomplete_count")

    @property
    @pulumi.getter(name="successfulCount")
    def successful_count(self) -> str:
        """
        The number of entities that had been processed successfully.
        """
        return pulumi.get(self, "successful_count")

    @property
    @pulumi.getter(name="successfulForecastPointCount")
    def successful_forecast_point_count(self) -> str:
        """
        The number of the successful forecast points that are generated by the forecasting model. This is ONLY used by the forecasting batch prediction.
        """
        return pulumi.get(self, "successful_forecast_point_count")


@pulumi.output_type
class GoogleCloudAiplatformV1ContainerSpecResponse(dict):
    """
    The spec of a Container.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "imageUri":
            suggest = "image_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ContainerSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ContainerSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ContainerSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 args: Sequence[str],
                 command: Sequence[str],
                 env: Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse'],
                 image_uri: str):
        """
        The spec of a Container.
        :param Sequence[str] args: The arguments to be passed when starting the container.
        :param Sequence[str] command: The command to be invoked when the container is started. It overrides the entrypoint instruction in Dockerfile when provided.
        :param Sequence['GoogleCloudAiplatformV1EnvVarResponse'] env: Environment variables to be passed to the container. Maximum limit is 100.
        :param str image_uri: The URI of a container image in the Container Registry that is to be run on each worker replica.
        """
        pulumi.set(__self__, "args", args)
        pulumi.set(__self__, "command", command)
        pulumi.set(__self__, "env", env)
        pulumi.set(__self__, "image_uri", image_uri)

    @property
    @pulumi.getter
    def args(self) -> Sequence[str]:
        """
        The arguments to be passed when starting the container.
        """
        return pulumi.get(self, "args")

    @property
    @pulumi.getter
    def command(self) -> Sequence[str]:
        """
        The command to be invoked when the container is started. It overrides the entrypoint instruction in Dockerfile when provided.
        """
        return pulumi.get(self, "command")

    @property
    @pulumi.getter
    def env(self) -> Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse']:
        """
        Environment variables to be passed to the container. Maximum limit is 100.
        """
        return pulumi.get(self, "env")

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> str:
        """
        The URI of a container image in the Container Registry that is to be run on each worker replica.
        """
        return pulumi.get(self, "image_uri")


@pulumi.output_type
class GoogleCloudAiplatformV1ContextResponse(dict):
    """
    Instance of a general context.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "createTime":
            suggest = "create_time"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "parentContexts":
            suggest = "parent_contexts"
        elif key == "schemaTitle":
            suggest = "schema_title"
        elif key == "schemaVersion":
            suggest = "schema_version"
        elif key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ContextResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ContextResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ContextResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 create_time: str,
                 description: str,
                 display_name: str,
                 etag: str,
                 labels: Mapping[str, str],
                 metadata: Mapping[str, Any],
                 name: str,
                 parent_contexts: Sequence[str],
                 schema_title: str,
                 schema_version: str,
                 update_time: str):
        """
        Instance of a general context.
        :param str create_time: Timestamp when this Context was created.
        :param str description: Description of the Context
        :param str display_name: User provided display name of the Context. May be up to 128 Unicode characters.
        :param str etag: An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        :param Mapping[str, str] labels: The labels with user-defined metadata to organize your Contexts. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Context (System labels are excluded).
        :param Mapping[str, Any] metadata: Properties of the Context. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        :param str name: Immutable. The resource name of the Context.
        :param Sequence[str] parent_contexts: A list of resource names of Contexts that are parents of this Context. A Context may have at most 10 parent_contexts.
        :param str schema_title: The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str schema_version: The version of the schema in schema_name to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str update_time: Timestamp when this Context was last updated.
        """
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "parent_contexts", parent_contexts)
        pulumi.set(__self__, "schema_title", schema_title)
        pulumi.set(__self__, "schema_version", schema_version)
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this Context was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Description of the Context
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        User provided display name of the Context. May be up to 128 Unicode characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize your Contexts. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Context (System labels are excluded).
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter
    def metadata(self) -> Mapping[str, Any]:
        """
        Properties of the Context. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Immutable. The resource name of the Context.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="parentContexts")
    def parent_contexts(self) -> Sequence[str]:
        """
        A list of resource names of Contexts that are parents of this Context. A Context may have at most 10 parent_contexts.
        """
        return pulumi.get(self, "parent_contexts")

    @property
    @pulumi.getter(name="schemaTitle")
    def schema_title(self) -> str:
        """
        The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_title")

    @property
    @pulumi.getter(name="schemaVersion")
    def schema_version(self) -> str:
        """
        The version of the schema in schema_name to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_version")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this Context was last updated.
        """
        return pulumi.get(self, "update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1CreatePipelineJobRequestResponse(dict):
    """
    Request message for PipelineService.CreatePipelineJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "pipelineJob":
            suggest = "pipeline_job"
        elif key == "pipelineJobId":
            suggest = "pipeline_job_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1CreatePipelineJobRequestResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1CreatePipelineJobRequestResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1CreatePipelineJobRequestResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 parent: str,
                 pipeline_job: 'outputs.GoogleCloudAiplatformV1PipelineJobResponse',
                 pipeline_job_id: str):
        """
        Request message for PipelineService.CreatePipelineJob.
        :param str parent: The resource name of the Location to create the PipelineJob in. Format: `projects/{project}/locations/{location}`
        :param 'GoogleCloudAiplatformV1PipelineJobResponse' pipeline_job: The PipelineJob to create.
        :param str pipeline_job_id: The ID to use for the PipelineJob, which will become the final component of the PipelineJob name. If not provided, an ID will be automatically generated. This value should be less than 128 characters, and valid characters are `/a-z-/`.
        """
        pulumi.set(__self__, "parent", parent)
        pulumi.set(__self__, "pipeline_job", pipeline_job)
        pulumi.set(__self__, "pipeline_job_id", pipeline_job_id)

    @property
    @pulumi.getter
    def parent(self) -> str:
        """
        The resource name of the Location to create the PipelineJob in. Format: `projects/{project}/locations/{location}`
        """
        return pulumi.get(self, "parent")

    @property
    @pulumi.getter(name="pipelineJob")
    def pipeline_job(self) -> 'outputs.GoogleCloudAiplatformV1PipelineJobResponse':
        """
        The PipelineJob to create.
        """
        return pulumi.get(self, "pipeline_job")

    @property
    @pulumi.getter(name="pipelineJobId")
    def pipeline_job_id(self) -> str:
        """
        The ID to use for the PipelineJob, which will become the final component of the PipelineJob name. If not provided, an ID will be automatically generated. This value should be less than 128 characters, and valid characters are `/a-z-/`.
        """
        return pulumi.get(self, "pipeline_job_id")


@pulumi.output_type
class GoogleCloudAiplatformV1CustomJobSpecResponse(dict):
    """
    Represents the spec of a CustomJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "baseOutputDirectory":
            suggest = "base_output_directory"
        elif key == "enableDashboardAccess":
            suggest = "enable_dashboard_access"
        elif key == "enableWebAccess":
            suggest = "enable_web_access"
        elif key == "experimentRun":
            suggest = "experiment_run"
        elif key == "protectedArtifactLocationId":
            suggest = "protected_artifact_location_id"
        elif key == "reservedIpRanges":
            suggest = "reserved_ip_ranges"
        elif key == "serviceAccount":
            suggest = "service_account"
        elif key == "workerPoolSpecs":
            suggest = "worker_pool_specs"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1CustomJobSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1CustomJobSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1CustomJobSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 base_output_directory: 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse',
                 enable_dashboard_access: bool,
                 enable_web_access: bool,
                 experiment: str,
                 experiment_run: str,
                 network: str,
                 protected_artifact_location_id: str,
                 reserved_ip_ranges: Sequence[str],
                 scheduling: 'outputs.GoogleCloudAiplatformV1SchedulingResponse',
                 service_account: str,
                 tensorboard: str,
                 worker_pool_specs: Sequence['outputs.GoogleCloudAiplatformV1WorkerPoolSpecResponse']):
        """
        Represents the spec of a CustomJob.
        :param 'GoogleCloudAiplatformV1GcsDestinationResponse' base_output_directory: The Cloud Storage location to store the output of this CustomJob or HyperparameterTuningJob. For HyperparameterTuningJob, the baseOutputDirectory of each child CustomJob backing a Trial is set to a subdirectory of name id under its parent HyperparameterTuningJob's baseOutputDirectory. The following Vertex AI environment variables will be passed to containers or python modules when this field is set: For CustomJob: * AIP_MODEL_DIR = `/model/` * AIP_CHECKPOINT_DIR = `/checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `/logs/` For CustomJob backing a Trial of HyperparameterTuningJob: * AIP_MODEL_DIR = `//model/` * AIP_CHECKPOINT_DIR = `//checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `//logs/`
        :param bool enable_dashboard_access: Optional. Whether you want Vertex AI to enable access to the customized dashboard in training chief container. If set to `true`, you can access the dashboard at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        :param bool enable_web_access: Optional. Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        :param str experiment: Optional. The Experiment associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}`
        :param str experiment_run: Optional. The Experiment Run associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}-{experiment-run-name}`
        :param str network: Optional. The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Job should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. To specify this field, you must have already [configured VPC Network Peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering). If this field is left unspecified, the job is not peered with any network.
        :param str protected_artifact_location_id: The ID of the location to store protected artifacts. e.g. us-central1. Populate only when the location is different than CustomJob location. List of supported locations: https://cloud.google.com/vertex-ai/docs/general/locations
        :param Sequence[str] reserved_ip_ranges: Optional. A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        :param 'GoogleCloudAiplatformV1SchedulingResponse' scheduling: Scheduling options for a CustomJob.
        :param str service_account: Specifies the service account for workload run-as account. Users submitting jobs must have act-as permission on this run-as account. If unspecified, the [Vertex AI Custom Code Service Agent](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) for the CustomJob's project is used.
        :param str tensorboard: Optional. The name of a Vertex AI Tensorboard resource to which this CustomJob will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
        :param Sequence['GoogleCloudAiplatformV1WorkerPoolSpecResponse'] worker_pool_specs: The spec of the worker pools including machine type and Docker image. All worker pools except the first one are optional and can be skipped by providing an empty value.
        """
        pulumi.set(__self__, "base_output_directory", base_output_directory)
        pulumi.set(__self__, "enable_dashboard_access", enable_dashboard_access)
        pulumi.set(__self__, "enable_web_access", enable_web_access)
        pulumi.set(__self__, "experiment", experiment)
        pulumi.set(__self__, "experiment_run", experiment_run)
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "protected_artifact_location_id", protected_artifact_location_id)
        pulumi.set(__self__, "reserved_ip_ranges", reserved_ip_ranges)
        pulumi.set(__self__, "scheduling", scheduling)
        pulumi.set(__self__, "service_account", service_account)
        pulumi.set(__self__, "tensorboard", tensorboard)
        pulumi.set(__self__, "worker_pool_specs", worker_pool_specs)

    @property
    @pulumi.getter(name="baseOutputDirectory")
    def base_output_directory(self) -> 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse':
        """
        The Cloud Storage location to store the output of this CustomJob or HyperparameterTuningJob. For HyperparameterTuningJob, the baseOutputDirectory of each child CustomJob backing a Trial is set to a subdirectory of name id under its parent HyperparameterTuningJob's baseOutputDirectory. The following Vertex AI environment variables will be passed to containers or python modules when this field is set: For CustomJob: * AIP_MODEL_DIR = `/model/` * AIP_CHECKPOINT_DIR = `/checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `/logs/` For CustomJob backing a Trial of HyperparameterTuningJob: * AIP_MODEL_DIR = `//model/` * AIP_CHECKPOINT_DIR = `//checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `//logs/`
        """
        return pulumi.get(self, "base_output_directory")

    @property
    @pulumi.getter(name="enableDashboardAccess")
    def enable_dashboard_access(self) -> bool:
        """
        Optional. Whether you want Vertex AI to enable access to the customized dashboard in training chief container. If set to `true`, you can access the dashboard at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        """
        return pulumi.get(self, "enable_dashboard_access")

    @property
    @pulumi.getter(name="enableWebAccess")
    def enable_web_access(self) -> bool:
        """
        Optional. Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        """
        return pulumi.get(self, "enable_web_access")

    @property
    @pulumi.getter
    def experiment(self) -> str:
        """
        Optional. The Experiment associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}`
        """
        return pulumi.get(self, "experiment")

    @property
    @pulumi.getter(name="experimentRun")
    def experiment_run(self) -> str:
        """
        Optional. The Experiment Run associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}-{experiment-run-name}`
        """
        return pulumi.get(self, "experiment_run")

    @property
    @pulumi.getter
    def network(self) -> str:
        """
        Optional. The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Job should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. To specify this field, you must have already [configured VPC Network Peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering). If this field is left unspecified, the job is not peered with any network.
        """
        return pulumi.get(self, "network")

    @property
    @pulumi.getter(name="protectedArtifactLocationId")
    def protected_artifact_location_id(self) -> str:
        """
        The ID of the location to store protected artifacts. e.g. us-central1. Populate only when the location is different than CustomJob location. List of supported locations: https://cloud.google.com/vertex-ai/docs/general/locations
        """
        return pulumi.get(self, "protected_artifact_location_id")

    @property
    @pulumi.getter(name="reservedIpRanges")
    def reserved_ip_ranges(self) -> Sequence[str]:
        """
        Optional. A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        """
        return pulumi.get(self, "reserved_ip_ranges")

    @property
    @pulumi.getter
    def scheduling(self) -> 'outputs.GoogleCloudAiplatformV1SchedulingResponse':
        """
        Scheduling options for a CustomJob.
        """
        return pulumi.get(self, "scheduling")

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> str:
        """
        Specifies the service account for workload run-as account. Users submitting jobs must have act-as permission on this run-as account. If unspecified, the [Vertex AI Custom Code Service Agent](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) for the CustomJob's project is used.
        """
        return pulumi.get(self, "service_account")

    @property
    @pulumi.getter
    def tensorboard(self) -> str:
        """
        Optional. The name of a Vertex AI Tensorboard resource to which this CustomJob will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
        """
        return pulumi.get(self, "tensorboard")

    @property
    @pulumi.getter(name="workerPoolSpecs")
    def worker_pool_specs(self) -> Sequence['outputs.GoogleCloudAiplatformV1WorkerPoolSpecResponse']:
        """
        The spec of the worker pools including machine type and Docker image. All worker pools except the first one are optional and can be skipped by providing an empty value.
        """
        return pulumi.get(self, "worker_pool_specs")


@pulumi.output_type
class GoogleCloudAiplatformV1DedicatedResourcesResponse(dict):
    """
    A description of resources that are dedicated to a DeployedModel, and that need a higher degree of manual configuration.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "autoscalingMetricSpecs":
            suggest = "autoscaling_metric_specs"
        elif key == "machineSpec":
            suggest = "machine_spec"
        elif key == "maxReplicaCount":
            suggest = "max_replica_count"
        elif key == "minReplicaCount":
            suggest = "min_replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DedicatedResourcesResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DedicatedResourcesResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DedicatedResourcesResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 autoscaling_metric_specs: Sequence['outputs.GoogleCloudAiplatformV1AutoscalingMetricSpecResponse'],
                 machine_spec: 'outputs.GoogleCloudAiplatformV1MachineSpecResponse',
                 max_replica_count: int,
                 min_replica_count: int):
        """
        A description of resources that are dedicated to a DeployedModel, and that need a higher degree of manual configuration.
        :param Sequence['GoogleCloudAiplatformV1AutoscalingMetricSpecResponse'] autoscaling_metric_specs: Immutable. The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        :param 'GoogleCloudAiplatformV1MachineSpecResponse' machine_spec: Immutable. The specification of a single machine used by the prediction.
        :param int max_replica_count: Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for (max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        :param int min_replica_count: Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "max_replica_count", max_replica_count)
        pulumi.set(__self__, "min_replica_count", min_replica_count)

    @property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Sequence['outputs.GoogleCloudAiplatformV1AutoscalingMetricSpecResponse']:
        """
        Immutable. The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.GoogleCloudAiplatformV1MachineSpecResponse':
        """
        Immutable. The specification of a single machine used by the prediction.
        """
        return pulumi.get(self, "machine_spec")

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> int:
        """
        Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for (max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        return pulumi.get(self, "max_replica_count")

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> int:
        """
        Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        return pulumi.get(self, "min_replica_count")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse(dict):
    """
    Configuration for an authentication provider, including support for [JSON Web Token (JWT)](https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allowedIssuers":
            suggest = "allowed_issuers"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 allowed_issuers: Sequence[str],
                 audiences: Sequence[str]):
        """
        Configuration for an authentication provider, including support for [JSON Web Token (JWT)](https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32).
        :param Sequence[str] allowed_issuers: A list of allowed JWT issuers. Each entry must be a valid Google service account, in the following format: `service-account-name@project-id.iam.gserviceaccount.com`
        :param Sequence[str] audiences: The list of JWT [audiences](https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32#section-4.1.3). that are allowed to access. A JWT containing any of these audiences will be accepted.
        """
        pulumi.set(__self__, "allowed_issuers", allowed_issuers)
        pulumi.set(__self__, "audiences", audiences)

    @property
    @pulumi.getter(name="allowedIssuers")
    def allowed_issuers(self) -> Sequence[str]:
        """
        A list of allowed JWT issuers. Each entry must be a valid Google service account, in the following format: `service-account-name@project-id.iam.gserviceaccount.com`
        """
        return pulumi.get(self, "allowed_issuers")

    @property
    @pulumi.getter
    def audiences(self) -> Sequence[str]:
        """
        The list of JWT [audiences](https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32#section-4.1.3). that are allowed to access. A JWT containing any of these audiences will be accepted.
        """
        return pulumi.get(self, "audiences")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse(dict):
    """
    Used to set up the auth on the DeployedIndex's private endpoint.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "authProvider":
            suggest = "auth_provider"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 auth_provider: 'outputs.GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse'):
        """
        Used to set up the auth on the DeployedIndex's private endpoint.
        :param 'GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse' auth_provider: Defines the authentication provider that the DeployedIndex uses.
        """
        pulumi.set(__self__, "auth_provider", auth_provider)

    @property
    @pulumi.getter(name="authProvider")
    def auth_provider(self) -> 'outputs.GoogleCloudAiplatformV1DeployedIndexAuthConfigAuthProviderResponse':
        """
        Defines the authentication provider that the DeployedIndex uses.
        """
        return pulumi.get(self, "auth_provider")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedIndexRefResponse(dict):
    """
    Points to a DeployedIndex.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "deployedIndexId":
            suggest = "deployed_index_id"
        elif key == "indexEndpoint":
            suggest = "index_endpoint"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedIndexRefResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedIndexRefResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedIndexRefResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 deployed_index_id: str,
                 index_endpoint: str):
        """
        Points to a DeployedIndex.
        :param str deployed_index_id: Immutable. The ID of the DeployedIndex in the above IndexEndpoint.
        :param str index_endpoint: Immutable. A resource name of the IndexEndpoint.
        """
        pulumi.set(__self__, "deployed_index_id", deployed_index_id)
        pulumi.set(__self__, "index_endpoint", index_endpoint)

    @property
    @pulumi.getter(name="deployedIndexId")
    def deployed_index_id(self) -> str:
        """
        Immutable. The ID of the DeployedIndex in the above IndexEndpoint.
        """
        return pulumi.get(self, "deployed_index_id")

    @property
    @pulumi.getter(name="indexEndpoint")
    def index_endpoint(self) -> str:
        """
        Immutable. A resource name of the IndexEndpoint.
        """
        return pulumi.get(self, "index_endpoint")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedIndexResponse(dict):
    """
    A deployment of an Index. IndexEndpoints contain one or more DeployedIndexes.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "automaticResources":
            suggest = "automatic_resources"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "dedicatedResources":
            suggest = "dedicated_resources"
        elif key == "deployedIndexAuthConfig":
            suggest = "deployed_index_auth_config"
        elif key == "deploymentGroup":
            suggest = "deployment_group"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "enableAccessLogging":
            suggest = "enable_access_logging"
        elif key == "indexSyncTime":
            suggest = "index_sync_time"
        elif key == "privateEndpoints":
            suggest = "private_endpoints"
        elif key == "reservedIpRanges":
            suggest = "reserved_ip_ranges"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedIndexResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedIndexResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedIndexResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 automatic_resources: 'outputs.GoogleCloudAiplatformV1AutomaticResourcesResponse',
                 create_time: str,
                 dedicated_resources: 'outputs.GoogleCloudAiplatformV1DedicatedResourcesResponse',
                 deployed_index_auth_config: 'outputs.GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse',
                 deployment_group: str,
                 display_name: str,
                 enable_access_logging: bool,
                 index: str,
                 index_sync_time: str,
                 private_endpoints: 'outputs.GoogleCloudAiplatformV1IndexPrivateEndpointsResponse',
                 reserved_ip_ranges: Sequence[str]):
        """
        A deployment of an Index. IndexEndpoints contain one or more DeployedIndexes.
        :param 'GoogleCloudAiplatformV1AutomaticResourcesResponse' automatic_resources: Optional. A description of resources that the DeployedIndex uses, which to large degree are decided by Vertex AI, and optionally allows only a modest additional configuration. If min_replica_count is not set, the default value is 2 (we don't provide SLA when min_replica_count=1). If max_replica_count is not set, the default value is min_replica_count. The max allowed replica count is 1000.
        :param str create_time: Timestamp when the DeployedIndex was created.
        :param 'GoogleCloudAiplatformV1DedicatedResourcesResponse' dedicated_resources: Optional. A description of resources that are dedicated to the DeployedIndex, and that need a higher degree of manual configuration. The field min_replica_count must be set to a value strictly greater than 0, or else validation will fail. We don't provide SLA when min_replica_count=1. If max_replica_count is not set, the default value is min_replica_count. The max allowed replica count is 1000. Available machine types for SMALL shard: e2-standard-2 and all machine types available for MEDIUM and LARGE shard. Available machine types for MEDIUM shard: e2-standard-16 and all machine types available for LARGE shard. Available machine types for LARGE shard: e2-highmem-16, n2d-standard-32. n1-standard-16 and n1-standard-32 are still available, but we recommend e2-standard-16 and e2-highmem-16 for cost efficiency.
        :param 'GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse' deployed_index_auth_config: Optional. If set, the authentication is enabled for the private endpoint.
        :param str deployment_group: Optional. The deployment group can be no longer than 64 characters (eg: 'test', 'prod'). If not set, we will use the 'default' deployment group. Creating `deployment_groups` with `reserved_ip_ranges` is a recommended practice when the peered network has multiple peering ranges. This creates your deployments from predictable IP spaces for easier traffic administration. Also, one deployment_group (except 'default') can only be used with the same reserved_ip_ranges which means if the deployment_group has been used with reserved_ip_ranges: [a, b, c], using it with [a, b] or [d, e] is disallowed. Note: we only support up to 5 deployment groups(not including 'default').
        :param str display_name: The display name of the DeployedIndex. If not provided upon creation, the Index's display_name is used.
        :param bool enable_access_logging: Optional. If true, private endpoint's access logs are sent to Cloud Logging. These logs are like standard server access logs, containing information like timestamp and latency for each MatchRequest. Note that logs may incur a cost, especially if the deployed index receives a high queries per second rate (QPS). Estimate your costs before enabling this option.
        :param str index: The name of the Index this is the deployment of. We may refer to this Index as the DeployedIndex's "original" Index.
        :param str index_sync_time: The DeployedIndex may depend on various data on its original Index. Additionally when certain changes to the original Index are being done (e.g. when what the Index contains is being changed) the DeployedIndex may be asynchronously updated in the background to reflect these changes. If this timestamp's value is at least the Index.update_time of the original Index, it means that this DeployedIndex and the original Index are in sync. If this timestamp is older, then to see which updates this DeployedIndex already contains (and which it does not), one must list the operations that are running on the original Index. Only the successfully completed Operations with update_time equal or before this sync time are contained in this DeployedIndex.
        :param 'GoogleCloudAiplatformV1IndexPrivateEndpointsResponse' private_endpoints: Provides paths for users to send requests directly to the deployed index services running on Cloud via private services access. This field is populated if network is configured.
        :param Sequence[str] reserved_ip_ranges: Optional. A list of reserved ip ranges under the VPC network that can be used for this DeployedIndex. If set, we will deploy the index within the provided ip ranges. Otherwise, the index might be deployed to any ip ranges under the provided VPC network. The value should be the name of the address (https://cloud.google.com/compute/docs/reference/rest/v1/addresses) Example: ['vertex-ai-ip-range']. For more information about subnets and network IP ranges, please see https://cloud.google.com/vpc/docs/subnets#manually_created_subnet_ip_ranges.
        """
        pulumi.set(__self__, "automatic_resources", automatic_resources)
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "dedicated_resources", dedicated_resources)
        pulumi.set(__self__, "deployed_index_auth_config", deployed_index_auth_config)
        pulumi.set(__self__, "deployment_group", deployment_group)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "enable_access_logging", enable_access_logging)
        pulumi.set(__self__, "index", index)
        pulumi.set(__self__, "index_sync_time", index_sync_time)
        pulumi.set(__self__, "private_endpoints", private_endpoints)
        pulumi.set(__self__, "reserved_ip_ranges", reserved_ip_ranges)

    @property
    @pulumi.getter(name="automaticResources")
    def automatic_resources(self) -> 'outputs.GoogleCloudAiplatformV1AutomaticResourcesResponse':
        """
        Optional. A description of resources that the DeployedIndex uses, which to large degree are decided by Vertex AI, and optionally allows only a modest additional configuration. If min_replica_count is not set, the default value is 2 (we don't provide SLA when min_replica_count=1). If max_replica_count is not set, the default value is min_replica_count. The max allowed replica count is 1000.
        """
        return pulumi.get(self, "automatic_resources")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when the DeployedIndex was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="dedicatedResources")
    def dedicated_resources(self) -> 'outputs.GoogleCloudAiplatformV1DedicatedResourcesResponse':
        """
        Optional. A description of resources that are dedicated to the DeployedIndex, and that need a higher degree of manual configuration. The field min_replica_count must be set to a value strictly greater than 0, or else validation will fail. We don't provide SLA when min_replica_count=1. If max_replica_count is not set, the default value is min_replica_count. The max allowed replica count is 1000. Available machine types for SMALL shard: e2-standard-2 and all machine types available for MEDIUM and LARGE shard. Available machine types for MEDIUM shard: e2-standard-16 and all machine types available for LARGE shard. Available machine types for LARGE shard: e2-highmem-16, n2d-standard-32. n1-standard-16 and n1-standard-32 are still available, but we recommend e2-standard-16 and e2-highmem-16 for cost efficiency.
        """
        return pulumi.get(self, "dedicated_resources")

    @property
    @pulumi.getter(name="deployedIndexAuthConfig")
    def deployed_index_auth_config(self) -> 'outputs.GoogleCloudAiplatformV1DeployedIndexAuthConfigResponse':
        """
        Optional. If set, the authentication is enabled for the private endpoint.
        """
        return pulumi.get(self, "deployed_index_auth_config")

    @property
    @pulumi.getter(name="deploymentGroup")
    def deployment_group(self) -> str:
        """
        Optional. The deployment group can be no longer than 64 characters (eg: 'test', 'prod'). If not set, we will use the 'default' deployment group. Creating `deployment_groups` with `reserved_ip_ranges` is a recommended practice when the peered network has multiple peering ranges. This creates your deployments from predictable IP spaces for easier traffic administration. Also, one deployment_group (except 'default') can only be used with the same reserved_ip_ranges which means if the deployment_group has been used with reserved_ip_ranges: [a, b, c], using it with [a, b] or [d, e] is disallowed. Note: we only support up to 5 deployment groups(not including 'default').
        """
        return pulumi.get(self, "deployment_group")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The display name of the DeployedIndex. If not provided upon creation, the Index's display_name is used.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="enableAccessLogging")
    def enable_access_logging(self) -> bool:
        """
        Optional. If true, private endpoint's access logs are sent to Cloud Logging. These logs are like standard server access logs, containing information like timestamp and latency for each MatchRequest. Note that logs may incur a cost, especially if the deployed index receives a high queries per second rate (QPS). Estimate your costs before enabling this option.
        """
        return pulumi.get(self, "enable_access_logging")

    @property
    @pulumi.getter
    def index(self) -> str:
        """
        The name of the Index this is the deployment of. We may refer to this Index as the DeployedIndex's "original" Index.
        """
        return pulumi.get(self, "index")

    @property
    @pulumi.getter(name="indexSyncTime")
    def index_sync_time(self) -> str:
        """
        The DeployedIndex may depend on various data on its original Index. Additionally when certain changes to the original Index are being done (e.g. when what the Index contains is being changed) the DeployedIndex may be asynchronously updated in the background to reflect these changes. If this timestamp's value is at least the Index.update_time of the original Index, it means that this DeployedIndex and the original Index are in sync. If this timestamp is older, then to see which updates this DeployedIndex already contains (and which it does not), one must list the operations that are running on the original Index. Only the successfully completed Operations with update_time equal or before this sync time are contained in this DeployedIndex.
        """
        return pulumi.get(self, "index_sync_time")

    @property
    @pulumi.getter(name="privateEndpoints")
    def private_endpoints(self) -> 'outputs.GoogleCloudAiplatformV1IndexPrivateEndpointsResponse':
        """
        Provides paths for users to send requests directly to the deployed index services running on Cloud via private services access. This field is populated if network is configured.
        """
        return pulumi.get(self, "private_endpoints")

    @property
    @pulumi.getter(name="reservedIpRanges")
    def reserved_ip_ranges(self) -> Sequence[str]:
        """
        Optional. A list of reserved ip ranges under the VPC network that can be used for this DeployedIndex. If set, we will deploy the index within the provided ip ranges. Otherwise, the index might be deployed to any ip ranges under the provided VPC network. The value should be the name of the address (https://cloud.google.com/compute/docs/reference/rest/v1/addresses) Example: ['vertex-ai-ip-range']. For more information about subnets and network IP ranges, please see https://cloud.google.com/vpc/docs/subnets#manually_created_subnet_ip_ranges.
        """
        return pulumi.get(self, "reserved_ip_ranges")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedModelRefResponse(dict):
    """
    Points to a DeployedModel.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "deployedModelId":
            suggest = "deployed_model_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedModelRefResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedModelRefResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedModelRefResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 deployed_model_id: str,
                 endpoint: str):
        """
        Points to a DeployedModel.
        :param str deployed_model_id: Immutable. An ID of a DeployedModel in the above Endpoint.
        :param str endpoint: Immutable. A resource name of an Endpoint.
        """
        pulumi.set(__self__, "deployed_model_id", deployed_model_id)
        pulumi.set(__self__, "endpoint", endpoint)

    @property
    @pulumi.getter(name="deployedModelId")
    def deployed_model_id(self) -> str:
        """
        Immutable. An ID of a DeployedModel in the above Endpoint.
        """
        return pulumi.get(self, "deployed_model_id")

    @property
    @pulumi.getter
    def endpoint(self) -> str:
        """
        Immutable. A resource name of an Endpoint.
        """
        return pulumi.get(self, "endpoint")


@pulumi.output_type
class GoogleCloudAiplatformV1DeployedModelResponse(dict):
    """
    A deployment of a Model. Endpoints contain one or more DeployedModels.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "automaticResources":
            suggest = "automatic_resources"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "dedicatedResources":
            suggest = "dedicated_resources"
        elif key == "disableContainerLogging":
            suggest = "disable_container_logging"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "enableAccessLogging":
            suggest = "enable_access_logging"
        elif key == "explanationSpec":
            suggest = "explanation_spec"
        elif key == "modelVersionId":
            suggest = "model_version_id"
        elif key == "privateEndpoints":
            suggest = "private_endpoints"
        elif key == "serviceAccount":
            suggest = "service_account"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DeployedModelResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DeployedModelResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DeployedModelResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 automatic_resources: 'outputs.GoogleCloudAiplatformV1AutomaticResourcesResponse',
                 create_time: str,
                 dedicated_resources: 'outputs.GoogleCloudAiplatformV1DedicatedResourcesResponse',
                 disable_container_logging: bool,
                 display_name: str,
                 enable_access_logging: bool,
                 explanation_spec: 'outputs.GoogleCloudAiplatformV1ExplanationSpecResponse',
                 model: str,
                 model_version_id: str,
                 private_endpoints: 'outputs.GoogleCloudAiplatformV1PrivateEndpointsResponse',
                 service_account: str):
        """
        A deployment of a Model. Endpoints contain one or more DeployedModels.
        :param 'GoogleCloudAiplatformV1AutomaticResourcesResponse' automatic_resources: A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        :param str create_time: Timestamp when the DeployedModel was created.
        :param 'GoogleCloudAiplatformV1DedicatedResourcesResponse' dedicated_resources: A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        :param bool disable_container_logging: For custom-trained Models and AutoML Tabular Models, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Cloud Logging by default. Please note that the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging/pricing). User can disable container logging by setting this flag to true.
        :param str display_name: The display name of the DeployedModel. If not provided upon creation, the Model's display_name is used.
        :param bool enable_access_logging: If true, online prediction access logs are sent to Cloud Logging. These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        :param 'GoogleCloudAiplatformV1ExplanationSpecResponse' explanation_spec: Explanation configuration for this DeployedModel. When deploying a Model using EndpointService.DeployModel, this value overrides the value of Model.explanation_spec. All fields of explanation_spec are optional in the request. If a field of explanation_spec is not populated, the value of the same field of Model.explanation_spec is inherited. If the corresponding Model.explanation_spec is not populated, all fields of the explanation_spec will be used for the explanation configuration.
        :param str model: The resource name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint. The resource name may contain version id or version alias to specify the version. Example: `projects/{project}/locations/{location}/models/{model}@2` or `projects/{project}/locations/{location}/models/{model}@golden` if no version is specified, the default version will be deployed.
        :param str model_version_id: The version ID of the model that is deployed.
        :param 'GoogleCloudAiplatformV1PrivateEndpointsResponse' private_endpoints: Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        :param str service_account: The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        pulumi.set(__self__, "automatic_resources", automatic_resources)
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "dedicated_resources", dedicated_resources)
        pulumi.set(__self__, "disable_container_logging", disable_container_logging)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "enable_access_logging", enable_access_logging)
        pulumi.set(__self__, "explanation_spec", explanation_spec)
        pulumi.set(__self__, "model", model)
        pulumi.set(__self__, "model_version_id", model_version_id)
        pulumi.set(__self__, "private_endpoints", private_endpoints)
        pulumi.set(__self__, "service_account", service_account)

    @property
    @pulumi.getter(name="automaticResources")
    def automatic_resources(self) -> 'outputs.GoogleCloudAiplatformV1AutomaticResourcesResponse':
        """
        A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
        """
        return pulumi.get(self, "automatic_resources")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when the DeployedModel was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="dedicatedResources")
    def dedicated_resources(self) -> 'outputs.GoogleCloudAiplatformV1DedicatedResourcesResponse':
        """
        A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
        """
        return pulumi.get(self, "dedicated_resources")

    @property
    @pulumi.getter(name="disableContainerLogging")
    def disable_container_logging(self) -> bool:
        """
        For custom-trained Models and AutoML Tabular Models, the container of the DeployedModel instances will send `stderr` and `stdout` streams to Cloud Logging by default. Please note that the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging/pricing). User can disable container logging by setting this flag to true.
        """
        return pulumi.get(self, "disable_container_logging")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The display name of the DeployedModel. If not provided upon creation, the Model's display_name is used.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="enableAccessLogging")
    def enable_access_logging(self) -> bool:
        """
        If true, online prediction access logs are sent to Cloud Logging. These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
        """
        return pulumi.get(self, "enable_access_logging")

    @property
    @pulumi.getter(name="explanationSpec")
    def explanation_spec(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationSpecResponse':
        """
        Explanation configuration for this DeployedModel. When deploying a Model using EndpointService.DeployModel, this value overrides the value of Model.explanation_spec. All fields of explanation_spec are optional in the request. If a field of explanation_spec is not populated, the value of the same field of Model.explanation_spec is inherited. If the corresponding Model.explanation_spec is not populated, all fields of the explanation_spec will be used for the explanation configuration.
        """
        return pulumi.get(self, "explanation_spec")

    @property
    @pulumi.getter
    def model(self) -> str:
        """
        The resource name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint. The resource name may contain version id or version alias to specify the version. Example: `projects/{project}/locations/{location}/models/{model}@2` or `projects/{project}/locations/{location}/models/{model}@golden` if no version is specified, the default version will be deployed.
        """
        return pulumi.get(self, "model")

    @property
    @pulumi.getter(name="modelVersionId")
    def model_version_id(self) -> str:
        """
        The version ID of the model that is deployed.
        """
        return pulumi.get(self, "model_version_id")

    @property
    @pulumi.getter(name="privateEndpoints")
    def private_endpoints(self) -> 'outputs.GoogleCloudAiplatformV1PrivateEndpointsResponse':
        """
        Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
        """
        return pulumi.get(self, "private_endpoints")

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> str:
        """
        The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        return pulumi.get(self, "service_account")


@pulumi.output_type
class GoogleCloudAiplatformV1DiskSpecResponse(dict):
    """
    Represents the spec of disk options.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bootDiskSizeGb":
            suggest = "boot_disk_size_gb"
        elif key == "bootDiskType":
            suggest = "boot_disk_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1DiskSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1DiskSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1DiskSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 boot_disk_size_gb: int,
                 boot_disk_type: str):
        """
        Represents the spec of disk options.
        :param int boot_disk_size_gb: Size in GB of the boot disk (default is 100GB).
        :param str boot_disk_type: Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        pulumi.set(__self__, "boot_disk_type", boot_disk_type)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> int:
        """
        Size in GB of the boot disk (default is 100GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> str:
        """
        Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")


@pulumi.output_type
class GoogleCloudAiplatformV1EncryptionSpecResponse(dict):
    """
    Represents a customer-managed encryption key spec that can be applied to a top-level resource.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1EncryptionSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1EncryptionSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1EncryptionSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: str):
        """
        Represents a customer-managed encryption key spec that can be applied to a top-level resource.
        :param str kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> str:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class GoogleCloudAiplatformV1EnvVarResponse(dict):
    """
    Represents an environment variable present in a Container or Python Module.
    """
    def __init__(__self__, *,
                 name: str,
                 value: str):
        """
        Represents an environment variable present in a Container or Python Module.
        :param str name: Name of the environment variable. Must be a valid C identifier.
        :param str value: Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Name of the environment variable. Must be a valid C identifier.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def value(self) -> str:
        """
        Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse(dict):
    """
    The Cloud Storage input instances.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "dataFormat":
            suggest = "data_format"
        elif key == "gcsSource":
            suggest = "gcs_source"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 data_format: str,
                 gcs_source: 'outputs.GoogleCloudAiplatformV1GcsSourceResponse'):
        """
        The Cloud Storage input instances.
        :param str data_format: The format in which instances are given, if not specified, assume it's JSONL format. Currently only JSONL format is supported.
        :param 'GoogleCloudAiplatformV1GcsSourceResponse' gcs_source: The Cloud Storage location for the input instances.
        """
        pulumi.set(__self__, "data_format", data_format)
        pulumi.set(__self__, "gcs_source", gcs_source)

    @property
    @pulumi.getter(name="dataFormat")
    def data_format(self) -> str:
        """
        The format in which instances are given, if not specified, assume it's JSONL format. Currently only JSONL format is supported.
        """
        return pulumi.get(self, "data_format")

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> 'outputs.GoogleCloudAiplatformV1GcsSourceResponse':
        """
        The Cloud Storage location for the input instances.
        """
        return pulumi.get(self, "gcs_source")


@pulumi.output_type
class GoogleCloudAiplatformV1ExamplesResponse(dict):
    """
    Example-based explainability that returns the nearest neighbors from the provided dataset.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exampleGcsSource":
            suggest = "example_gcs_source"
        elif key == "nearestNeighborSearchConfig":
            suggest = "nearest_neighbor_search_config"
        elif key == "neighborCount":
            suggest = "neighbor_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExamplesResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExamplesResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExamplesResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 example_gcs_source: 'outputs.GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse',
                 nearest_neighbor_search_config: Any,
                 neighbor_count: int,
                 presets: 'outputs.GoogleCloudAiplatformV1PresetsResponse'):
        """
        Example-based explainability that returns the nearest neighbors from the provided dataset.
        :param 'GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse' example_gcs_source: The Cloud Storage input instances.
        :param Any nearest_neighbor_search_config: The full configuration for the generated index, the semantics are the same as metadata and should match [NearestNeighborSearchConfig](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-example-based#nearest-neighbor-search-config).
        :param int neighbor_count: The number of neighbors to return when querying for examples.
        :param 'GoogleCloudAiplatformV1PresetsResponse' presets: Simplified preset configuration, which automatically sets configuration values based on the desired query speed-precision trade-off and modality.
        """
        pulumi.set(__self__, "example_gcs_source", example_gcs_source)
        pulumi.set(__self__, "nearest_neighbor_search_config", nearest_neighbor_search_config)
        pulumi.set(__self__, "neighbor_count", neighbor_count)
        pulumi.set(__self__, "presets", presets)

    @property
    @pulumi.getter(name="exampleGcsSource")
    def example_gcs_source(self) -> 'outputs.GoogleCloudAiplatformV1ExamplesExampleGcsSourceResponse':
        """
        The Cloud Storage input instances.
        """
        return pulumi.get(self, "example_gcs_source")

    @property
    @pulumi.getter(name="nearestNeighborSearchConfig")
    def nearest_neighbor_search_config(self) -> Any:
        """
        The full configuration for the generated index, the semantics are the same as metadata and should match [NearestNeighborSearchConfig](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-example-based#nearest-neighbor-search-config).
        """
        return pulumi.get(self, "nearest_neighbor_search_config")

    @property
    @pulumi.getter(name="neighborCount")
    def neighbor_count(self) -> int:
        """
        The number of neighbors to return when querying for examples.
        """
        return pulumi.get(self, "neighbor_count")

    @property
    @pulumi.getter
    def presets(self) -> 'outputs.GoogleCloudAiplatformV1PresetsResponse':
        """
        Simplified preset configuration, which automatically sets configuration values based on the desired query speed-precision trade-off and modality.
        """
        return pulumi.get(self, "presets")


@pulumi.output_type
class GoogleCloudAiplatformV1ExecutionResponse(dict):
    """
    Instance of a general execution.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "createTime":
            suggest = "create_time"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "schemaTitle":
            suggest = "schema_title"
        elif key == "schemaVersion":
            suggest = "schema_version"
        elif key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExecutionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExecutionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExecutionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 create_time: str,
                 description: str,
                 display_name: str,
                 etag: str,
                 labels: Mapping[str, str],
                 metadata: Mapping[str, Any],
                 name: str,
                 schema_title: str,
                 schema_version: str,
                 state: str,
                 update_time: str):
        """
        Instance of a general execution.
        :param str create_time: Timestamp when this Execution was created.
        :param str description: Description of the Execution
        :param str display_name: User provided display name of the Execution. May be up to 128 Unicode characters.
        :param str etag: An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        :param Mapping[str, str] labels: The labels with user-defined metadata to organize your Executions. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Execution (System labels are excluded).
        :param Mapping[str, Any] metadata: Properties of the Execution. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        :param str name: The resource name of the Execution.
        :param str schema_title: The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str schema_version: The version of the schema in `schema_title` to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        :param str state: The state of this Execution. This is a property of the Execution, and does not imply or capture any ongoing process. This property is managed by clients (such as Vertex AI Pipelines) and the system does not prescribe or check the validity of state transitions.
        :param str update_time: Timestamp when this Execution was last updated.
        """
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "schema_title", schema_title)
        pulumi.set(__self__, "schema_version", schema_version)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this Execution was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Description of the Execution
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        User provided display name of the Execution. May be up to 128 Unicode characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        An eTag used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize your Executions. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. No more than 64 user labels can be associated with one Execution (System labels are excluded).
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter
    def metadata(self) -> Mapping[str, Any]:
        """
        Properties of the Execution. Top level metadata keys' heading and trailing spaces will be trimmed. The size of this field should not exceed 200KB.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The resource name of the Execution.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="schemaTitle")
    def schema_title(self) -> str:
        """
        The title of the schema describing the metadata. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_title")

    @property
    @pulumi.getter(name="schemaVersion")
    def schema_version(self) -> str:
        """
        The version of the schema in `schema_title` to use. Schema title and version is expected to be registered in earlier Create Schema calls. And both are used together as unique identifiers to identify schemas within the local metadata store.
        """
        return pulumi.get(self, "schema_version")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The state of this Execution. This is a property of the Execution, and does not imply or capture any ongoing process. This property is managed by clients (such as Vertex AI Pipelines) and the system does not prescribe or check the validity of state transitions.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this Execution was last updated.
        """
        return pulumi.get(self, "update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse(dict):
    """
    Domain details of the input feature value. Provides numeric information about the feature, such as its range (min, max). If the feature has been pre-processed, for example with z-scoring, then it provides information about how to recover the original feature. For example, if the input feature is an image and it has been pre-processed to obtain 0-mean and stddev = 1 values, then original_mean, and original_stddev refer to the mean and stddev of the original feature (e.g. image tensor) from which input feature (with mean = 0 and stddev = 1) was obtained.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"
        elif key == "originalMean":
            suggest = "original_mean"
        elif key == "originalStddev":
            suggest = "original_stddev"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_value: float,
                 min_value: float,
                 original_mean: float,
                 original_stddev: float):
        """
        Domain details of the input feature value. Provides numeric information about the feature, such as its range (min, max). If the feature has been pre-processed, for example with z-scoring, then it provides information about how to recover the original feature. For example, if the input feature is an image and it has been pre-processed to obtain 0-mean and stddev = 1 values, then original_mean, and original_stddev refer to the mean and stddev of the original feature (e.g. image tensor) from which input feature (with mean = 0 and stddev = 1) was obtained.
        :param float max_value: The maximum permissible value for this feature.
        :param float min_value: The minimum permissible value for this feature.
        :param float original_mean: If this input feature has been normalized to a mean value of 0, the original_mean specifies the mean value of the domain prior to normalization.
        :param float original_stddev: If this input feature has been normalized to a standard deviation of 1.0, the original_stddev specifies the standard deviation of the domain prior to normalization.
        """
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)
        pulumi.set(__self__, "original_mean", original_mean)
        pulumi.set(__self__, "original_stddev", original_stddev)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> float:
        """
        The maximum permissible value for this feature.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> float:
        """
        The minimum permissible value for this feature.
        """
        return pulumi.get(self, "min_value")

    @property
    @pulumi.getter(name="originalMean")
    def original_mean(self) -> float:
        """
        If this input feature has been normalized to a mean value of 0, the original_mean specifies the mean value of the domain prior to normalization.
        """
        return pulumi.get(self, "original_mean")

    @property
    @pulumi.getter(name="originalStddev")
    def original_stddev(self) -> float:
        """
        If this input feature has been normalized to a standard deviation of 1.0, the original_stddev specifies the standard deviation of the domain prior to normalization.
        """
        return pulumi.get(self, "original_stddev")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse(dict):
    """
    Metadata of the input of a feature. Fields other than InputMetadata.input_baselines are applicable only for Models that are using Vertex AI-provided images for Tensorflow.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "denseShapeTensorName":
            suggest = "dense_shape_tensor_name"
        elif key == "encodedBaselines":
            suggest = "encoded_baselines"
        elif key == "encodedTensorName":
            suggest = "encoded_tensor_name"
        elif key == "featureValueDomain":
            suggest = "feature_value_domain"
        elif key == "groupName":
            suggest = "group_name"
        elif key == "indexFeatureMapping":
            suggest = "index_feature_mapping"
        elif key == "indicesTensorName":
            suggest = "indices_tensor_name"
        elif key == "inputBaselines":
            suggest = "input_baselines"
        elif key == "inputTensorName":
            suggest = "input_tensor_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 dense_shape_tensor_name: str,
                 encoded_baselines: Sequence[Any],
                 encoded_tensor_name: str,
                 encoding: str,
                 feature_value_domain: 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse',
                 group_name: str,
                 index_feature_mapping: Sequence[str],
                 indices_tensor_name: str,
                 input_baselines: Sequence[Any],
                 input_tensor_name: str,
                 modality: str,
                 visualization: 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse'):
        """
        Metadata of the input of a feature. Fields other than InputMetadata.input_baselines are applicable only for Models that are using Vertex AI-provided images for Tensorflow.
        :param str dense_shape_tensor_name: Specifies the shape of the values of the input if the input is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        :param Sequence[Any] encoded_baselines: A list of baselines for the encoded tensor. The shape of each baseline should match the shape of the encoded tensor. If a scalar is provided, Vertex AI broadcasts to the same shape as the encoded tensor.
        :param str encoded_tensor_name: Encoded tensor is a transformation of the input tensor. Must be provided if choosing Integrated Gradients attribution or XRAI attribution and the input tensor is not differentiable. An encoded tensor is generated if the input tensor is encoded by a lookup table.
        :param str encoding: Defines how the feature is encoded into the input tensor. Defaults to IDENTITY.
        :param 'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse' feature_value_domain: The domain details of the input feature value. Like min/max, original mean or standard deviation if normalized.
        :param str group_name: Name of the group that the input belongs to. Features with the same group name will be treated as one feature when computing attributions. Features grouped together can have different shapes in value. If provided, there will be one single attribution generated in Attribution.feature_attributions, keyed by the group name.
        :param Sequence[str] index_feature_mapping: A list of feature names for each index in the input tensor. Required when the input InputMetadata.encoding is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
        :param str indices_tensor_name: Specifies the index of the values of the input tensor. Required when the input tensor is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        :param Sequence[Any] input_baselines: Baseline inputs for this feature. If no baseline is specified, Vertex AI chooses the baseline for this feature. If multiple baselines are specified, Vertex AI returns the average attributions across them in Attribution.feature_attributions. For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape of each baseline must match the shape of the input tensor. If a scalar is provided, we broadcast to the same shape as the input tensor. For custom images, the element of the baselines must be in the same format as the feature's input in the instance[]. The schema of any single instance may be specified via Endpoint's DeployedModels' Model's PredictSchemata's instance_schema_uri.
        :param str input_tensor_name: Name of the input tensor for this feature. Required and is only applicable to Vertex AI-provided images for Tensorflow.
        :param str modality: Modality of the feature. Valid values are: numeric, image. Defaults to numeric.
        :param 'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse' visualization: Visualization configurations for image explanation.
        """
        pulumi.set(__self__, "dense_shape_tensor_name", dense_shape_tensor_name)
        pulumi.set(__self__, "encoded_baselines", encoded_baselines)
        pulumi.set(__self__, "encoded_tensor_name", encoded_tensor_name)
        pulumi.set(__self__, "encoding", encoding)
        pulumi.set(__self__, "feature_value_domain", feature_value_domain)
        pulumi.set(__self__, "group_name", group_name)
        pulumi.set(__self__, "index_feature_mapping", index_feature_mapping)
        pulumi.set(__self__, "indices_tensor_name", indices_tensor_name)
        pulumi.set(__self__, "input_baselines", input_baselines)
        pulumi.set(__self__, "input_tensor_name", input_tensor_name)
        pulumi.set(__self__, "modality", modality)
        pulumi.set(__self__, "visualization", visualization)

    @property
    @pulumi.getter(name="denseShapeTensorName")
    def dense_shape_tensor_name(self) -> str:
        """
        Specifies the shape of the values of the input if the input is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """
        return pulumi.get(self, "dense_shape_tensor_name")

    @property
    @pulumi.getter(name="encodedBaselines")
    def encoded_baselines(self) -> Sequence[Any]:
        """
        A list of baselines for the encoded tensor. The shape of each baseline should match the shape of the encoded tensor. If a scalar is provided, Vertex AI broadcasts to the same shape as the encoded tensor.
        """
        return pulumi.get(self, "encoded_baselines")

    @property
    @pulumi.getter(name="encodedTensorName")
    def encoded_tensor_name(self) -> str:
        """
        Encoded tensor is a transformation of the input tensor. Must be provided if choosing Integrated Gradients attribution or XRAI attribution and the input tensor is not differentiable. An encoded tensor is generated if the input tensor is encoded by a lookup table.
        """
        return pulumi.get(self, "encoded_tensor_name")

    @property
    @pulumi.getter
    def encoding(self) -> str:
        """
        Defines how the feature is encoded into the input tensor. Defaults to IDENTITY.
        """
        return pulumi.get(self, "encoding")

    @property
    @pulumi.getter(name="featureValueDomain")
    def feature_value_domain(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainResponse':
        """
        The domain details of the input feature value. Like min/max, original mean or standard deviation if normalized.
        """
        return pulumi.get(self, "feature_value_domain")

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> str:
        """
        Name of the group that the input belongs to. Features with the same group name will be treated as one feature when computing attributions. Features grouped together can have different shapes in value. If provided, there will be one single attribution generated in Attribution.feature_attributions, keyed by the group name.
        """
        return pulumi.get(self, "group_name")

    @property
    @pulumi.getter(name="indexFeatureMapping")
    def index_feature_mapping(self) -> Sequence[str]:
        """
        A list of feature names for each index in the input tensor. Required when the input InputMetadata.encoding is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
        """
        return pulumi.get(self, "index_feature_mapping")

    @property
    @pulumi.getter(name="indicesTensorName")
    def indices_tensor_name(self) -> str:
        """
        Specifies the index of the values of the input tensor. Required when the input tensor is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """
        return pulumi.get(self, "indices_tensor_name")

    @property
    @pulumi.getter(name="inputBaselines")
    def input_baselines(self) -> Sequence[Any]:
        """
        Baseline inputs for this feature. If no baseline is specified, Vertex AI chooses the baseline for this feature. If multiple baselines are specified, Vertex AI returns the average attributions across them in Attribution.feature_attributions. For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape of each baseline must match the shape of the input tensor. If a scalar is provided, we broadcast to the same shape as the input tensor. For custom images, the element of the baselines must be in the same format as the feature's input in the instance[]. The schema of any single instance may be specified via Endpoint's DeployedModels' Model's PredictSchemata's instance_schema_uri.
        """
        return pulumi.get(self, "input_baselines")

    @property
    @pulumi.getter(name="inputTensorName")
    def input_tensor_name(self) -> str:
        """
        Name of the input tensor for this feature. Required and is only applicable to Vertex AI-provided images for Tensorflow.
        """
        return pulumi.get(self, "input_tensor_name")

    @property
    @pulumi.getter
    def modality(self) -> str:
        """
        Modality of the feature. Valid values are: numeric, image. Defaults to numeric.
        """
        return pulumi.get(self, "modality")

    @property
    @pulumi.getter
    def visualization(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse':
        """
        Visualization configurations for image explanation.
        """
        return pulumi.get(self, "visualization")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse(dict):
    """
    Visualization configurations for image explanation.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "clipPercentLowerbound":
            suggest = "clip_percent_lowerbound"
        elif key == "clipPercentUpperbound":
            suggest = "clip_percent_upperbound"
        elif key == "colorMap":
            suggest = "color_map"
        elif key == "overlayType":
            suggest = "overlay_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 clip_percent_lowerbound: float,
                 clip_percent_upperbound: float,
                 color_map: str,
                 overlay_type: str,
                 polarity: str,
                 type: str):
        """
        Visualization configurations for image explanation.
        :param float clip_percent_lowerbound: Excludes attributions below the specified percentile, from the highlighted areas. Defaults to 62.
        :param float clip_percent_upperbound: Excludes attributions above the specified percentile from the highlighted areas. Using the clip_percent_upperbound and clip_percent_lowerbound together can be useful for filtering out noise and making it easier to see areas of strong attribution. Defaults to 99.9.
        :param str color_map: The color scheme used for the highlighted areas. Defaults to PINK_GREEN for Integrated Gradients attribution, which shows positive attributions in green and negative in pink. Defaults to VIRIDIS for XRAI attribution, which highlights the most influential regions in yellow and the least influential in blue.
        :param str overlay_type: How the original image is displayed in the visualization. Adjusting the overlay can help increase visual clarity if the original image makes it difficult to view the visualization. Defaults to NONE.
        :param str polarity: Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE.
        :param str type: Type of the image visualization. Only applicable to Integrated Gradients attribution. OUTLINES shows regions of attribution, while PIXELS shows per-pixel attribution. Defaults to OUTLINES.
        """
        pulumi.set(__self__, "clip_percent_lowerbound", clip_percent_lowerbound)
        pulumi.set(__self__, "clip_percent_upperbound", clip_percent_upperbound)
        pulumi.set(__self__, "color_map", color_map)
        pulumi.set(__self__, "overlay_type", overlay_type)
        pulumi.set(__self__, "polarity", polarity)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="clipPercentLowerbound")
    def clip_percent_lowerbound(self) -> float:
        """
        Excludes attributions below the specified percentile, from the highlighted areas. Defaults to 62.
        """
        return pulumi.get(self, "clip_percent_lowerbound")

    @property
    @pulumi.getter(name="clipPercentUpperbound")
    def clip_percent_upperbound(self) -> float:
        """
        Excludes attributions above the specified percentile from the highlighted areas. Using the clip_percent_upperbound and clip_percent_lowerbound together can be useful for filtering out noise and making it easier to see areas of strong attribution. Defaults to 99.9.
        """
        return pulumi.get(self, "clip_percent_upperbound")

    @property
    @pulumi.getter(name="colorMap")
    def color_map(self) -> str:
        """
        The color scheme used for the highlighted areas. Defaults to PINK_GREEN for Integrated Gradients attribution, which shows positive attributions in green and negative in pink. Defaults to VIRIDIS for XRAI attribution, which highlights the most influential regions in yellow and the least influential in blue.
        """
        return pulumi.get(self, "color_map")

    @property
    @pulumi.getter(name="overlayType")
    def overlay_type(self) -> str:
        """
        How the original image is displayed in the visualization. Adjusting the overlay can help increase visual clarity if the original image makes it difficult to view the visualization. Defaults to NONE.
        """
        return pulumi.get(self, "overlay_type")

    @property
    @pulumi.getter
    def polarity(self) -> str:
        """
        Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE.
        """
        return pulumi.get(self, "polarity")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        Type of the image visualization. Only applicable to Integrated Gradients attribution. OUTLINES shows regions of attribution, while PIXELS shows per-pixel attribution. Defaults to OUTLINES.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse(dict):
    """
    Metadata of the prediction output to be explained.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "displayNameMappingKey":
            suggest = "display_name_mapping_key"
        elif key == "indexDisplayNameMapping":
            suggest = "index_display_name_mapping"
        elif key == "outputTensorName":
            suggest = "output_tensor_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 display_name_mapping_key: str,
                 index_display_name_mapping: Any,
                 output_tensor_name: str):
        """
        Metadata of the prediction output to be explained.
        :param str display_name_mapping_key: Specify a field name in the prediction to look for the display name. Use this if the prediction contains the display names for the outputs. The display names in the prediction must have the same shape of the outputs, so that it can be located by Attribution.output_index for a specific output.
        :param Any index_display_name_mapping: Static mapping between the index and display name. Use this if the outputs are a deterministic n-dimensional array, e.g. a list of scores of all the classes in a pre-defined order for a multi-classification Model. It's not feasible if the outputs are non-deterministic, e.g. the Model produces top-k classes or sort the outputs by their values. The shape of the value must be an n-dimensional array of strings. The number of dimensions must match that of the outputs to be explained. The Attribution.output_display_name is populated by locating in the mapping with Attribution.output_index.
        :param str output_tensor_name: Name of the output tensor. Required and is only applicable to Vertex AI provided images for Tensorflow.
        """
        pulumi.set(__self__, "display_name_mapping_key", display_name_mapping_key)
        pulumi.set(__self__, "index_display_name_mapping", index_display_name_mapping)
        pulumi.set(__self__, "output_tensor_name", output_tensor_name)

    @property
    @pulumi.getter(name="displayNameMappingKey")
    def display_name_mapping_key(self) -> str:
        """
        Specify a field name in the prediction to look for the display name. Use this if the prediction contains the display names for the outputs. The display names in the prediction must have the same shape of the outputs, so that it can be located by Attribution.output_index for a specific output.
        """
        return pulumi.get(self, "display_name_mapping_key")

    @property
    @pulumi.getter(name="indexDisplayNameMapping")
    def index_display_name_mapping(self) -> Any:
        """
        Static mapping between the index and display name. Use this if the outputs are a deterministic n-dimensional array, e.g. a list of scores of all the classes in a pre-defined order for a multi-classification Model. It's not feasible if the outputs are non-deterministic, e.g. the Model produces top-k classes or sort the outputs by their values. The shape of the value must be an n-dimensional array of strings. The number of dimensions must match that of the outputs to be explained. The Attribution.output_display_name is populated by locating in the mapping with Attribution.output_index.
        """
        return pulumi.get(self, "index_display_name_mapping")

    @property
    @pulumi.getter(name="outputTensorName")
    def output_tensor_name(self) -> str:
        """
        Name of the output tensor. Required and is only applicable to Vertex AI provided images for Tensorflow.
        """
        return pulumi.get(self, "output_tensor_name")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationMetadataResponse(dict):
    """
    Metadata describing the Model's input and output for explanation.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureAttributionsSchemaUri":
            suggest = "feature_attributions_schema_uri"
        elif key == "latentSpaceSource":
            suggest = "latent_space_source"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationMetadataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationMetadataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_attributions_schema_uri: str,
                 inputs: Mapping[str, 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse'],
                 latent_space_source: str,
                 outputs: Mapping[str, 'outputs.GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse']):
        """
        Metadata describing the Model's input and output for explanation.
        :param str feature_attributions_schema_uri: Points to a YAML file stored on Google Cloud Storage describing the format of the feature attributions. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML tabular Models always have this field populated by Vertex AI. Note: The URI given on output may be different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param Mapping[str, 'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse'] inputs: Map from feature names to feature input metadata. Keys are the name of the features. Values are the specification of the feature. An empty InputMetadata is valid. It describes a text feature which has the name specified as the key in ExplanationMetadata.inputs. The baseline of the empty feature is chosen by Vertex AI. For Vertex AI-provided Tensorflow images, the key can be any friendly name of the feature. Once specified, featureAttributions are keyed by this key (if not grouped with another feature). For custom images, the key must match with the key in instance.
        :param str latent_space_source: Name of the source to generate embeddings for example based explanations.
        :param Mapping[str, 'GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse'] outputs: Map from output names to output metadata. For Vertex AI-provided Tensorflow images, keys can be any user defined string that consists of any UTF-8 characters. For custom images, keys are the name of the output field in the prediction to be explained. Currently only one key is allowed.
        """
        pulumi.set(__self__, "feature_attributions_schema_uri", feature_attributions_schema_uri)
        pulumi.set(__self__, "inputs", inputs)
        pulumi.set(__self__, "latent_space_source", latent_space_source)
        pulumi.set(__self__, "outputs", outputs)

    @property
    @pulumi.getter(name="featureAttributionsSchemaUri")
    def feature_attributions_schema_uri(self) -> str:
        """
        Points to a YAML file stored on Google Cloud Storage describing the format of the feature attributions. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML tabular Models always have this field populated by Vertex AI. Note: The URI given on output may be different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "feature_attributions_schema_uri")

    @property
    @pulumi.getter
    def inputs(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ExplanationMetadataInputMetadataResponse']:
        """
        Map from feature names to feature input metadata. Keys are the name of the features. Values are the specification of the feature. An empty InputMetadata is valid. It describes a text feature which has the name specified as the key in ExplanationMetadata.inputs. The baseline of the empty feature is chosen by Vertex AI. For Vertex AI-provided Tensorflow images, the key can be any friendly name of the feature. Once specified, featureAttributions are keyed by this key (if not grouped with another feature). For custom images, the key must match with the key in instance.
        """
        return pulumi.get(self, "inputs")

    @property
    @pulumi.getter(name="latentSpaceSource")
    def latent_space_source(self) -> str:
        """
        Name of the source to generate embeddings for example based explanations.
        """
        return pulumi.get(self, "latent_space_source")

    @property
    @pulumi.getter
    def outputs(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataResponse']:
        """
        Map from output names to output metadata. For Vertex AI-provided Tensorflow images, keys can be any user defined string that consists of any UTF-8 characters. For custom images, keys are the name of the output field in the prediction to be explained. Currently only one key is allowed.
        """
        return pulumi.get(self, "outputs")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationParametersResponse(dict):
    """
    Parameters to configure explaining for Model's predictions.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "integratedGradientsAttribution":
            suggest = "integrated_gradients_attribution"
        elif key == "outputIndices":
            suggest = "output_indices"
        elif key == "sampledShapleyAttribution":
            suggest = "sampled_shapley_attribution"
        elif key == "topK":
            suggest = "top_k"
        elif key == "xraiAttribution":
            suggest = "xrai_attribution"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ExplanationParametersResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ExplanationParametersResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ExplanationParametersResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 examples: 'outputs.GoogleCloudAiplatformV1ExamplesResponse',
                 integrated_gradients_attribution: 'outputs.GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse',
                 output_indices: Sequence[Any],
                 sampled_shapley_attribution: 'outputs.GoogleCloudAiplatformV1SampledShapleyAttributionResponse',
                 top_k: int,
                 xrai_attribution: 'outputs.GoogleCloudAiplatformV1XraiAttributionResponse'):
        """
        Parameters to configure explaining for Model's predictions.
        :param 'GoogleCloudAiplatformV1ExamplesResponse' examples: Example-based explanations that returns the nearest neighbors from the provided dataset.
        :param 'GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse' integrated_gradients_attribution: An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param Sequence[Any] output_indices: If populated, only returns attributions that have output_index contained in output_indices. It must be an ndarray of integers, with the same shape of the output it's explaining. If not populated, returns attributions for top_k indices of outputs. If neither top_k nor output_indices is populated, returns the argmax index of the outputs. Only applicable to Models that predict multiple outputs (e,g, multi-class Models that predict multiple classes).
        :param 'GoogleCloudAiplatformV1SampledShapleyAttributionResponse' sampled_shapley_attribution: An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features. Refer to this paper for model details: https://arxiv.org/abs/1306.4265.
        :param int top_k: If populated, returns attributions for top K indices of outputs (defaults to 1). Only applies to Models that predicts more than one outputs (e,g, multi-class Models). When set to -1, returns explanations for all outputs.
        :param 'GoogleCloudAiplatformV1XraiAttributionResponse' xrai_attribution: An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 XRAI currently performs better on natural images, like a picture of a house or an animal. If the images are taken in artificial environments, like a lab or manufacturing line, or from diagnostic equipment, like x-rays or quality-control cameras, use Integrated Gradients instead.
        """
        pulumi.set(__self__, "examples", examples)
        pulumi.set(__self__, "integrated_gradients_attribution", integrated_gradients_attribution)
        pulumi.set(__self__, "output_indices", output_indices)
        pulumi.set(__self__, "sampled_shapley_attribution", sampled_shapley_attribution)
        pulumi.set(__self__, "top_k", top_k)
        pulumi.set(__self__, "xrai_attribution", xrai_attribution)

    @property
    @pulumi.getter
    def examples(self) -> 'outputs.GoogleCloudAiplatformV1ExamplesResponse':
        """
        Example-based explanations that returns the nearest neighbors from the provided dataset.
        """
        return pulumi.get(self, "examples")

    @property
    @pulumi.getter(name="integratedGradientsAttribution")
    def integrated_gradients_attribution(self) -> 'outputs.GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse':
        """
        An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        """
        return pulumi.get(self, "integrated_gradients_attribution")

    @property
    @pulumi.getter(name="outputIndices")
    def output_indices(self) -> Sequence[Any]:
        """
        If populated, only returns attributions that have output_index contained in output_indices. It must be an ndarray of integers, with the same shape of the output it's explaining. If not populated, returns attributions for top_k indices of outputs. If neither top_k nor output_indices is populated, returns the argmax index of the outputs. Only applicable to Models that predict multiple outputs (e,g, multi-class Models that predict multiple classes).
        """
        return pulumi.get(self, "output_indices")

    @property
    @pulumi.getter(name="sampledShapleyAttribution")
    def sampled_shapley_attribution(self) -> 'outputs.GoogleCloudAiplatformV1SampledShapleyAttributionResponse':
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features. Refer to this paper for model details: https://arxiv.org/abs/1306.4265.
        """
        return pulumi.get(self, "sampled_shapley_attribution")

    @property
    @pulumi.getter(name="topK")
    def top_k(self) -> int:
        """
        If populated, returns attributions for top K indices of outputs (defaults to 1). Only applies to Models that predicts more than one outputs (e,g, multi-class Models). When set to -1, returns explanations for all outputs.
        """
        return pulumi.get(self, "top_k")

    @property
    @pulumi.getter(name="xraiAttribution")
    def xrai_attribution(self) -> 'outputs.GoogleCloudAiplatformV1XraiAttributionResponse':
        """
        An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 XRAI currently performs better on natural images, like a picture of a house or an animal. If the images are taken in artificial environments, like a lab or manufacturing line, or from diagnostic equipment, like x-rays or quality-control cameras, use Integrated Gradients instead.
        """
        return pulumi.get(self, "xrai_attribution")


@pulumi.output_type
class GoogleCloudAiplatformV1ExplanationSpecResponse(dict):
    """
    Specification of Model explanation.
    """
    def __init__(__self__, *,
                 metadata: 'outputs.GoogleCloudAiplatformV1ExplanationMetadataResponse',
                 parameters: 'outputs.GoogleCloudAiplatformV1ExplanationParametersResponse'):
        """
        Specification of Model explanation.
        :param 'GoogleCloudAiplatformV1ExplanationMetadataResponse' metadata: Optional. Metadata describing the Model's input and output for explanation.
        :param 'GoogleCloudAiplatformV1ExplanationParametersResponse' parameters: Parameters that configure explaining of the Model's predictions.
        """
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter
    def metadata(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationMetadataResponse':
        """
        Optional. Metadata describing the Model's input and output for explanation.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter
    def parameters(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationParametersResponse':
        """
        Parameters that configure explaining of the Model's predictions.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureGroupBigQueryResponse(dict):
    """
    Input source type for BigQuery Tables and Views.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigQuerySource":
            suggest = "big_query_source"
        elif key == "entityIdColumns":
            suggest = "entity_id_columns"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureGroupBigQueryResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureGroupBigQueryResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureGroupBigQueryResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 big_query_source: 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse',
                 entity_id_columns: Sequence[str]):
        """
        Input source type for BigQuery Tables and Views.
        :param 'GoogleCloudAiplatformV1BigQuerySourceResponse' big_query_source: Immutable. The BigQuery source URI that points to either a BigQuery Table or View.
        :param Sequence[str] entity_id_columns: Optional. Columns to construct entity_id / row keys. Currently only supports 1 entity_id_column. If not provided defaults to `entity_id`.
        """
        pulumi.set(__self__, "big_query_source", big_query_source)
        pulumi.set(__self__, "entity_id_columns", entity_id_columns)

    @property
    @pulumi.getter(name="bigQuerySource")
    def big_query_source(self) -> 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse':
        """
        Immutable. The BigQuery source URI that points to either a BigQuery Table or View.
        """
        return pulumi.get(self, "big_query_source")

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Sequence[str]:
        """
        Optional. Columns to construct entity_id / row keys. Currently only supports 1 entity_id_column. If not provided defaults to `entity_id`.
        """
        return pulumi.get(self, "entity_id_columns")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureMonitoringStatsAnomalyResponse(dict):
    """
    A list of historical SnapshotAnalysis or ImportFeaturesAnalysis stats requested by user, sorted by FeatureStatsAnomaly.start_time descending.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureStatsAnomaly":
            suggest = "feature_stats_anomaly"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureMonitoringStatsAnomalyResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureMonitoringStatsAnomalyResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureMonitoringStatsAnomalyResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_stats_anomaly: 'outputs.GoogleCloudAiplatformV1FeatureStatsAnomalyResponse',
                 objective: str):
        """
        A list of historical SnapshotAnalysis or ImportFeaturesAnalysis stats requested by user, sorted by FeatureStatsAnomaly.start_time descending.
        :param 'GoogleCloudAiplatformV1FeatureStatsAnomalyResponse' feature_stats_anomaly: The stats and anomalies generated at specific timestamp.
        :param str objective: The objective for each stats.
        """
        pulumi.set(__self__, "feature_stats_anomaly", feature_stats_anomaly)
        pulumi.set(__self__, "objective", objective)

    @property
    @pulumi.getter(name="featureStatsAnomaly")
    def feature_stats_anomaly(self) -> 'outputs.GoogleCloudAiplatformV1FeatureStatsAnomalyResponse':
        """
        The stats and anomalies generated at specific timestamp.
        """
        return pulumi.get(self, "feature_stats_anomaly")

    @property
    @pulumi.getter
    def objective(self) -> str:
        """
        The objective for each stats.
        """
        return pulumi.get(self, "objective")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureResponse(dict):
    """
    Noise sigma for a single feature.
    """
    def __init__(__self__, *,
                 name: str,
                 sigma: float):
        """
        Noise sigma for a single feature.
        :param str name: The name of the input feature for which noise sigma is provided. The features are defined in explanation metadata inputs.
        :param float sigma: This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients. Similar to noise_sigma but represents the noise added to the current feature. Defaults to 0.1.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "sigma", sigma)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The name of the input feature for which noise sigma is provided. The features are defined in explanation metadata inputs.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def sigma(self) -> float:
        """
        This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients. Similar to noise_sigma but represents the noise added to the current feature. Defaults to 0.1.
        """
        return pulumi.get(self, "sigma")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureNoiseSigmaResponse(dict):
    """
    Noise sigma by features. Noise sigma represents the standard deviation of the gaussian kernel that will be used to add noise to interpolated inputs prior to computing gradients.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "noiseSigma":
            suggest = "noise_sigma"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureNoiseSigmaResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureNoiseSigmaResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureNoiseSigmaResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 noise_sigma: Sequence['outputs.GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureResponse']):
        """
        Noise sigma by features. Noise sigma represents the standard deviation of the gaussian kernel that will be used to add noise to interpolated inputs prior to computing gradients.
        :param Sequence['GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureResponse'] noise_sigma: Noise sigma per feature. No noise is added to features that are not set.
        """
        pulumi.set(__self__, "noise_sigma", noise_sigma)

    @property
    @pulumi.getter(name="noiseSigma")
    def noise_sigma(self) -> Sequence['outputs.GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureResponse']:
        """
        Noise sigma per feature. No noise is added to features that are not set.
        """
        return pulumi.get(self, "noise_sigma")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "cpuUtilizationTarget":
            suggest = "cpu_utilization_target"
        elif key == "maxNodeCount":
            suggest = "max_node_count"
        elif key == "minNodeCount":
            suggest = "min_node_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 cpu_utilization_target: int,
                 max_node_count: int,
                 min_node_count: int):
        """
        :param int cpu_utilization_target: Optional. A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        :param int max_node_count: The maximum number of nodes to scale up to. Must be greater than or equal to min_node_count, and less than or equal to 10 times of 'min_node_count'.
        :param int min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> int:
        """
        Optional. A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        return pulumi.get(self, "cpu_utilization_target")

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> int:
        """
        The maximum number of nodes to scale up to. Must be greater than or equal to min_node_count, and less than or equal to 10 times of 'min_node_count'.
        """
        return pulumi.get(self, "max_node_count")

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> int:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureOnlineStoreBigtableResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "autoScaling":
            suggest = "auto_scaling"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureOnlineStoreBigtableResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureOnlineStoreBigtableResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureOnlineStoreBigtableResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 auto_scaling: 'outputs.GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse'):
        """
        :param 'GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse' auto_scaling: Autoscaling config applied to Bigtable Instance.
        """
        pulumi.set(__self__, "auto_scaling", auto_scaling)

    @property
    @pulumi.getter(name="autoScaling")
    def auto_scaling(self) -> 'outputs.GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingResponse':
        """
        Autoscaling config applied to Bigtable Instance.
        """
        return pulumi.get(self, "auto_scaling")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureStatsAnomalyResponse(dict):
    """
    Stats and Anomaly generated at specific timestamp for specific Feature. The start_time and end_time are used to define the time range of the dataset that current stats belongs to, e.g. prediction traffic is bucketed into prediction datasets by time window. If the Dataset is not defined by time window, start_time = end_time. Timestamp of the stats and anomalies always refers to end_time. Raw stats and anomalies are stored in stats_uri or anomaly_uri in the tensorflow defined protos. Field data_stats contains almost identical information with the raw stats in Vertex AI defined proto, for UI to display.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "anomalyDetectionThreshold":
            suggest = "anomaly_detection_threshold"
        elif key == "anomalyUri":
            suggest = "anomaly_uri"
        elif key == "distributionDeviation":
            suggest = "distribution_deviation"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "startTime":
            suggest = "start_time"
        elif key == "statsUri":
            suggest = "stats_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureStatsAnomalyResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureStatsAnomalyResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureStatsAnomalyResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 anomaly_detection_threshold: float,
                 anomaly_uri: str,
                 distribution_deviation: float,
                 end_time: str,
                 score: float,
                 start_time: str,
                 stats_uri: str):
        """
        Stats and Anomaly generated at specific timestamp for specific Feature. The start_time and end_time are used to define the time range of the dataset that current stats belongs to, e.g. prediction traffic is bucketed into prediction datasets by time window. If the Dataset is not defined by time window, start_time = end_time. Timestamp of the stats and anomalies always refers to end_time. Raw stats and anomalies are stored in stats_uri or anomaly_uri in the tensorflow defined protos. Field data_stats contains almost identical information with the raw stats in Vertex AI defined proto, for UI to display.
        :param float anomaly_detection_threshold: This is the threshold used when detecting anomalies. The threshold can be changed by user, so this one might be different from ThresholdConfig.value.
        :param str anomaly_uri: Path of the anomaly file for current feature values in Cloud Storage bucket. Format: gs:////anomalies. Example: gs://monitoring_bucket/feature_name/anomalies. Stats are stored as binary format with Protobuf message Anoamlies are stored as binary format with Protobuf message [tensorflow.metadata.v0.AnomalyInfo] (https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/anomalies.proto).
        :param float distribution_deviation: Deviation from the current stats to baseline stats. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence.
        :param str end_time: The end timestamp of window where stats were generated. For objectives where time window doesn't make sense (e.g. Featurestore Snapshot Monitoring), end_time indicates the timestamp of the data used to generate stats (e.g. timestamp we take snapshots for feature values).
        :param float score: Feature importance score, only populated when cross-feature monitoring is enabled. For now only used to represent feature attribution score within range [0, 1] for ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_SKEW and ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_DRIFT.
        :param str start_time: The start timestamp of window where stats were generated. For objectives where time window doesn't make sense (e.g. Featurestore Snapshot Monitoring), start_time is only used to indicate the monitoring intervals, so it always equals to (end_time - monitoring_interval).
        :param str stats_uri: Path of the stats file for current feature values in Cloud Storage bucket. Format: gs:////stats. Example: gs://monitoring_bucket/feature_name/stats. Stats are stored as binary format with Protobuf message [tensorflow.metadata.v0.FeatureNameStatistics](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/statistics.proto).
        """
        pulumi.set(__self__, "anomaly_detection_threshold", anomaly_detection_threshold)
        pulumi.set(__self__, "anomaly_uri", anomaly_uri)
        pulumi.set(__self__, "distribution_deviation", distribution_deviation)
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "score", score)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "stats_uri", stats_uri)

    @property
    @pulumi.getter(name="anomalyDetectionThreshold")
    def anomaly_detection_threshold(self) -> float:
        """
        This is the threshold used when detecting anomalies. The threshold can be changed by user, so this one might be different from ThresholdConfig.value.
        """
        return pulumi.get(self, "anomaly_detection_threshold")

    @property
    @pulumi.getter(name="anomalyUri")
    def anomaly_uri(self) -> str:
        """
        Path of the anomaly file for current feature values in Cloud Storage bucket. Format: gs:////anomalies. Example: gs://monitoring_bucket/feature_name/anomalies. Stats are stored as binary format with Protobuf message Anoamlies are stored as binary format with Protobuf message [tensorflow.metadata.v0.AnomalyInfo] (https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/anomalies.proto).
        """
        return pulumi.get(self, "anomaly_uri")

    @property
    @pulumi.getter(name="distributionDeviation")
    def distribution_deviation(self) -> float:
        """
        Deviation from the current stats to baseline stats. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence.
        """
        return pulumi.get(self, "distribution_deviation")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        The end timestamp of window where stats were generated. For objectives where time window doesn't make sense (e.g. Featurestore Snapshot Monitoring), end_time indicates the timestamp of the data used to generate stats (e.g. timestamp we take snapshots for feature values).
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter
    def score(self) -> float:
        """
        Feature importance score, only populated when cross-feature monitoring is enabled. For now only used to represent feature attribution score within range [0, 1] for ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_SKEW and ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_DRIFT.
        """
        return pulumi.get(self, "score")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        The start timestamp of window where stats were generated. For objectives where time window doesn't make sense (e.g. Featurestore Snapshot Monitoring), start_time is only used to indicate the monitoring intervals, so it always equals to (end_time - monitoring_interval).
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter(name="statsUri")
    def stats_uri(self) -> str:
        """
        Path of the stats file for current feature values in Cloud Storage bucket. Format: gs:////stats. Example: gs://monitoring_bucket/feature_name/stats. Stats are stored as binary format with Protobuf message [tensorflow.metadata.v0.FeatureNameStatistics](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/statistics.proto).
        """
        return pulumi.get(self, "stats_uri")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureViewBigQuerySourceResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "entityIdColumns":
            suggest = "entity_id_columns"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureViewBigQuerySourceResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureViewBigQuerySourceResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureViewBigQuerySourceResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 entity_id_columns: Sequence[str],
                 uri: str):
        """
        :param Sequence[str] entity_id_columns: Columns to construct entity_id / row keys. Start by supporting 1 only.
        :param str uri: The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        pulumi.set(__self__, "entity_id_columns", entity_id_columns)
        pulumi.set(__self__, "uri", uri)

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Sequence[str]:
        """
        Columns to construct entity_id / row keys. Start by supporting 1 only.
        """
        return pulumi.get(self, "entity_id_columns")

    @property
    @pulumi.getter
    def uri(self) -> str:
        """
        The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        return pulumi.get(self, "uri")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse(dict):
    """
    Features belonging to a single feature group that will be synced to Online Store.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureGroupId":
            suggest = "feature_group_id"
        elif key == "featureIds":
            suggest = "feature_ids"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_group_id: str,
                 feature_ids: Sequence[str]):
        """
        Features belonging to a single feature group that will be synced to Online Store.
        :param str feature_group_id: Identifier of the feature group.
        :param Sequence[str] feature_ids: Identifiers of features under the feature group.
        """
        pulumi.set(__self__, "feature_group_id", feature_group_id)
        pulumi.set(__self__, "feature_ids", feature_ids)

    @property
    @pulumi.getter(name="featureGroupId")
    def feature_group_id(self) -> str:
        """
        Identifier of the feature group.
        """
        return pulumi.get(self, "feature_group_id")

    @property
    @pulumi.getter(name="featureIds")
    def feature_ids(self) -> Sequence[str]:
        """
        Identifiers of features under the feature group.
        """
        return pulumi.get(self, "feature_ids")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceResponse(dict):
    """
    A Feature Registry source for features that need to be synced to Online Store.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureGroups":
            suggest = "feature_groups"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_groups: Sequence['outputs.GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse']):
        """
        A Feature Registry source for features that need to be synced to Online Store.
        :param Sequence['GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse'] feature_groups: List of features that need to be synced to Online Store.
        """
        pulumi.set(__self__, "feature_groups", feature_groups)

    @property
    @pulumi.getter(name="featureGroups")
    def feature_groups(self) -> Sequence['outputs.GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupResponse']:
        """
        List of features that need to be synced to Online Store.
        """
        return pulumi.get(self, "feature_groups")


@pulumi.output_type
class GoogleCloudAiplatformV1FeatureViewSyncConfigResponse(dict):
    def __init__(__self__, *,
                 cron: str):
        """
        :param str cron: Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
        """
        pulumi.set(__self__, "cron", cron)

    @property
    @pulumi.getter
    def cron(self) -> str:
        """
        Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
        """
        return pulumi.get(self, "cron")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse(dict):
    """
    Configuration of the Featurestore's ImportFeature Analysis Based Monitoring. This type of analysis generates statistics for values of each Feature imported by every ImportFeatureValues operation.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "anomalyDetectionBaseline":
            suggest = "anomaly_detection_baseline"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 anomaly_detection_baseline: str,
                 state: str):
        """
        Configuration of the Featurestore's ImportFeature Analysis Based Monitoring. This type of analysis generates statistics for values of each Feature imported by every ImportFeatureValues operation.
        :param str anomaly_detection_baseline: The baseline used to do anomaly detection for the statistics generated by import features analysis.
        :param str state: Whether to enable / disable / inherite default hebavior for import features analysis.
        """
        pulumi.set(__self__, "anomaly_detection_baseline", anomaly_detection_baseline)
        pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="anomalyDetectionBaseline")
    def anomaly_detection_baseline(self) -> str:
        """
        The baseline used to do anomaly detection for the statistics generated by import features analysis.
        """
        return pulumi.get(self, "anomaly_detection_baseline")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        Whether to enable / disable / inherite default hebavior for import features analysis.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigResponse(dict):
    """
    Configuration of how features in Featurestore are monitored.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "categoricalThresholdConfig":
            suggest = "categorical_threshold_config"
        elif key == "importFeaturesAnalysis":
            suggest = "import_features_analysis"
        elif key == "numericalThresholdConfig":
            suggest = "numerical_threshold_config"
        elif key == "snapshotAnalysis":
            suggest = "snapshot_analysis"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeaturestoreMonitoringConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 categorical_threshold_config: 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse',
                 import_features_analysis: 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse',
                 numerical_threshold_config: 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse',
                 snapshot_analysis: 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse'):
        """
        Configuration of how features in Featurestore are monitored.
        :param 'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse' categorical_threshold_config: Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        :param 'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse' import_features_analysis: The config for ImportFeatures Analysis Based Feature Monitoring.
        :param 'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse' numerical_threshold_config: Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        :param 'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse' snapshot_analysis: The config for Snapshot Analysis Based Feature Monitoring.
        """
        pulumi.set(__self__, "categorical_threshold_config", categorical_threshold_config)
        pulumi.set(__self__, "import_features_analysis", import_features_analysis)
        pulumi.set(__self__, "numerical_threshold_config", numerical_threshold_config)
        pulumi.set(__self__, "snapshot_analysis", snapshot_analysis)

    @property
    @pulumi.getter(name="categoricalThresholdConfig")
    def categorical_threshold_config(self) -> 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse':
        """
        Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        """
        return pulumi.get(self, "categorical_threshold_config")

    @property
    @pulumi.getter(name="importFeaturesAnalysis")
    def import_features_analysis(self) -> 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisResponse':
        """
        The config for ImportFeatures Analysis Based Feature Monitoring.
        """
        return pulumi.get(self, "import_features_analysis")

    @property
    @pulumi.getter(name="numericalThresholdConfig")
    def numerical_threshold_config(self) -> 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse':
        """
        Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        """
        return pulumi.get(self, "numerical_threshold_config")

    @property
    @pulumi.getter(name="snapshotAnalysis")
    def snapshot_analysis(self) -> 'outputs.GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse':
        """
        The config for Snapshot Analysis Based Feature Monitoring.
        """
        return pulumi.get(self, "snapshot_analysis")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse(dict):
    """
    Configuration of the Featurestore's Snapshot Analysis Based Monitoring. This type of analysis generates statistics for each Feature based on a snapshot of the latest feature value of each entities every monitoring_interval.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "monitoringIntervalDays":
            suggest = "monitoring_interval_days"
        elif key == "stalenessDays":
            suggest = "staleness_days"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disabled: bool,
                 monitoring_interval_days: int,
                 staleness_days: int):
        """
        Configuration of the Featurestore's Snapshot Analysis Based Monitoring. This type of analysis generates statistics for each Feature based on a snapshot of the latest feature value of each entities every monitoring_interval.
        :param bool disabled: The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoring_interval for Features under it. Feature-level config: disabled = true indicates disabled regardless of the EntityType-level config; unset monitoring_interval indicates going with EntityType-level config; otherwise run snapshot analysis monitoring with monitoring_interval regardless of the EntityType-level config. Explicitly Disable the snapshot analysis based monitoring.
        :param int monitoring_interval_days: Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days.
        :param int staleness_days: Customized export features time window for snapshot analysis. Unit is one day. Default value is 3 weeks. Minimum value is 1 day. Maximum value is 4000 days.
        """
        pulumi.set(__self__, "disabled", disabled)
        pulumi.set(__self__, "monitoring_interval_days", monitoring_interval_days)
        pulumi.set(__self__, "staleness_days", staleness_days)

    @property
    @pulumi.getter
    def disabled(self) -> bool:
        """
        The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoring_interval for Features under it. Feature-level config: disabled = true indicates disabled regardless of the EntityType-level config; unset monitoring_interval indicates going with EntityType-level config; otherwise run snapshot analysis monitoring with monitoring_interval regardless of the EntityType-level config. Explicitly Disable the snapshot analysis based monitoring.
        """
        return pulumi.get(self, "disabled")

    @property
    @pulumi.getter(name="monitoringIntervalDays")
    def monitoring_interval_days(self) -> int:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days.
        """
        return pulumi.get(self, "monitoring_interval_days")

    @property
    @pulumi.getter(name="stalenessDays")
    def staleness_days(self) -> int:
        """
        Customized export features time window for snapshot analysis. Unit is one day. Default value is 3 weeks. Minimum value is 1 day. Maximum value is 4000 days.
        """
        return pulumi.get(self, "staleness_days")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigResponse(dict):
    """
    The config for Featurestore Monitoring threshold.
    """
    def __init__(__self__, *,
                 value: float):
        """
        The config for Featurestore Monitoring threshold.
        :param float value: Specify a threshold value that can trigger the alert. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> float:
        """
        Specify a threshold value that can trigger the alert. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigResponse(dict):
    """
    OnlineServingConfig specifies the details for provisioning online serving resources.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "fixedNodeCount":
            suggest = "fixed_node_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 fixed_node_count: int,
                 scaling: 'outputs.GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse'):
        """
        OnlineServingConfig specifies the details for provisioning online serving resources.
        :param int fixed_node_count: The number of nodes for the online store. The number of nodes doesn't scale automatically, but you can manually update the number of nodes. If set to 0, the featurestore will not have an online store and cannot be used for online serving.
        :param 'GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse' scaling: Online serving scaling configuration. Only one of `fixed_node_count` and `scaling` can be set. Setting one will reset the other.
        """
        pulumi.set(__self__, "fixed_node_count", fixed_node_count)
        pulumi.set(__self__, "scaling", scaling)

    @property
    @pulumi.getter(name="fixedNodeCount")
    def fixed_node_count(self) -> int:
        """
        The number of nodes for the online store. The number of nodes doesn't scale automatically, but you can manually update the number of nodes. If set to 0, the featurestore will not have an online store and cannot be used for online serving.
        """
        return pulumi.get(self, "fixed_node_count")

    @property
    @pulumi.getter
    def scaling(self) -> 'outputs.GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse':
        """
        Online serving scaling configuration. Only one of `fixed_node_count` and `scaling` can be set. Setting one will reset the other.
        """
        return pulumi.get(self, "scaling")


@pulumi.output_type
class GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse(dict):
    """
    Online serving scaling configuration. If min_node_count and max_node_count are set to the same value, the cluster will be configured with the fixed number of node (no auto-scaling).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "cpuUtilizationTarget":
            suggest = "cpu_utilization_target"
        elif key == "maxNodeCount":
            suggest = "max_node_count"
        elif key == "minNodeCount":
            suggest = "min_node_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 cpu_utilization_target: int,
                 max_node_count: int,
                 min_node_count: int):
        """
        Online serving scaling configuration. If min_node_count and max_node_count are set to the same value, the cluster will be configured with the fixed number of node (no auto-scaling).
        :param int cpu_utilization_target: Optional. The cpu utilization that the Autoscaler should be trying to achieve. This number is on a scale from 0 (no utilization) to 100 (total utilization), and is limited between 10 and 80. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set or set to 0, default to 50.
        :param int max_node_count: The maximum number of nodes to scale up to. Must be greater than min_node_count, and less than or equal to 10 times of 'min_node_count'.
        :param int min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)

    @property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> int:
        """
        Optional. The cpu utilization that the Autoscaler should be trying to achieve. This number is on a scale from 0 (no utilization) to 100 (total utilization), and is limited between 10 and 80. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set or set to 0, default to 50.
        """
        return pulumi.get(self, "cpu_utilization_target")

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> int:
        """
        The maximum number of nodes to scale up to. Must be greater than min_node_count, and less than or equal to 10 times of 'min_node_count'.
        """
        return pulumi.get(self, "max_node_count")

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> int:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")


@pulumi.output_type
class GoogleCloudAiplatformV1FilterSplitResponse(dict):
    """
    Assigns input data to training, validation, and test sets based on the given filters, data pieces not matched by any filter are ignored. Currently only supported for Datasets containing DataItems. If any of the filters in this message are to match nothing, then they can be set as '-' (the minus sign). Supported only for unstructured Datasets. 
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "testFilter":
            suggest = "test_filter"
        elif key == "trainingFilter":
            suggest = "training_filter"
        elif key == "validationFilter":
            suggest = "validation_filter"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FilterSplitResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FilterSplitResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FilterSplitResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 test_filter: str,
                 training_filter: str,
                 validation_filter: str):
        """
        Assigns input data to training, validation, and test sets based on the given filters, data pieces not matched by any filter are ignored. Currently only supported for Datasets containing DataItems. If any of the filters in this message are to match nothing, then they can be set as '-' (the minus sign). Supported only for unstructured Datasets. 
        :param str test_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to test the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        :param str training_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to train the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        :param str validation_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to validate the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        pulumi.set(__self__, "test_filter", test_filter)
        pulumi.set(__self__, "training_filter", training_filter)
        pulumi.set(__self__, "validation_filter", validation_filter)

    @property
    @pulumi.getter(name="testFilter")
    def test_filter(self) -> str:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to test the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "test_filter")

    @property
    @pulumi.getter(name="trainingFilter")
    def training_filter(self) -> str:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to train the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "training_filter")

    @property
    @pulumi.getter(name="validationFilter")
    def validation_filter(self) -> str:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to validate the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "validation_filter")


@pulumi.output_type
class GoogleCloudAiplatformV1FractionSplitResponse(dict):
    """
    Assigns the input data to training, validation, and test sets as per the given fractions. Any of `training_fraction`, `validation_fraction` and `test_fraction` may optionally be provided, they must sum to up to 1. If the provided ones sum to less than 1, the remainder is assigned to sets as decided by Vertex AI. If none of the fractions are set, by default roughly 80% of data is used for training, 10% for validation, and 10% for test.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "testFraction":
            suggest = "test_fraction"
        elif key == "trainingFraction":
            suggest = "training_fraction"
        elif key == "validationFraction":
            suggest = "validation_fraction"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1FractionSplitResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1FractionSplitResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1FractionSplitResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 test_fraction: float,
                 training_fraction: float,
                 validation_fraction: float):
        """
        Assigns the input data to training, validation, and test sets as per the given fractions. Any of `training_fraction`, `validation_fraction` and `test_fraction` may optionally be provided, they must sum to up to 1. If the provided ones sum to less than 1, the remainder is assigned to sets as decided by Vertex AI. If none of the fractions are set, by default roughly 80% of data is used for training, 10% for validation, and 10% for test.
        :param float test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param float training_fraction: The fraction of the input data that is to be used to train the Model.
        :param float validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        pulumi.set(__self__, "test_fraction", test_fraction)
        pulumi.set(__self__, "training_fraction", training_fraction)
        pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")


@pulumi.output_type
class GoogleCloudAiplatformV1GcsDestinationResponse(dict):
    """
    The Google Cloud Storage location where the output is to be written to.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "outputUriPrefix":
            suggest = "output_uri_prefix"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1GcsDestinationResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1GcsDestinationResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1GcsDestinationResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 output_uri_prefix: str):
        """
        The Google Cloud Storage location where the output is to be written to.
        :param str output_uri_prefix: Google Cloud Storage URI to output directory. If the uri doesn't end with '/', a '/' will be automatically appended. The directory is created if it doesn't exist.
        """
        pulumi.set(__self__, "output_uri_prefix", output_uri_prefix)

    @property
    @pulumi.getter(name="outputUriPrefix")
    def output_uri_prefix(self) -> str:
        """
        Google Cloud Storage URI to output directory. If the uri doesn't end with '/', a '/' will be automatically appended. The directory is created if it doesn't exist.
        """
        return pulumi.get(self, "output_uri_prefix")


@pulumi.output_type
class GoogleCloudAiplatformV1GcsSourceResponse(dict):
    """
    The Google Cloud Storage location for the input content.
    """
    def __init__(__self__, *,
                 uris: Sequence[str]):
        """
        The Google Cloud Storage location for the input content.
        :param Sequence[str] uris: Google Cloud Storage URI(-s) to the input file(s). May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.
        """
        pulumi.set(__self__, "uris", uris)

    @property
    @pulumi.getter
    def uris(self) -> Sequence[str]:
        """
        Google Cloud Storage URI(-s) to the input file(s). May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.
        """
        return pulumi.get(self, "uris")


@pulumi.output_type
class GoogleCloudAiplatformV1IndexPrivateEndpointsResponse(dict):
    """
    IndexPrivateEndpoints proto is used to provide paths for users to send requests via private endpoints (e.g. private service access, private service connect). To send request via private service access, use match_grpc_address. To send request via private service connect, use service_attachment.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "matchGrpcAddress":
            suggest = "match_grpc_address"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1IndexPrivateEndpointsResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1IndexPrivateEndpointsResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1IndexPrivateEndpointsResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 match_grpc_address: str,
                 service_attachment: str):
        """
        IndexPrivateEndpoints proto is used to provide paths for users to send requests via private endpoints (e.g. private service access, private service connect). To send request via private service access, use match_grpc_address. To send request via private service connect, use service_attachment.
        :param str match_grpc_address: The ip address used to send match gRPC requests.
        :param str service_attachment: The name of the service attachment resource. Populated if private service connect is enabled.
        """
        pulumi.set(__self__, "match_grpc_address", match_grpc_address)
        pulumi.set(__self__, "service_attachment", service_attachment)

    @property
    @pulumi.getter(name="matchGrpcAddress")
    def match_grpc_address(self) -> str:
        """
        The ip address used to send match gRPC requests.
        """
        return pulumi.get(self, "match_grpc_address")

    @property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> str:
        """
        The name of the service attachment resource. Populated if private service connect is enabled.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class GoogleCloudAiplatformV1IndexStatsResponse(dict):
    """
    Stats of the Index.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "shardsCount":
            suggest = "shards_count"
        elif key == "vectorsCount":
            suggest = "vectors_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1IndexStatsResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1IndexStatsResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1IndexStatsResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 shards_count: int,
                 vectors_count: str):
        """
        Stats of the Index.
        :param int shards_count: The number of shards in the Index.
        :param str vectors_count: The number of vectors in the Index.
        """
        pulumi.set(__self__, "shards_count", shards_count)
        pulumi.set(__self__, "vectors_count", vectors_count)

    @property
    @pulumi.getter(name="shardsCount")
    def shards_count(self) -> int:
        """
        The number of shards in the Index.
        """
        return pulumi.get(self, "shards_count")

    @property
    @pulumi.getter(name="vectorsCount")
    def vectors_count(self) -> str:
        """
        The number of vectors in the Index.
        """
        return pulumi.get(self, "vectors_count")


@pulumi.output_type
class GoogleCloudAiplatformV1InputDataConfigResponse(dict):
    """
    Specifies Vertex AI owned input data to be used for training, and possibly evaluating, the Model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "annotationSchemaUri":
            suggest = "annotation_schema_uri"
        elif key == "annotationsFilter":
            suggest = "annotations_filter"
        elif key == "bigqueryDestination":
            suggest = "bigquery_destination"
        elif key == "datasetId":
            suggest = "dataset_id"
        elif key == "filterSplit":
            suggest = "filter_split"
        elif key == "fractionSplit":
            suggest = "fraction_split"
        elif key == "gcsDestination":
            suggest = "gcs_destination"
        elif key == "persistMlUseAssignment":
            suggest = "persist_ml_use_assignment"
        elif key == "predefinedSplit":
            suggest = "predefined_split"
        elif key == "savedQueryId":
            suggest = "saved_query_id"
        elif key == "stratifiedSplit":
            suggest = "stratified_split"
        elif key == "timestampSplit":
            suggest = "timestamp_split"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1InputDataConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1InputDataConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1InputDataConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 annotation_schema_uri: str,
                 annotations_filter: str,
                 bigquery_destination: 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse',
                 dataset_id: str,
                 filter_split: 'outputs.GoogleCloudAiplatformV1FilterSplitResponse',
                 fraction_split: 'outputs.GoogleCloudAiplatformV1FractionSplitResponse',
                 gcs_destination: 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse',
                 persist_ml_use_assignment: bool,
                 predefined_split: 'outputs.GoogleCloudAiplatformV1PredefinedSplitResponse',
                 saved_query_id: str,
                 stratified_split: 'outputs.GoogleCloudAiplatformV1StratifiedSplitResponse',
                 timestamp_split: 'outputs.GoogleCloudAiplatformV1TimestampSplitResponse'):
        """
        Specifies Vertex AI owned input data to be used for training, and possibly evaluating, the Model.
        :param str annotation_schema_uri: Applicable only to custom training with Datasets that have DataItems and Annotations. Cloud Storage URI that points to a YAML file describing the annotation schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). The schema files that can be used here are found in gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the chosen schema must be consistent with metadata of the Dataset specified by dataset_id. Only Annotations that both match this schema and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both annotations_filter and annotation_schema_uri.
        :param str annotations_filter: Applicable only to Datasets that have DataItems and Annotations. A filter on Annotations of the Dataset. Only Annotations that both match this filter and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on (for the auto-assigned that role is decided by Vertex AI). A filter with same syntax as the one used in ListAnnotations may be used, but note here it filters across all Annotations of the Dataset, and not just within a single DataItem.
        :param 'GoogleCloudAiplatformV1BigQueryDestinationResponse' bigquery_destination: Only applicable to custom training with tabular Dataset with BigQuery source. The BigQuery project location where the training data is to be written to. In the given project a new dataset is created with name `dataset___` where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training input data is written into that dataset. In the dataset three tables are created, `training`, `validation` and `test`. * AIP_DATA_FORMAT = "bigquery". * AIP_TRAINING_DATA_URI = "bigquery_destination.dataset___.training" * AIP_VALIDATION_DATA_URI = "bigquery_destination.dataset___.validation" * AIP_TEST_DATA_URI = "bigquery_destination.dataset___.test"
        :param str dataset_id: The ID of the Dataset in the same Project and Location which data will be used to train the Model. The Dataset must use schema compatible with Model being trained, and what is compatible should be described in the used TrainingPipeline's training_task_definition. For tabular Datasets, all their data is exported to training, to pick and choose from.
        :param 'GoogleCloudAiplatformV1FilterSplitResponse' filter_split: Split based on the provided filters for each set.
        :param 'GoogleCloudAiplatformV1FractionSplitResponse' fraction_split: Split based on fractions defining the size of each set.
        :param 'GoogleCloudAiplatformV1GcsDestinationResponse' gcs_destination: The Cloud Storage location where the training data is to be written to. In the given directory a new directory is created with name: `dataset---` where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All training input data is written into that directory. The Vertex AI environment variables representing Cloud Storage data URIs are represented in the Cloud Storage wildcard format to support sharded data. e.g.: "gs://.../training-*.jsonl" * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data * AIP_TRAINING_DATA_URI = "gcs_destination/dataset---/training-*.${AIP_DATA_FORMAT}" * AIP_VALIDATION_DATA_URI = "gcs_destination/dataset---/validation-*.${AIP_DATA_FORMAT}" * AIP_TEST_DATA_URI = "gcs_destination/dataset---/test-*.${AIP_DATA_FORMAT}"
        :param bool persist_ml_use_assignment: Whether to persist the ML use assignment to data item system labels.
        :param 'GoogleCloudAiplatformV1PredefinedSplitResponse' predefined_split: Supported only for tabular Datasets. Split based on a predefined key.
        :param str saved_query_id: Only applicable to Datasets that have SavedQueries. The ID of a SavedQuery (annotation set) under the Dataset specified by dataset_id used for filtering Annotations for training. Only Annotations that are associated with this SavedQuery are used in respectively training. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both saved_query_id and annotations_filter. Only one of saved_query_id and annotation_schema_uri should be specified as both of them represent the same thing: problem type.
        :param 'GoogleCloudAiplatformV1StratifiedSplitResponse' stratified_split: Supported only for tabular Datasets. Split based on the distribution of the specified column.
        :param 'GoogleCloudAiplatformV1TimestampSplitResponse' timestamp_split: Supported only for tabular Datasets. Split based on the timestamp of the input data pieces.
        """
        pulumi.set(__self__, "annotation_schema_uri", annotation_schema_uri)
        pulumi.set(__self__, "annotations_filter", annotations_filter)
        pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        pulumi.set(__self__, "dataset_id", dataset_id)
        pulumi.set(__self__, "filter_split", filter_split)
        pulumi.set(__self__, "fraction_split", fraction_split)
        pulumi.set(__self__, "gcs_destination", gcs_destination)
        pulumi.set(__self__, "persist_ml_use_assignment", persist_ml_use_assignment)
        pulumi.set(__self__, "predefined_split", predefined_split)
        pulumi.set(__self__, "saved_query_id", saved_query_id)
        pulumi.set(__self__, "stratified_split", stratified_split)
        pulumi.set(__self__, "timestamp_split", timestamp_split)

    @property
    @pulumi.getter(name="annotationSchemaUri")
    def annotation_schema_uri(self) -> str:
        """
        Applicable only to custom training with Datasets that have DataItems and Annotations. Cloud Storage URI that points to a YAML file describing the annotation schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). The schema files that can be used here are found in gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the chosen schema must be consistent with metadata of the Dataset specified by dataset_id. Only Annotations that both match this schema and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both annotations_filter and annotation_schema_uri.
        """
        return pulumi.get(self, "annotation_schema_uri")

    @property
    @pulumi.getter(name="annotationsFilter")
    def annotations_filter(self) -> str:
        """
        Applicable only to Datasets that have DataItems and Annotations. A filter on Annotations of the Dataset. Only Annotations that both match this filter and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on (for the auto-assigned that role is decided by Vertex AI). A filter with same syntax as the one used in ListAnnotations may be used, but note here it filters across all Annotations of the Dataset, and not just within a single DataItem.
        """
        return pulumi.get(self, "annotations_filter")

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse':
        """
        Only applicable to custom training with tabular Dataset with BigQuery source. The BigQuery project location where the training data is to be written to. In the given project a new dataset is created with name `dataset___` where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training input data is written into that dataset. In the dataset three tables are created, `training`, `validation` and `test`. * AIP_DATA_FORMAT = "bigquery". * AIP_TRAINING_DATA_URI = "bigquery_destination.dataset___.training" * AIP_VALIDATION_DATA_URI = "bigquery_destination.dataset___.validation" * AIP_TEST_DATA_URI = "bigquery_destination.dataset___.test"
        """
        return pulumi.get(self, "bigquery_destination")

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> str:
        """
        The ID of the Dataset in the same Project and Location which data will be used to train the Model. The Dataset must use schema compatible with Model being trained, and what is compatible should be described in the used TrainingPipeline's training_task_definition. For tabular Datasets, all their data is exported to training, to pick and choose from.
        """
        return pulumi.get(self, "dataset_id")

    @property
    @pulumi.getter(name="filterSplit")
    def filter_split(self) -> 'outputs.GoogleCloudAiplatformV1FilterSplitResponse':
        """
        Split based on the provided filters for each set.
        """
        return pulumi.get(self, "filter_split")

    @property
    @pulumi.getter(name="fractionSplit")
    def fraction_split(self) -> 'outputs.GoogleCloudAiplatformV1FractionSplitResponse':
        """
        Split based on fractions defining the size of each set.
        """
        return pulumi.get(self, "fraction_split")

    @property
    @pulumi.getter(name="gcsDestination")
    def gcs_destination(self) -> 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse':
        """
        The Cloud Storage location where the training data is to be written to. In the given directory a new directory is created with name: `dataset---` where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All training input data is written into that directory. The Vertex AI environment variables representing Cloud Storage data URIs are represented in the Cloud Storage wildcard format to support sharded data. e.g.: "gs://.../training-*.jsonl" * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data * AIP_TRAINING_DATA_URI = "gcs_destination/dataset---/training-*.${AIP_DATA_FORMAT}" * AIP_VALIDATION_DATA_URI = "gcs_destination/dataset---/validation-*.${AIP_DATA_FORMAT}" * AIP_TEST_DATA_URI = "gcs_destination/dataset---/test-*.${AIP_DATA_FORMAT}"
        """
        return pulumi.get(self, "gcs_destination")

    @property
    @pulumi.getter(name="persistMlUseAssignment")
    def persist_ml_use_assignment(self) -> bool:
        """
        Whether to persist the ML use assignment to data item system labels.
        """
        return pulumi.get(self, "persist_ml_use_assignment")

    @property
    @pulumi.getter(name="predefinedSplit")
    def predefined_split(self) -> 'outputs.GoogleCloudAiplatformV1PredefinedSplitResponse':
        """
        Supported only for tabular Datasets. Split based on a predefined key.
        """
        return pulumi.get(self, "predefined_split")

    @property
    @pulumi.getter(name="savedQueryId")
    def saved_query_id(self) -> str:
        """
        Only applicable to Datasets that have SavedQueries. The ID of a SavedQuery (annotation set) under the Dataset specified by dataset_id used for filtering Annotations for training. Only Annotations that are associated with this SavedQuery are used in respectively training. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both saved_query_id and annotations_filter. Only one of saved_query_id and annotation_schema_uri should be specified as both of them represent the same thing: problem type.
        """
        return pulumi.get(self, "saved_query_id")

    @property
    @pulumi.getter(name="stratifiedSplit")
    def stratified_split(self) -> 'outputs.GoogleCloudAiplatformV1StratifiedSplitResponse':
        """
        Supported only for tabular Datasets. Split based on the distribution of the specified column.
        """
        return pulumi.get(self, "stratified_split")

    @property
    @pulumi.getter(name="timestampSplit")
    def timestamp_split(self) -> 'outputs.GoogleCloudAiplatformV1TimestampSplitResponse':
        """
        Supported only for tabular Datasets. Split based on the timestamp of the input data pieces.
        """
        return pulumi.get(self, "timestamp_split")


@pulumi.output_type
class GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse(dict):
    """
    An attribution method that computes the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "blurBaselineConfig":
            suggest = "blur_baseline_config"
        elif key == "smoothGradConfig":
            suggest = "smooth_grad_config"
        elif key == "stepCount":
            suggest = "step_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1IntegratedGradientsAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 blur_baseline_config: 'outputs.GoogleCloudAiplatformV1BlurBaselineConfigResponse',
                 smooth_grad_config: 'outputs.GoogleCloudAiplatformV1SmoothGradConfigResponse',
                 step_count: int):
        """
        An attribution method that computes the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param 'GoogleCloudAiplatformV1BlurBaselineConfigResponse' blur_baseline_config: Config for IG with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param 'GoogleCloudAiplatformV1SmoothGradConfigResponse' smooth_grad_config: Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        :param int step_count: The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        pulumi.set(__self__, "blur_baseline_config", blur_baseline_config)
        pulumi.set(__self__, "smooth_grad_config", smooth_grad_config)
        pulumi.set(__self__, "step_count", step_count)

    @property
    @pulumi.getter(name="blurBaselineConfig")
    def blur_baseline_config(self) -> 'outputs.GoogleCloudAiplatformV1BlurBaselineConfigResponse':
        """
        Config for IG with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        """
        return pulumi.get(self, "blur_baseline_config")

    @property
    @pulumi.getter(name="smoothGradConfig")
    def smooth_grad_config(self) -> 'outputs.GoogleCloudAiplatformV1SmoothGradConfigResponse':
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        return pulumi.get(self, "smooth_grad_config")

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> int:
        """
        The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        return pulumi.get(self, "step_count")


@pulumi.output_type
class GoogleCloudAiplatformV1MachineSpecResponse(dict):
    """
    Specification of a single machine.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorCount":
            suggest = "accelerator_count"
        elif key == "acceleratorType":
            suggest = "accelerator_type"
        elif key == "machineType":
            suggest = "machine_type"
        elif key == "tpuTopology":
            suggest = "tpu_topology"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1MachineSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1MachineSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1MachineSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_count: int,
                 accelerator_type: str,
                 machine_type: str,
                 tpu_topology: str):
        """
        Specification of a single machine.
        :param int accelerator_count: The number of accelerators to attach to the machine.
        :param str accelerator_type: Immutable. The type of accelerator(s) that may be attached to the machine as per accelerator_count.
        :param str machine_type: Immutable. The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required.
        :param str tpu_topology: Immutable. The topology of the TPUs. Corresponds to the TPU topologies available from GKE. (Example: tpu_topology: "2x2x1").
        """
        pulumi.set(__self__, "accelerator_count", accelerator_count)
        pulumi.set(__self__, "accelerator_type", accelerator_type)
        pulumi.set(__self__, "machine_type", machine_type)
        pulumi.set(__self__, "tpu_topology", tpu_topology)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> int:
        """
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> str:
        """
        Immutable. The type of accelerator(s) that may be attached to the machine as per accelerator_count.
        """
        return pulumi.get(self, "accelerator_type")

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> str:
        """
        Immutable. The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required.
        """
        return pulumi.get(self, "machine_type")

    @property
    @pulumi.getter(name="tpuTopology")
    def tpu_topology(self) -> str:
        """
        Immutable. The topology of the TPUs. Corresponds to the TPU topologies available from GKE. (Example: tpu_topology: "2x2x1").
        """
        return pulumi.get(self, "tpu_topology")


@pulumi.output_type
class GoogleCloudAiplatformV1ManualBatchTuningParametersResponse(dict):
    """
    Manual batch tuning parameters.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "batchSize":
            suggest = "batch_size"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ManualBatchTuningParametersResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ManualBatchTuningParametersResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ManualBatchTuningParametersResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 batch_size: int):
        """
        Manual batch tuning parameters.
        :param int batch_size: Immutable. The number of the records (e.g. instances) of the operation given in each batch to a machine replica. Machine type, and size of a single record should be considered when setting this parameter, higher value speeds up the batch operation's execution, but too high value will result in a whole batch not fitting in a machine's memory, and the whole operation will fail. The default value is 64.
        """
        pulumi.set(__self__, "batch_size", batch_size)

    @property
    @pulumi.getter(name="batchSize")
    def batch_size(self) -> int:
        """
        Immutable. The number of the records (e.g. instances) of the operation given in each batch to a machine replica. Machine type, and size of a single record should be considered when setting this parameter, higher value speeds up the batch operation's execution, but too high value will result in a whole batch not fitting in a machine's memory, and the whole operation will fail. The default value is 64.
        """
        return pulumi.get(self, "batch_size")


@pulumi.output_type
class GoogleCloudAiplatformV1MeasurementMetricResponse(dict):
    """
    A message representing a metric in the measurement.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricId":
            suggest = "metric_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1MeasurementMetricResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1MeasurementMetricResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1MeasurementMetricResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric_id: str,
                 value: float):
        """
        A message representing a metric in the measurement.
        :param str metric_id: The ID of the Metric. The Metric should be defined in StudySpec's Metrics.
        :param float value: The value for this metric.
        """
        pulumi.set(__self__, "metric_id", metric_id)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter(name="metricId")
    def metric_id(self) -> str:
        """
        The ID of the Metric. The Metric should be defined in StudySpec's Metrics.
        """
        return pulumi.get(self, "metric_id")

    @property
    @pulumi.getter
    def value(self) -> float:
        """
        The value for this metric.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudAiplatformV1MeasurementResponse(dict):
    """
    A message representing a Measurement of a Trial. A Measurement contains the Metrics got by executing a Trial using suggested hyperparameter values.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "elapsedDuration":
            suggest = "elapsed_duration"
        elif key == "stepCount":
            suggest = "step_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1MeasurementResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1MeasurementResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1MeasurementResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 elapsed_duration: str,
                 metrics: Sequence['outputs.GoogleCloudAiplatformV1MeasurementMetricResponse'],
                 step_count: str):
        """
        A message representing a Measurement of a Trial. A Measurement contains the Metrics got by executing a Trial using suggested hyperparameter values.
        :param str elapsed_duration: Time that the Trial has been running at the point of this Measurement.
        :param Sequence['GoogleCloudAiplatformV1MeasurementMetricResponse'] metrics: A list of metrics got by evaluating the objective functions using suggested Parameter values.
        :param str step_count: The number of steps the machine learning model has been trained for. Must be non-negative.
        """
        pulumi.set(__self__, "elapsed_duration", elapsed_duration)
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "step_count", step_count)

    @property
    @pulumi.getter(name="elapsedDuration")
    def elapsed_duration(self) -> str:
        """
        Time that the Trial has been running at the point of this Measurement.
        """
        return pulumi.get(self, "elapsed_duration")

    @property
    @pulumi.getter
    def metrics(self) -> Sequence['outputs.GoogleCloudAiplatformV1MeasurementMetricResponse']:
        """
        A list of metrics got by evaluating the objective functions using suggested Parameter values.
        """
        return pulumi.get(self, "metrics")

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> str:
        """
        The number of steps the machine learning model has been trained for. Must be non-negative.
        """
        return pulumi.get(self, "step_count")


@pulumi.output_type
class GoogleCloudAiplatformV1MetadataStoreMetadataStoreStateResponse(dict):
    """
    Represents state information for a MetadataStore.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "diskUtilizationBytes":
            suggest = "disk_utilization_bytes"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1MetadataStoreMetadataStoreStateResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1MetadataStoreMetadataStoreStateResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1MetadataStoreMetadataStoreStateResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disk_utilization_bytes: str):
        """
        Represents state information for a MetadataStore.
        :param str disk_utilization_bytes: The disk utilization of the MetadataStore in bytes.
        """
        pulumi.set(__self__, "disk_utilization_bytes", disk_utilization_bytes)

    @property
    @pulumi.getter(name="diskUtilizationBytes")
    def disk_utilization_bytes(self) -> str:
        """
        The disk utilization of the MetadataStore in bytes.
        """
        return pulumi.get(self, "disk_utilization_bytes")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelContainerSpecResponse(dict):
    """
    Specification of a container for serving predictions. Some fields in this message correspond to fields in the [Kubernetes Container v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "deploymentTimeout":
            suggest = "deployment_timeout"
        elif key == "healthProbe":
            suggest = "health_probe"
        elif key == "healthRoute":
            suggest = "health_route"
        elif key == "imageUri":
            suggest = "image_uri"
        elif key == "predictRoute":
            suggest = "predict_route"
        elif key == "sharedMemorySizeMb":
            suggest = "shared_memory_size_mb"
        elif key == "startupProbe":
            suggest = "startup_probe"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelContainerSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelContainerSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelContainerSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 args: Sequence[str],
                 command: Sequence[str],
                 deployment_timeout: str,
                 env: Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse'],
                 health_probe: 'outputs.GoogleCloudAiplatformV1ProbeResponse',
                 health_route: str,
                 image_uri: str,
                 ports: Sequence['outputs.GoogleCloudAiplatformV1PortResponse'],
                 predict_route: str,
                 shared_memory_size_mb: str,
                 startup_probe: 'outputs.GoogleCloudAiplatformV1ProbeResponse'):
        """
        Specification of a container for serving predictions. Some fields in this message correspond to fields in the [Kubernetes Container v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param Sequence[str] args: Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `command` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param Sequence[str] command: Immutable. Specifies the command that runs when the container starts. This overrides the container's [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param str deployment_timeout: Immutable. Deployment timeout. TODO (b/306244185): Revise documentation before exposing.
        :param Sequence['GoogleCloudAiplatformV1EnvVarResponse'] env: Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param 'GoogleCloudAiplatformV1ProbeResponse' health_probe: Immutable. Specification for Kubernetes readiness probe. TODO (b/306244185): Revise documentation before exposing.
        :param str health_route: Immutable. HTTP path on the container to send health checks to. Vertex AI intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health). For example, if you set this field to `/bar`, then Vertex AI intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/ DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param str image_uri: Immutable. URI of the Docker image to be used as the custom container for serving predictions. This URI must identify an image in Artifact Registry or Container Registry. Learn more about the [container publishing requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing), including permissions requirements for the Vertex AI Service Agent. The container image is ingested upon ModelService.UploadModel, stored internally, and this original path is afterwards not used. To learn about the requirements for the Docker image itself, see [Custom container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#). You can use the URI to one of Vertex AI's [pre-built container images for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) in this field.
        :param Sequence['GoogleCloudAiplatformV1PortResponse'] ports: Immutable. List of ports to expose from the container. Vertex AI sends any prediction requests that it receives to the first port on this list. Vertex AI also sends [liveness and health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` Vertex AI does not use ports other than the first one listed. This field corresponds to the `ports` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param str predict_route: Immutable. HTTP path on the container to send prediction requests to. Vertex AI forwards requests sent using projects.locations.endpoints.predict to this path on the container's IP address and port. Vertex AI then returns the container's response in the API response. For example, if you set this field to `/foo`, then when Vertex AI receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param str shared_memory_size_mb: Immutable. The amount of the VM memory to reserve as the shared memory for the model in megabytes. TODO (b/306244185): Revise documentation before exposing.
        :param 'GoogleCloudAiplatformV1ProbeResponse' startup_probe: Immutable. Specification for Kubernetes startup probe. TODO (b/306244185): Revise documentation before exposing.
        """
        pulumi.set(__self__, "args", args)
        pulumi.set(__self__, "command", command)
        pulumi.set(__self__, "deployment_timeout", deployment_timeout)
        pulumi.set(__self__, "env", env)
        pulumi.set(__self__, "health_probe", health_probe)
        pulumi.set(__self__, "health_route", health_route)
        pulumi.set(__self__, "image_uri", image_uri)
        pulumi.set(__self__, "ports", ports)
        pulumi.set(__self__, "predict_route", predict_route)
        pulumi.set(__self__, "shared_memory_size_mb", shared_memory_size_mb)
        pulumi.set(__self__, "startup_probe", startup_probe)

    @property
    @pulumi.getter
    def args(self) -> Sequence[str]:
        """
        Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `command` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "args")

    @property
    @pulumi.getter
    def command(self) -> Sequence[str]:
        """
        Immutable. Specifies the command that runs when the container starts. This overrides the container's [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "command")

    @property
    @pulumi.getter(name="deploymentTimeout")
    def deployment_timeout(self) -> str:
        """
        Immutable. Deployment timeout. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "deployment_timeout")

    @property
    @pulumi.getter
    def env(self) -> Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse']:
        """
        Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "env")

    @property
    @pulumi.getter(name="healthProbe")
    def health_probe(self) -> 'outputs.GoogleCloudAiplatformV1ProbeResponse':
        """
        Immutable. Specification for Kubernetes readiness probe. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "health_probe")

    @property
    @pulumi.getter(name="healthRoute")
    def health_route(self) -> str:
        """
        Immutable. HTTP path on the container to send health checks to. Vertex AI intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health). For example, if you set this field to `/bar`, then Vertex AI intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/ DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "health_route")

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> str:
        """
        Immutable. URI of the Docker image to be used as the custom container for serving predictions. This URI must identify an image in Artifact Registry or Container Registry. Learn more about the [container publishing requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing), including permissions requirements for the Vertex AI Service Agent. The container image is ingested upon ModelService.UploadModel, stored internally, and this original path is afterwards not used. To learn about the requirements for the Docker image itself, see [Custom container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#). You can use the URI to one of Vertex AI's [pre-built container images for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) in this field.
        """
        return pulumi.get(self, "image_uri")

    @property
    @pulumi.getter
    def ports(self) -> Sequence['outputs.GoogleCloudAiplatformV1PortResponse']:
        """
        Immutable. List of ports to expose from the container. Vertex AI sends any prediction requests that it receives to the first port on this list. Vertex AI also sends [liveness and health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` Vertex AI does not use ports other than the first one listed. This field corresponds to the `ports` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "ports")

    @property
    @pulumi.getter(name="predictRoute")
    def predict_route(self) -> str:
        """
        Immutable. HTTP path on the container to send prediction requests to. Vertex AI forwards requests sent using projects.locations.endpoints.predict to this path on the container's IP address and port. Vertex AI then returns the container's response in the API response. For example, if you set this field to `/foo`, then when Vertex AI receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "predict_route")

    @property
    @pulumi.getter(name="sharedMemorySizeMb")
    def shared_memory_size_mb(self) -> str:
        """
        Immutable. The amount of the VM memory to reserve as the shared memory for the model in megabytes. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "shared_memory_size_mb")

    @property
    @pulumi.getter(name="startupProbe")
    def startup_probe(self) -> 'outputs.GoogleCloudAiplatformV1ProbeResponse':
        """
        Immutable. Specification for Kubernetes startup probe. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "startup_probe")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringBigQueryTableResponse(dict):
    """
    ModelDeploymentMonitoringBigQueryTable specifies the BigQuery table name as well as some information of the logs stored in this table.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryTablePath":
            suggest = "bigquery_table_path"
        elif key == "logSource":
            suggest = "log_source"
        elif key == "logType":
            suggest = "log_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelDeploymentMonitoringBigQueryTableResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringBigQueryTableResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringBigQueryTableResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_table_path: str,
                 log_source: str,
                 log_type: str):
        """
        ModelDeploymentMonitoringBigQueryTable specifies the BigQuery table name as well as some information of the logs stored in this table.
        :param str bigquery_table_path: The created BigQuery table to store logs. Customer could do their own query & analysis. Format: `bq://.model_deployment_monitoring_._`
        :param str log_source: The source of log.
        :param str log_type: The type of log.
        """
        pulumi.set(__self__, "bigquery_table_path", bigquery_table_path)
        pulumi.set(__self__, "log_source", log_source)
        pulumi.set(__self__, "log_type", log_type)

    @property
    @pulumi.getter(name="bigqueryTablePath")
    def bigquery_table_path(self) -> str:
        """
        The created BigQuery table to store logs. Customer could do their own query & analysis. Format: `bq://.model_deployment_monitoring_._`
        """
        return pulumi.get(self, "bigquery_table_path")

    @property
    @pulumi.getter(name="logSource")
    def log_source(self) -> str:
        """
        The source of log.
        """
        return pulumi.get(self, "log_source")

    @property
    @pulumi.getter(name="logType")
    def log_type(self) -> str:
        """
        The type of log.
        """
        return pulumi.get(self, "log_type")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse(dict):
    """
    All metadata of most recent monitoring pipelines.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "runTime":
            suggest = "run_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 run_time: str,
                 status: 'outputs.GoogleRpcStatusResponse'):
        """
        All metadata of most recent monitoring pipelines.
        :param str run_time: The time that most recent monitoring pipelines that is related to this run.
        :param 'GoogleRpcStatusResponse' status: The status of the most recent monitoring pipeline.
        """
        pulumi.set(__self__, "run_time", run_time)
        pulumi.set(__self__, "status", status)

    @property
    @pulumi.getter(name="runTime")
    def run_time(self) -> str:
        """
        The time that most recent monitoring pipelines that is related to this run.
        """
        return pulumi.get(self, "run_time")

    @property
    @pulumi.getter
    def status(self) -> 'outputs.GoogleRpcStatusResponse':
        """
        The status of the most recent monitoring pipeline.
        """
        return pulumi.get(self, "status")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigResponse(dict):
    """
    ModelDeploymentMonitoringObjectiveConfig contains the pair of deployed_model_id to ModelMonitoringObjectiveConfig.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "deployedModelId":
            suggest = "deployed_model_id"
        elif key == "objectiveConfig":
            suggest = "objective_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 deployed_model_id: str,
                 objective_config: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse'):
        """
        ModelDeploymentMonitoringObjectiveConfig contains the pair of deployed_model_id to ModelMonitoringObjectiveConfig.
        :param str deployed_model_id: The DeployedModel ID of the objective config.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse' objective_config: The objective config of for the modelmonitoring job of this deployed model.
        """
        pulumi.set(__self__, "deployed_model_id", deployed_model_id)
        pulumi.set(__self__, "objective_config", objective_config)

    @property
    @pulumi.getter(name="deployedModelId")
    def deployed_model_id(self) -> str:
        """
        The DeployedModel ID of the objective config.
        """
        return pulumi.get(self, "deployed_model_id")

    @property
    @pulumi.getter(name="objectiveConfig")
    def objective_config(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse':
        """
        The objective config of for the modelmonitoring job of this deployed model.
        """
        return pulumi.get(self, "objective_config")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigResponse(dict):
    """
    The config for scheduling monitoring job.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "monitorInterval":
            suggest = "monitor_interval"
        elif key == "monitorWindow":
            suggest = "monitor_window"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 monitor_interval: str,
                 monitor_window: str):
        """
        The config for scheduling monitoring job.
        :param str monitor_interval: The model monitoring job scheduling interval. It will be rounded up to next full hour. This defines how often the monitoring jobs are triggered.
        :param str monitor_window: The time window of the prediction data being included in each prediction dataset. This window specifies how long the data should be collected from historical model results for each run. If not set, ModelDeploymentMonitoringScheduleConfig.monitor_interval will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the monitoring statistics.
        """
        pulumi.set(__self__, "monitor_interval", monitor_interval)
        pulumi.set(__self__, "monitor_window", monitor_window)

    @property
    @pulumi.getter(name="monitorInterval")
    def monitor_interval(self) -> str:
        """
        The model monitoring job scheduling interval. It will be rounded up to next full hour. This defines how often the monitoring jobs are triggered.
        """
        return pulumi.get(self, "monitor_interval")

    @property
    @pulumi.getter(name="monitorWindow")
    def monitor_window(self) -> str:
        """
        The time window of the prediction data being included in each prediction dataset. This window specifies how long the data should be collected from historical model results for each run. If not set, ModelDeploymentMonitoringScheduleConfig.monitor_interval will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the monitoring statistics.
        """
        return pulumi.get(self, "monitor_window")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelExportFormatResponse(dict):
    """
    Represents export format supported by the Model. All formats export to Google Cloud Storage.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exportableContents":
            suggest = "exportable_contents"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelExportFormatResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelExportFormatResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelExportFormatResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exportable_contents: Sequence[str]):
        """
        Represents export format supported by the Model. All formats export to Google Cloud Storage.
        :param Sequence[str] exportable_contents: The content of this Model that may be exported.
        """
        pulumi.set(__self__, "exportable_contents", exportable_contents)

    @property
    @pulumi.getter(name="exportableContents")
    def exportable_contents(self) -> Sequence[str]:
        """
        The content of this Model that may be exported.
        """
        return pulumi.get(self, "exportable_contents")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse(dict):
    """
    The config for email alert.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "userEmails":
            suggest = "user_emails"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 user_emails: Sequence[str]):
        """
        The config for email alert.
        :param Sequence[str] user_emails: The email addresses to send the alert.
        """
        pulumi.set(__self__, "user_emails", user_emails)

    @property
    @pulumi.getter(name="userEmails")
    def user_emails(self) -> Sequence[str]:
        """
        The email addresses to send the alert.
        """
        return pulumi.get(self, "user_emails")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringAlertConfigResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "emailAlertConfig":
            suggest = "email_alert_config"
        elif key == "enableLogging":
            suggest = "enable_logging"
        elif key == "notificationChannels":
            suggest = "notification_channels"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringAlertConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringAlertConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringAlertConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 email_alert_config: 'outputs.GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse',
                 enable_logging: bool,
                 notification_channels: Sequence[str]):
        """
        :param 'GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse' email_alert_config: Email alert config.
        :param bool enable_logging: Dump the anomalies to Cloud Logging. The anomalies will be put to json payload encoded from proto google.cloud.aiplatform.logging.ModelMonitoringAnomaliesLogEntry. This can be further sinked to Pub/Sub or any other services supported by Cloud Logging.
        :param Sequence[str] notification_channels: Resource names of the NotificationChannels to send alert. Must be of the format `projects//notificationChannels/`
        """
        pulumi.set(__self__, "email_alert_config", email_alert_config)
        pulumi.set(__self__, "enable_logging", enable_logging)
        pulumi.set(__self__, "notification_channels", notification_channels)

    @property
    @pulumi.getter(name="emailAlertConfig")
    def email_alert_config(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigResponse':
        """
        Email alert config.
        """
        return pulumi.get(self, "email_alert_config")

    @property
    @pulumi.getter(name="enableLogging")
    def enable_logging(self) -> bool:
        """
        Dump the anomalies to Cloud Logging. The anomalies will be put to json payload encoded from proto google.cloud.aiplatform.logging.ModelMonitoringAnomaliesLogEntry. This can be further sinked to Pub/Sub or any other services supported by Cloud Logging.
        """
        return pulumi.get(self, "enable_logging")

    @property
    @pulumi.getter(name="notificationChannels")
    def notification_channels(self) -> Sequence[str]:
        """
        Resource names of the NotificationChannels to send alert. Must be of the format `projects//notificationChannels/`
        """
        return pulumi.get(self, "notification_channels")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse(dict):
    """
    Output from BatchPredictionJob for Model Monitoring baseline dataset, which can be used to generate baseline attribution scores.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "predictionFormat":
            suggest = "prediction_format"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery: 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse',
                 gcs: 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse',
                 prediction_format: str):
        """
        Output from BatchPredictionJob for Model Monitoring baseline dataset, which can be used to generate baseline attribution scores.
        :param 'GoogleCloudAiplatformV1BigQueryDestinationResponse' bigquery: BigQuery location for BatchExplain output.
        :param 'GoogleCloudAiplatformV1GcsDestinationResponse' gcs: Cloud Storage location for BatchExplain output.
        :param str prediction_format: The storage format of the predictions generated BatchPrediction job.
        """
        pulumi.set(__self__, "bigquery", bigquery)
        pulumi.set(__self__, "gcs", gcs)
        pulumi.set(__self__, "prediction_format", prediction_format)

    @property
    @pulumi.getter
    def bigquery(self) -> 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse':
        """
        BigQuery location for BatchExplain output.
        """
        return pulumi.get(self, "bigquery")

    @property
    @pulumi.getter
    def gcs(self) -> 'outputs.GoogleCloudAiplatformV1GcsDestinationResponse':
        """
        Cloud Storage location for BatchExplain output.
        """
        return pulumi.get(self, "gcs")

    @property
    @pulumi.getter(name="predictionFormat")
    def prediction_format(self) -> str:
        """
        The storage format of the predictions generated BatchPrediction job.
        """
        return pulumi.get(self, "prediction_format")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse(dict):
    """
    The config for integrating with Vertex Explainable AI. Only applicable if the Model has explanation_spec populated.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableFeatureAttributes":
            suggest = "enable_feature_attributes"
        elif key == "explanationBaseline":
            suggest = "explanation_baseline"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_feature_attributes: bool,
                 explanation_baseline: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse'):
        """
        The config for integrating with Vertex Explainable AI. Only applicable if the Model has explanation_spec populated.
        :param bool enable_feature_attributes: If want to analyze the Vertex Explainable AI feature attribute scores or not. If set to true, Vertex AI will log the feature attributions from explain response and do the skew/drift detection for them.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse' explanation_baseline: Predictions generated by the BatchPredictionJob using baseline dataset.
        """
        pulumi.set(__self__, "enable_feature_attributes", enable_feature_attributes)
        pulumi.set(__self__, "explanation_baseline", explanation_baseline)

    @property
    @pulumi.getter(name="enableFeatureAttributes")
    def enable_feature_attributes(self) -> bool:
        """
        If want to analyze the Vertex Explainable AI feature attribute scores or not. If set to true, Vertex AI will log the feature attributions from explain response and do the skew/drift detection for them.
        """
        return pulumi.get(self, "enable_feature_attributes")

    @property
    @pulumi.getter(name="explanationBaseline")
    def explanation_baseline(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineResponse':
        """
        Predictions generated by the BatchPredictionJob using baseline dataset.
        """
        return pulumi.get(self, "explanation_baseline")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse(dict):
    """
    The config for Prediction data drift detection.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "attributionScoreDriftThresholds":
            suggest = "attribution_score_drift_thresholds"
        elif key == "defaultDriftThreshold":
            suggest = "default_drift_threshold"
        elif key == "driftThresholds":
            suggest = "drift_thresholds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 attribution_score_drift_thresholds: Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse'],
                 default_drift_threshold: 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse',
                 drift_thresholds: Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']):
        """
        The config for Prediction data drift detection.
        :param Mapping[str, 'GoogleCloudAiplatformV1ThresholdConfigResponse'] attribution_score_drift_thresholds: Key is the feature name and value is the threshold. The threshold here is against attribution score distance between different time windows.
        :param 'GoogleCloudAiplatformV1ThresholdConfigResponse' default_drift_threshold: Drift anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        :param Mapping[str, 'GoogleCloudAiplatformV1ThresholdConfigResponse'] drift_thresholds: Key is the feature name and value is the threshold. If a feature needs to be monitored for drift, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between different time windws.
        """
        pulumi.set(__self__, "attribution_score_drift_thresholds", attribution_score_drift_thresholds)
        pulumi.set(__self__, "default_drift_threshold", default_drift_threshold)
        pulumi.set(__self__, "drift_thresholds", drift_thresholds)

    @property
    @pulumi.getter(name="attributionScoreDriftThresholds")
    def attribution_score_drift_thresholds(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']:
        """
        Key is the feature name and value is the threshold. The threshold here is against attribution score distance between different time windows.
        """
        return pulumi.get(self, "attribution_score_drift_thresholds")

    @property
    @pulumi.getter(name="defaultDriftThreshold")
    def default_drift_threshold(self) -> 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse':
        """
        Drift anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        """
        return pulumi.get(self, "default_drift_threshold")

    @property
    @pulumi.getter(name="driftThresholds")
    def drift_thresholds(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']:
        """
        Key is the feature name and value is the threshold. If a feature needs to be monitored for drift, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between different time windws.
        """
        return pulumi.get(self, "drift_thresholds")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse(dict):
    """
    The objective configuration for model monitoring, including the information needed to detect anomalies for one particular model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "explanationConfig":
            suggest = "explanation_config"
        elif key == "predictionDriftDetectionConfig":
            suggest = "prediction_drift_detection_config"
        elif key == "trainingDataset":
            suggest = "training_dataset"
        elif key == "trainingPredictionSkewDetectionConfig":
            suggest = "training_prediction_skew_detection_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 explanation_config: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse',
                 prediction_drift_detection_config: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse',
                 training_dataset: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse',
                 training_prediction_skew_detection_config: 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse'):
        """
        The objective configuration for model monitoring, including the information needed to detect anomalies for one particular model.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse' explanation_config: The config for integrating with Vertex Explainable AI.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse' prediction_drift_detection_config: The config for drift of prediction data.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse' training_dataset: Training dataset for models. This field has to be set only if TrainingPredictionSkewDetectionConfig is specified.
        :param 'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse' training_prediction_skew_detection_config: The config for skew between training data and prediction data.
        """
        pulumi.set(__self__, "explanation_config", explanation_config)
        pulumi.set(__self__, "prediction_drift_detection_config", prediction_drift_detection_config)
        pulumi.set(__self__, "training_dataset", training_dataset)
        pulumi.set(__self__, "training_prediction_skew_detection_config", training_prediction_skew_detection_config)

    @property
    @pulumi.getter(name="explanationConfig")
    def explanation_config(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigResponse':
        """
        The config for integrating with Vertex Explainable AI.
        """
        return pulumi.get(self, "explanation_config")

    @property
    @pulumi.getter(name="predictionDriftDetectionConfig")
    def prediction_drift_detection_config(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigResponse':
        """
        The config for drift of prediction data.
        """
        return pulumi.get(self, "prediction_drift_detection_config")

    @property
    @pulumi.getter(name="trainingDataset")
    def training_dataset(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse':
        """
        Training dataset for models. This field has to be set only if TrainingPredictionSkewDetectionConfig is specified.
        """
        return pulumi.get(self, "training_dataset")

    @property
    @pulumi.getter(name="trainingPredictionSkewDetectionConfig")
    def training_prediction_skew_detection_config(self) -> 'outputs.GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse':
        """
        The config for skew between training data and prediction data.
        """
        return pulumi.get(self, "training_prediction_skew_detection_config")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse(dict):
    """
    Training Dataset information.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigquerySource":
            suggest = "bigquery_source"
        elif key == "dataFormat":
            suggest = "data_format"
        elif key == "gcsSource":
            suggest = "gcs_source"
        elif key == "loggingSamplingStrategy":
            suggest = "logging_sampling_strategy"
        elif key == "targetField":
            suggest = "target_field"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_source: 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse',
                 data_format: str,
                 dataset: str,
                 gcs_source: 'outputs.GoogleCloudAiplatformV1GcsSourceResponse',
                 logging_sampling_strategy: 'outputs.GoogleCloudAiplatformV1SamplingStrategyResponse',
                 target_field: str):
        """
        Training Dataset information.
        :param 'GoogleCloudAiplatformV1BigQuerySourceResponse' bigquery_source: The BigQuery table of the unmanaged Dataset used to train this Model.
        :param str data_format: Data format of the dataset, only applicable if the input is from Google Cloud Storage. The possible formats are: "tf-record" The source file is a TFRecord file. "csv" The source file is a CSV file. "jsonl" The source file is a JSONL file.
        :param str dataset: The resource name of the Dataset used to train this Model.
        :param 'GoogleCloudAiplatformV1GcsSourceResponse' gcs_source: The Google Cloud Storage uri of the unmanaged Dataset used to train this Model.
        :param 'GoogleCloudAiplatformV1SamplingStrategyResponse' logging_sampling_strategy: Strategy to sample data from Training Dataset. If not set, we process the whole dataset.
        :param str target_field: The target field name the model is to predict. This field will be excluded when doing Predict and (or) Explain for the training data.
        """
        pulumi.set(__self__, "bigquery_source", bigquery_source)
        pulumi.set(__self__, "data_format", data_format)
        pulumi.set(__self__, "dataset", dataset)
        pulumi.set(__self__, "gcs_source", gcs_source)
        pulumi.set(__self__, "logging_sampling_strategy", logging_sampling_strategy)
        pulumi.set(__self__, "target_field", target_field)

    @property
    @pulumi.getter(name="bigquerySource")
    def bigquery_source(self) -> 'outputs.GoogleCloudAiplatformV1BigQuerySourceResponse':
        """
        The BigQuery table of the unmanaged Dataset used to train this Model.
        """
        return pulumi.get(self, "bigquery_source")

    @property
    @pulumi.getter(name="dataFormat")
    def data_format(self) -> str:
        """
        Data format of the dataset, only applicable if the input is from Google Cloud Storage. The possible formats are: "tf-record" The source file is a TFRecord file. "csv" The source file is a CSV file. "jsonl" The source file is a JSONL file.
        """
        return pulumi.get(self, "data_format")

    @property
    @pulumi.getter
    def dataset(self) -> str:
        """
        The resource name of the Dataset used to train this Model.
        """
        return pulumi.get(self, "dataset")

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> 'outputs.GoogleCloudAiplatformV1GcsSourceResponse':
        """
        The Google Cloud Storage uri of the unmanaged Dataset used to train this Model.
        """
        return pulumi.get(self, "gcs_source")

    @property
    @pulumi.getter(name="loggingSamplingStrategy")
    def logging_sampling_strategy(self) -> 'outputs.GoogleCloudAiplatformV1SamplingStrategyResponse':
        """
        Strategy to sample data from Training Dataset. If not set, we process the whole dataset.
        """
        return pulumi.get(self, "logging_sampling_strategy")

    @property
    @pulumi.getter(name="targetField")
    def target_field(self) -> str:
        """
        The target field name the model is to predict. This field will be excluded when doing Predict and (or) Explain for the training data.
        """
        return pulumi.get(self, "target_field")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse(dict):
    """
    The config for Training & Prediction data skew detection. It specifies the training dataset sources and the skew detection parameters.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "attributionScoreSkewThresholds":
            suggest = "attribution_score_skew_thresholds"
        elif key == "defaultSkewThreshold":
            suggest = "default_skew_threshold"
        elif key == "skewThresholds":
            suggest = "skew_thresholds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 attribution_score_skew_thresholds: Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse'],
                 default_skew_threshold: 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse',
                 skew_thresholds: Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']):
        """
        The config for Training & Prediction data skew detection. It specifies the training dataset sources and the skew detection parameters.
        :param Mapping[str, 'GoogleCloudAiplatformV1ThresholdConfigResponse'] attribution_score_skew_thresholds: Key is the feature name and value is the threshold. The threshold here is against attribution score distance between the training and prediction feature.
        :param 'GoogleCloudAiplatformV1ThresholdConfigResponse' default_skew_threshold: Skew anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        :param Mapping[str, 'GoogleCloudAiplatformV1ThresholdConfigResponse'] skew_thresholds: Key is the feature name and value is the threshold. If a feature needs to be monitored for skew, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between the training and prediction feature.
        """
        pulumi.set(__self__, "attribution_score_skew_thresholds", attribution_score_skew_thresholds)
        pulumi.set(__self__, "default_skew_threshold", default_skew_threshold)
        pulumi.set(__self__, "skew_thresholds", skew_thresholds)

    @property
    @pulumi.getter(name="attributionScoreSkewThresholds")
    def attribution_score_skew_thresholds(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']:
        """
        Key is the feature name and value is the threshold. The threshold here is against attribution score distance between the training and prediction feature.
        """
        return pulumi.get(self, "attribution_score_skew_thresholds")

    @property
    @pulumi.getter(name="defaultSkewThreshold")
    def default_skew_threshold(self) -> 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse':
        """
        Skew anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        """
        return pulumi.get(self, "default_skew_threshold")

    @property
    @pulumi.getter(name="skewThresholds")
    def skew_thresholds(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ThresholdConfigResponse']:
        """
        Key is the feature name and value is the threshold. If a feature needs to be monitored for skew, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between the training and prediction feature.
        """
        return pulumi.get(self, "skew_thresholds")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelOriginalModelInfoResponse(dict):
    """
    Contains information about the original Model if this Model is a copy.
    """
    def __init__(__self__, *,
                 model: str):
        """
        Contains information about the original Model if this Model is a copy.
        :param str model: The resource name of the Model this Model is a copy of, including the revision. Format: `projects/{project}/locations/{location}/models/{model_id}@{version_id}`
        """
        pulumi.set(__self__, "model", model)

    @property
    @pulumi.getter
    def model(self) -> str:
        """
        The resource name of the Model this Model is a copy of, including the revision. Format: `projects/{project}/locations/{location}/models/{model_id}@{version_id}`
        """
        return pulumi.get(self, "model")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelResponse(dict):
    """
    A trained machine learning Model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "artifactUri":
            suggest = "artifact_uri"
        elif key == "containerSpec":
            suggest = "container_spec"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "deployedModels":
            suggest = "deployed_models"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "encryptionSpec":
            suggest = "encryption_spec"
        elif key == "explanationSpec":
            suggest = "explanation_spec"
        elif key == "metadataArtifact":
            suggest = "metadata_artifact"
        elif key == "metadataSchemaUri":
            suggest = "metadata_schema_uri"
        elif key == "modelSourceInfo":
            suggest = "model_source_info"
        elif key == "originalModelInfo":
            suggest = "original_model_info"
        elif key == "pipelineJob":
            suggest = "pipeline_job"
        elif key == "predictSchemata":
            suggest = "predict_schemata"
        elif key == "supportedDeploymentResourcesTypes":
            suggest = "supported_deployment_resources_types"
        elif key == "supportedExportFormats":
            suggest = "supported_export_formats"
        elif key == "supportedInputStorageFormats":
            suggest = "supported_input_storage_formats"
        elif key == "supportedOutputStorageFormats":
            suggest = "supported_output_storage_formats"
        elif key == "trainingPipeline":
            suggest = "training_pipeline"
        elif key == "updateTime":
            suggest = "update_time"
        elif key == "versionAliases":
            suggest = "version_aliases"
        elif key == "versionCreateTime":
            suggest = "version_create_time"
        elif key == "versionDescription":
            suggest = "version_description"
        elif key == "versionId":
            suggest = "version_id"
        elif key == "versionUpdateTime":
            suggest = "version_update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 artifact_uri: str,
                 container_spec: 'outputs.GoogleCloudAiplatformV1ModelContainerSpecResponse',
                 create_time: str,
                 deployed_models: Sequence['outputs.GoogleCloudAiplatformV1DeployedModelRefResponse'],
                 description: str,
                 display_name: str,
                 encryption_spec: 'outputs.GoogleCloudAiplatformV1EncryptionSpecResponse',
                 etag: str,
                 explanation_spec: 'outputs.GoogleCloudAiplatformV1ExplanationSpecResponse',
                 labels: Mapping[str, str],
                 metadata: Any,
                 metadata_artifact: str,
                 metadata_schema_uri: str,
                 model_source_info: 'outputs.GoogleCloudAiplatformV1ModelSourceInfoResponse',
                 name: str,
                 original_model_info: 'outputs.GoogleCloudAiplatformV1ModelOriginalModelInfoResponse',
                 pipeline_job: str,
                 predict_schemata: 'outputs.GoogleCloudAiplatformV1PredictSchemataResponse',
                 supported_deployment_resources_types: Sequence[str],
                 supported_export_formats: Sequence['outputs.GoogleCloudAiplatformV1ModelExportFormatResponse'],
                 supported_input_storage_formats: Sequence[str],
                 supported_output_storage_formats: Sequence[str],
                 training_pipeline: str,
                 update_time: str,
                 version_aliases: Sequence[str],
                 version_create_time: str,
                 version_description: str,
                 version_id: str,
                 version_update_time: str):
        """
        A trained machine learning Model.
        :param str artifact_uri: Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models or Large Models.
        :param 'GoogleCloudAiplatformV1ModelContainerSpecResponse' container_spec: Input only. The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models or Large Models.
        :param str create_time: Timestamp when this Model was uploaded into Vertex AI.
        :param Sequence['GoogleCloudAiplatformV1DeployedModelRefResponse'] deployed_models: The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.
        :param str description: The description of the Model.
        :param str display_name: The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param 'GoogleCloudAiplatformV1EncryptionSpecResponse' encryption_spec: Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
        :param str etag: Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        :param 'GoogleCloudAiplatformV1ExplanationSpecResponse' explanation_spec: The default explanation specification for this Model. The Model can be used for requesting explanation after being deployed if it is populated. The Model can be used for batch explanation if it is populated. All fields of the explanation_spec can be overridden by explanation_spec of DeployModelRequest.deployed_model, or explanation_spec of BatchPredictionJob. If the default explanation specification is not set for this Model, this Model can still be used for requesting explanation by setting explanation_spec of DeployModelRequest.deployed_model and for batch explanation by setting explanation_spec of BatchPredictionJob.
        :param Mapping[str, str] labels: The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        :param Any metadata: Immutable. An additional information about the Model; the schema of the metadata can be found in metadata_schema. Unset if the Model does not have any additional information.
        :param str metadata_artifact: The resource name of the Artifact that was created in MetadataStore when creating the Model. The Artifact resource name pattern is `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
        :param str metadata_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it. Unset if the Model does not have any additional information. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no additional metadata is needed, this field is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param 'GoogleCloudAiplatformV1ModelSourceInfoResponse' model_source_info: Source of a model. It can either be automl training pipeline, custom training pipeline, BigQuery ML, or existing Vertex AI Model.
        :param str name: The resource name of the Model.
        :param 'GoogleCloudAiplatformV1ModelOriginalModelInfoResponse' original_model_info: If this Model is a copy of another Model, this contains info about the original.
        :param str pipeline_job: Optional. This field is populated if the model is produced by a pipeline job.
        :param 'GoogleCloudAiplatformV1PredictSchemataResponse' predict_schemata: The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.
        :param Sequence[str] supported_deployment_resources_types: When this Model is deployed, its prediction resources are described by the `prediction_resources` field of the Endpoint.deployed_models object. Because not all Models support all resource configuration types, the configuration types this Model supports are listed here. If no configuration types are listed, the Model cannot be deployed to an Endpoint and does not support online predictions (PredictionService.Predict or PredictionService.Explain). Such a Model can serve predictions by using a BatchPredictionJob, if it has at least one entry each in supported_input_storage_formats and supported_output_storage_formats.
        :param Sequence['GoogleCloudAiplatformV1ModelExportFormatResponse'] supported_export_formats: The formats in which this Model may be exported. If empty, this Model is not available for export.
        :param Sequence[str] supported_input_storage_formats: The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema. The possible formats are: * `jsonl` The JSON Lines format, where each instance is a single line. Uses GcsSource. * `csv` The CSV format, where each instance is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsSource. * `tf-record` The TFRecord format, where each instance is a single record in tfrecord syntax. Uses GcsSource. * `tf-record-gzip` Similar to `tf-record`, but the file is gzipped. Uses GcsSource. * `bigquery` Each instance is a single row in BigQuery. Uses BigQuerySource. * `file-list` Each line of the file is the location of an instance to process, uses `gcs_source` field of the InputConfig object. If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain.
        :param Sequence[str] supported_output_storage_formats: The formats this Model supports in BatchPredictionJob.output_config. If both PredictSchemata.instance_schema_uri and PredictSchemata.prediction_schema_uri exist, the predictions are returned together with their instances. In other words, the prediction has the original instance data first, followed by the actual prediction content (as per the schema). The possible formats are: * `jsonl` The JSON Lines format, where each prediction is a single line. Uses GcsDestination. * `csv` The CSV format, where each prediction is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsDestination. * `bigquery` Each prediction is a single row in a BigQuery table, uses BigQueryDestination . If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain.
        :param str training_pipeline: The resource name of the TrainingPipeline that uploaded this Model, if any.
        :param str update_time: Timestamp when this Model was most recently updated.
        :param Sequence[str] version_aliases: User provided version aliases so that a model version can be referenced via alias (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_alias}` instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_id})`. The format is a-z{0,126}[a-z0-9] to distinguish from version_id. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.
        :param str version_create_time: Timestamp when this version was created.
        :param str version_description: The description of this version.
        :param str version_id: Immutable. The version ID of the model. A new version is committed when a new model version is uploaded or trained under an existing model id. It is an auto-incrementing decimal number in string representation.
        :param str version_update_time: Timestamp when this version was most recently updated.
        """
        pulumi.set(__self__, "artifact_uri", artifact_uri)
        pulumi.set(__self__, "container_spec", container_spec)
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "deployed_models", deployed_models)
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "encryption_spec", encryption_spec)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "explanation_spec", explanation_spec)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "metadata_artifact", metadata_artifact)
        pulumi.set(__self__, "metadata_schema_uri", metadata_schema_uri)
        pulumi.set(__self__, "model_source_info", model_source_info)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "original_model_info", original_model_info)
        pulumi.set(__self__, "pipeline_job", pipeline_job)
        pulumi.set(__self__, "predict_schemata", predict_schemata)
        pulumi.set(__self__, "supported_deployment_resources_types", supported_deployment_resources_types)
        pulumi.set(__self__, "supported_export_formats", supported_export_formats)
        pulumi.set(__self__, "supported_input_storage_formats", supported_input_storage_formats)
        pulumi.set(__self__, "supported_output_storage_formats", supported_output_storage_formats)
        pulumi.set(__self__, "training_pipeline", training_pipeline)
        pulumi.set(__self__, "update_time", update_time)
        pulumi.set(__self__, "version_aliases", version_aliases)
        pulumi.set(__self__, "version_create_time", version_create_time)
        pulumi.set(__self__, "version_description", version_description)
        pulumi.set(__self__, "version_id", version_id)
        pulumi.set(__self__, "version_update_time", version_update_time)

    @property
    @pulumi.getter(name="artifactUri")
    def artifact_uri(self) -> str:
        """
        Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models or Large Models.
        """
        return pulumi.get(self, "artifact_uri")

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> 'outputs.GoogleCloudAiplatformV1ModelContainerSpecResponse':
        """
        Input only. The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models or Large Models.
        """
        return pulumi.get(self, "container_spec")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this Model was uploaded into Vertex AI.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="deployedModels")
    def deployed_models(self) -> Sequence['outputs.GoogleCloudAiplatformV1DeployedModelRefResponse']:
        """
        The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.
        """
        return pulumi.get(self, "deployed_models")

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        The description of the Model.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> 'outputs.GoogleCloudAiplatformV1EncryptionSpecResponse':
        """
        Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter(name="explanationSpec")
    def explanation_spec(self) -> 'outputs.GoogleCloudAiplatformV1ExplanationSpecResponse':
        """
        The default explanation specification for this Model. The Model can be used for requesting explanation after being deployed if it is populated. The Model can be used for batch explanation if it is populated. All fields of the explanation_spec can be overridden by explanation_spec of DeployModelRequest.deployed_model, or explanation_spec of BatchPredictionJob. If the default explanation specification is not set for this Model, this Model can still be used for requesting explanation by setting explanation_spec of DeployModelRequest.deployed_model and for batch explanation by setting explanation_spec of BatchPredictionJob.
        """
        return pulumi.get(self, "explanation_spec")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter
    def metadata(self) -> Any:
        """
        Immutable. An additional information about the Model; the schema of the metadata can be found in metadata_schema. Unset if the Model does not have any additional information.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter(name="metadataArtifact")
    def metadata_artifact(self) -> str:
        """
        The resource name of the Artifact that was created in MetadataStore when creating the Model. The Artifact resource name pattern is `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
        """
        return pulumi.get(self, "metadata_artifact")

    @property
    @pulumi.getter(name="metadataSchemaUri")
    def metadata_schema_uri(self) -> str:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it. Unset if the Model does not have any additional information. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no additional metadata is needed, this field is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "metadata_schema_uri")

    @property
    @pulumi.getter(name="modelSourceInfo")
    def model_source_info(self) -> 'outputs.GoogleCloudAiplatformV1ModelSourceInfoResponse':
        """
        Source of a model. It can either be automl training pipeline, custom training pipeline, BigQuery ML, or existing Vertex AI Model.
        """
        return pulumi.get(self, "model_source_info")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The resource name of the Model.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="originalModelInfo")
    def original_model_info(self) -> 'outputs.GoogleCloudAiplatformV1ModelOriginalModelInfoResponse':
        """
        If this Model is a copy of another Model, this contains info about the original.
        """
        return pulumi.get(self, "original_model_info")

    @property
    @pulumi.getter(name="pipelineJob")
    def pipeline_job(self) -> str:
        """
        Optional. This field is populated if the model is produced by a pipeline job.
        """
        return pulumi.get(self, "pipeline_job")

    @property
    @pulumi.getter(name="predictSchemata")
    def predict_schemata(self) -> 'outputs.GoogleCloudAiplatformV1PredictSchemataResponse':
        """
        The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.
        """
        return pulumi.get(self, "predict_schemata")

    @property
    @pulumi.getter(name="supportedDeploymentResourcesTypes")
    def supported_deployment_resources_types(self) -> Sequence[str]:
        """
        When this Model is deployed, its prediction resources are described by the `prediction_resources` field of the Endpoint.deployed_models object. Because not all Models support all resource configuration types, the configuration types this Model supports are listed here. If no configuration types are listed, the Model cannot be deployed to an Endpoint and does not support online predictions (PredictionService.Predict or PredictionService.Explain). Such a Model can serve predictions by using a BatchPredictionJob, if it has at least one entry each in supported_input_storage_formats and supported_output_storage_formats.
        """
        return pulumi.get(self, "supported_deployment_resources_types")

    @property
    @pulumi.getter(name="supportedExportFormats")
    def supported_export_formats(self) -> Sequence['outputs.GoogleCloudAiplatformV1ModelExportFormatResponse']:
        """
        The formats in which this Model may be exported. If empty, this Model is not available for export.
        """
        return pulumi.get(self, "supported_export_formats")

    @property
    @pulumi.getter(name="supportedInputStorageFormats")
    def supported_input_storage_formats(self) -> Sequence[str]:
        """
        The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema. The possible formats are: * `jsonl` The JSON Lines format, where each instance is a single line. Uses GcsSource. * `csv` The CSV format, where each instance is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsSource. * `tf-record` The TFRecord format, where each instance is a single record in tfrecord syntax. Uses GcsSource. * `tf-record-gzip` Similar to `tf-record`, but the file is gzipped. Uses GcsSource. * `bigquery` Each instance is a single row in BigQuery. Uses BigQuerySource. * `file-list` Each line of the file is the location of an instance to process, uses `gcs_source` field of the InputConfig object. If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain.
        """
        return pulumi.get(self, "supported_input_storage_formats")

    @property
    @pulumi.getter(name="supportedOutputStorageFormats")
    def supported_output_storage_formats(self) -> Sequence[str]:
        """
        The formats this Model supports in BatchPredictionJob.output_config. If both PredictSchemata.instance_schema_uri and PredictSchemata.prediction_schema_uri exist, the predictions are returned together with their instances. In other words, the prediction has the original instance data first, followed by the actual prediction content (as per the schema). The possible formats are: * `jsonl` The JSON Lines format, where each prediction is a single line. Uses GcsDestination. * `csv` The CSV format, where each prediction is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsDestination. * `bigquery` Each prediction is a single row in a BigQuery table, uses BigQueryDestination . If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain.
        """
        return pulumi.get(self, "supported_output_storage_formats")

    @property
    @pulumi.getter(name="trainingPipeline")
    def training_pipeline(self) -> str:
        """
        The resource name of the TrainingPipeline that uploaded this Model, if any.
        """
        return pulumi.get(self, "training_pipeline")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this Model was most recently updated.
        """
        return pulumi.get(self, "update_time")

    @property
    @pulumi.getter(name="versionAliases")
    def version_aliases(self) -> Sequence[str]:
        """
        User provided version aliases so that a model version can be referenced via alias (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_alias}` instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_id})`. The format is a-z{0,126}[a-z0-9] to distinguish from version_id. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.
        """
        return pulumi.get(self, "version_aliases")

    @property
    @pulumi.getter(name="versionCreateTime")
    def version_create_time(self) -> str:
        """
        Timestamp when this version was created.
        """
        return pulumi.get(self, "version_create_time")

    @property
    @pulumi.getter(name="versionDescription")
    def version_description(self) -> str:
        """
        The description of this version.
        """
        return pulumi.get(self, "version_description")

    @property
    @pulumi.getter(name="versionId")
    def version_id(self) -> str:
        """
        Immutable. The version ID of the model. A new version is committed when a new model version is uploaded or trained under an existing model id. It is an auto-incrementing decimal number in string representation.
        """
        return pulumi.get(self, "version_id")

    @property
    @pulumi.getter(name="versionUpdateTime")
    def version_update_time(self) -> str:
        """
        Timestamp when this version was most recently updated.
        """
        return pulumi.get(self, "version_update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1ModelSourceInfoResponse(dict):
    """
    Detail description of the source information of the model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sourceType":
            suggest = "source_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ModelSourceInfoResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ModelSourceInfoResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ModelSourceInfoResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 copy: bool,
                 source_type: str):
        """
        Detail description of the source information of the model.
        :param bool copy: If this Model is copy of another Model. If true then source_type pertains to the original.
        :param str source_type: Type of the model source.
        """
        pulumi.set(__self__, "copy", copy)
        pulumi.set(__self__, "source_type", source_type)

    @property
    @pulumi.getter
    def copy(self) -> bool:
        """
        If this Model is copy of another Model. If true then source_type pertains to the original.
        """
        return pulumi.get(self, "copy")

    @property
    @pulumi.getter(name="sourceType")
    def source_type(self) -> str:
        """
        Type of the model source.
        """
        return pulumi.get(self, "source_type")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse(dict):
    """
    The output of a multi-trial Neural Architecture Search (NAS) jobs.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "searchTrials":
            suggest = "search_trials"
        elif key == "trainTrials":
            suggest = "train_trials"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 search_trials: Sequence['outputs.GoogleCloudAiplatformV1NasTrialResponse'],
                 train_trials: Sequence['outputs.GoogleCloudAiplatformV1NasTrialResponse']):
        """
        The output of a multi-trial Neural Architecture Search (NAS) jobs.
        :param Sequence['GoogleCloudAiplatformV1NasTrialResponse'] search_trials: List of NasTrials that were started as part of search stage.
        :param Sequence['GoogleCloudAiplatformV1NasTrialResponse'] train_trials: List of NasTrials that were started as part of train stage.
        """
        pulumi.set(__self__, "search_trials", search_trials)
        pulumi.set(__self__, "train_trials", train_trials)

    @property
    @pulumi.getter(name="searchTrials")
    def search_trials(self) -> Sequence['outputs.GoogleCloudAiplatformV1NasTrialResponse']:
        """
        List of NasTrials that were started as part of search stage.
        """
        return pulumi.get(self, "search_trials")

    @property
    @pulumi.getter(name="trainTrials")
    def train_trials(self) -> Sequence['outputs.GoogleCloudAiplatformV1NasTrialResponse']:
        """
        List of NasTrials that were started as part of train stage.
        """
        return pulumi.get(self, "train_trials")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobOutputResponse(dict):
    """
    Represents a uCAIP NasJob output.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "multiTrialJobOutput":
            suggest = "multi_trial_job_output"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 multi_trial_job_output: 'outputs.GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse'):
        """
        Represents a uCAIP NasJob output.
        :param 'GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse' multi_trial_job_output: The output of this multi-trial Neural Architecture Search (NAS) job.
        """
        pulumi.set(__self__, "multi_trial_job_output", multi_trial_job_output)

    @property
    @pulumi.getter(name="multiTrialJobOutput")
    def multi_trial_job_output(self) -> 'outputs.GoogleCloudAiplatformV1NasJobOutputMultiTrialJobOutputResponse':
        """
        The output of this multi-trial Neural Architecture Search (NAS) job.
        """
        return pulumi.get(self, "multi_trial_job_output")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse(dict):
    """
    Represents a metric to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricId":
            suggest = "metric_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 goal: str,
                 metric_id: str):
        """
        Represents a metric to optimize.
        :param str goal: The optimization goal of the metric.
        :param str metric_id: The ID of the metric. Must not contain whitespaces.
        """
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "metric_id", metric_id)

    @property
    @pulumi.getter
    def goal(self) -> str:
        """
        The optimization goal of the metric.
        """
        return pulumi.get(self, "goal")

    @property
    @pulumi.getter(name="metricId")
    def metric_id(self) -> str:
        """
        The ID of the metric. Must not contain whitespaces.
        """
        return pulumi.get(self, "metric_id")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse(dict):
    """
    The spec of multi-trial Neural Architecture Search (NAS).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "multiTrialAlgorithm":
            suggest = "multi_trial_algorithm"
        elif key == "searchTrialSpec":
            suggest = "search_trial_spec"
        elif key == "trainTrialSpec":
            suggest = "train_trial_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 metric: 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse',
                 multi_trial_algorithm: str,
                 search_trial_spec: 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse',
                 train_trial_spec: 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse'):
        """
        The spec of multi-trial Neural Architecture Search (NAS).
        :param 'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse' metric: Metric specs for the NAS job. Validation for this field is done at `multi_trial_algorithm_spec` field.
        :param str multi_trial_algorithm: The multi-trial Neural Architecture Search (NAS) algorithm type. Defaults to `REINFORCEMENT_LEARNING`.
        :param 'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse' search_trial_spec: Spec for search trials.
        :param 'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse' train_trial_spec: Spec for train trials. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        pulumi.set(__self__, "metric", metric)
        pulumi.set(__self__, "multi_trial_algorithm", multi_trial_algorithm)
        pulumi.set(__self__, "search_trial_spec", search_trial_spec)
        pulumi.set(__self__, "train_trial_spec", train_trial_spec)

    @property
    @pulumi.getter
    def metric(self) -> 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecResponse':
        """
        Metric specs for the NAS job. Validation for this field is done at `multi_trial_algorithm_spec` field.
        """
        return pulumi.get(self, "metric")

    @property
    @pulumi.getter(name="multiTrialAlgorithm")
    def multi_trial_algorithm(self) -> str:
        """
        The multi-trial Neural Architecture Search (NAS) algorithm type. Defaults to `REINFORCEMENT_LEARNING`.
        """
        return pulumi.get(self, "multi_trial_algorithm")

    @property
    @pulumi.getter(name="searchTrialSpec")
    def search_trial_spec(self) -> 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse':
        """
        Spec for search trials.
        """
        return pulumi.get(self, "search_trial_spec")

    @property
    @pulumi.getter(name="trainTrialSpec")
    def train_trial_spec(self) -> 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse':
        """
        Spec for train trials. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        return pulumi.get(self, "train_trial_spec")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse(dict):
    """
    Represent spec for search trials.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxFailedTrialCount":
            suggest = "max_failed_trial_count"
        elif key == "maxParallelTrialCount":
            suggest = "max_parallel_trial_count"
        elif key == "maxTrialCount":
            suggest = "max_trial_count"
        elif key == "searchTrialJobSpec":
            suggest = "search_trial_job_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_failed_trial_count: int,
                 max_parallel_trial_count: int,
                 max_trial_count: int,
                 search_trial_job_spec: 'outputs.GoogleCloudAiplatformV1CustomJobSpecResponse'):
        """
        Represent spec for search trials.
        :param int max_failed_trial_count: The number of failed trials that need to be seen before failing the NasJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.
        :param int max_parallel_trial_count: The maximum number of trials to run in parallel.
        :param int max_trial_count: The maximum number of Neural Architecture Search (NAS) trials to run.
        :param 'GoogleCloudAiplatformV1CustomJobSpecResponse' search_trial_job_spec: The spec of a search trial job. The same spec applies to all search trials.
        """
        pulumi.set(__self__, "max_failed_trial_count", max_failed_trial_count)
        pulumi.set(__self__, "max_parallel_trial_count", max_parallel_trial_count)
        pulumi.set(__self__, "max_trial_count", max_trial_count)
        pulumi.set(__self__, "search_trial_job_spec", search_trial_job_spec)

    @property
    @pulumi.getter(name="maxFailedTrialCount")
    def max_failed_trial_count(self) -> int:
        """
        The number of failed trials that need to be seen before failing the NasJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.
        """
        return pulumi.get(self, "max_failed_trial_count")

    @property
    @pulumi.getter(name="maxParallelTrialCount")
    def max_parallel_trial_count(self) -> int:
        """
        The maximum number of trials to run in parallel.
        """
        return pulumi.get(self, "max_parallel_trial_count")

    @property
    @pulumi.getter(name="maxTrialCount")
    def max_trial_count(self) -> int:
        """
        The maximum number of Neural Architecture Search (NAS) trials to run.
        """
        return pulumi.get(self, "max_trial_count")

    @property
    @pulumi.getter(name="searchTrialJobSpec")
    def search_trial_job_spec(self) -> 'outputs.GoogleCloudAiplatformV1CustomJobSpecResponse':
        """
        The spec of a search trial job. The same spec applies to all search trials.
        """
        return pulumi.get(self, "search_trial_job_spec")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse(dict):
    """
    Represent spec for train trials.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxParallelTrialCount":
            suggest = "max_parallel_trial_count"
        elif key == "trainTrialJobSpec":
            suggest = "train_trial_job_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 frequency: int,
                 max_parallel_trial_count: int,
                 train_trial_job_spec: 'outputs.GoogleCloudAiplatformV1CustomJobSpecResponse'):
        """
        Represent spec for train trials.
        :param int frequency: Frequency of search trials to start train stage. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        :param int max_parallel_trial_count: The maximum number of trials to run in parallel.
        :param 'GoogleCloudAiplatformV1CustomJobSpecResponse' train_trial_job_spec: The spec of a train trial job. The same spec applies to all train trials.
        """
        pulumi.set(__self__, "frequency", frequency)
        pulumi.set(__self__, "max_parallel_trial_count", max_parallel_trial_count)
        pulumi.set(__self__, "train_trial_job_spec", train_trial_job_spec)

    @property
    @pulumi.getter
    def frequency(self) -> int:
        """
        Frequency of search trials to start train stage. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        return pulumi.get(self, "frequency")

    @property
    @pulumi.getter(name="maxParallelTrialCount")
    def max_parallel_trial_count(self) -> int:
        """
        The maximum number of trials to run in parallel.
        """
        return pulumi.get(self, "max_parallel_trial_count")

    @property
    @pulumi.getter(name="trainTrialJobSpec")
    def train_trial_job_spec(self) -> 'outputs.GoogleCloudAiplatformV1CustomJobSpecResponse':
        """
        The spec of a train trial job. The same spec applies to all train trials.
        """
        return pulumi.get(self, "train_trial_job_spec")


@pulumi.output_type
class GoogleCloudAiplatformV1NasJobSpecResponse(dict):
    """
    Represents the spec of a NasJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "multiTrialAlgorithmSpec":
            suggest = "multi_trial_algorithm_spec"
        elif key == "resumeNasJobId":
            suggest = "resume_nas_job_id"
        elif key == "searchSpaceSpec":
            suggest = "search_space_spec"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasJobSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasJobSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasJobSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 multi_trial_algorithm_spec: 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse',
                 resume_nas_job_id: str,
                 search_space_spec: str):
        """
        Represents the spec of a NasJob.
        :param 'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse' multi_trial_algorithm_spec: The spec of multi-trial algorithms.
        :param str resume_nas_job_id: The ID of the existing NasJob in the same Project and Location which will be used to resume search. search_space_spec and nas_algorithm_spec are obtained from previous NasJob hence should not provide them again for this NasJob.
        :param str search_space_spec: It defines the search space for Neural Architecture Search (NAS).
        """
        pulumi.set(__self__, "multi_trial_algorithm_spec", multi_trial_algorithm_spec)
        pulumi.set(__self__, "resume_nas_job_id", resume_nas_job_id)
        pulumi.set(__self__, "search_space_spec", search_space_spec)

    @property
    @pulumi.getter(name="multiTrialAlgorithmSpec")
    def multi_trial_algorithm_spec(self) -> 'outputs.GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecResponse':
        """
        The spec of multi-trial algorithms.
        """
        return pulumi.get(self, "multi_trial_algorithm_spec")

    @property
    @pulumi.getter(name="resumeNasJobId")
    def resume_nas_job_id(self) -> str:
        """
        The ID of the existing NasJob in the same Project and Location which will be used to resume search. search_space_spec and nas_algorithm_spec are obtained from previous NasJob hence should not provide them again for this NasJob.
        """
        return pulumi.get(self, "resume_nas_job_id")

    @property
    @pulumi.getter(name="searchSpaceSpec")
    def search_space_spec(self) -> str:
        """
        It defines the search space for Neural Architecture Search (NAS).
        """
        return pulumi.get(self, "search_space_spec")


@pulumi.output_type
class GoogleCloudAiplatformV1NasTrialResponse(dict):
    """
    Represents a uCAIP NasJob trial.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "endTime":
            suggest = "end_time"
        elif key == "finalMeasurement":
            suggest = "final_measurement"
        elif key == "startTime":
            suggest = "start_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NasTrialResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NasTrialResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NasTrialResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 end_time: str,
                 final_measurement: 'outputs.GoogleCloudAiplatformV1MeasurementResponse',
                 start_time: str,
                 state: str):
        """
        Represents a uCAIP NasJob trial.
        :param str end_time: Time when the NasTrial's status changed to `SUCCEEDED` or `INFEASIBLE`.
        :param 'GoogleCloudAiplatformV1MeasurementResponse' final_measurement: The final measurement containing the objective value.
        :param str start_time: Time when the NasTrial was started.
        :param str state: The detailed state of the NasTrial.
        """
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "final_measurement", final_measurement)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        Time when the NasTrial's status changed to `SUCCEEDED` or `INFEASIBLE`.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter(name="finalMeasurement")
    def final_measurement(self) -> 'outputs.GoogleCloudAiplatformV1MeasurementResponse':
        """
        The final measurement containing the objective value.
        """
        return pulumi.get(self, "final_measurement")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        Time when the NasTrial was started.
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The detailed state of the NasTrial.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class GoogleCloudAiplatformV1NetworkSpecResponse(dict):
    """
    Network spec.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableInternetAccess":
            suggest = "enable_internet_access"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NetworkSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NetworkSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NetworkSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_internet_access: bool,
                 network: str,
                 subnetwork: str):
        """
        Network spec.
        :param bool enable_internet_access: Whether to enable public internet access. Default false.
        :param str network: The full name of the Google Compute Engine [network](https://cloud.google.com//compute/docs/networks-and-firewalls#networks)
        :param str subnetwork: The name of the subnet that this instance is in. Format: `projects/{project_id_or_number}/regions/{region}/subnetworks/{subnetwork_id}`
        """
        pulumi.set(__self__, "enable_internet_access", enable_internet_access)
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "subnetwork", subnetwork)

    @property
    @pulumi.getter(name="enableInternetAccess")
    def enable_internet_access(self) -> bool:
        """
        Whether to enable public internet access. Default false.
        """
        return pulumi.get(self, "enable_internet_access")

    @property
    @pulumi.getter
    def network(self) -> str:
        """
        The full name of the Google Compute Engine [network](https://cloud.google.com//compute/docs/networks-and-firewalls#networks)
        """
        return pulumi.get(self, "network")

    @property
    @pulumi.getter
    def subnetwork(self) -> str:
        """
        The name of the subnet that this instance is in. Format: `projects/{project_id_or_number}/regions/{region}/subnetworks/{subnetwork_id}`
        """
        return pulumi.get(self, "subnetwork")


@pulumi.output_type
class GoogleCloudAiplatformV1NfsMountResponse(dict):
    """
    Represents a mount configuration for Network File System (NFS) to mount.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "mountPoint":
            suggest = "mount_point"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NfsMountResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NfsMountResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NfsMountResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 mount_point: str,
                 path: str,
                 server: str):
        """
        Represents a mount configuration for Network File System (NFS) to mount.
        :param str mount_point: Destination mount path. The NFS will be mounted for the user under /mnt/nfs/
        :param str path: Source path exported from NFS server. Has to start with '/', and combined with the ip address, it indicates the source mount path in the form of `server:path`
        :param str server: IP address of the NFS server.
        """
        pulumi.set(__self__, "mount_point", mount_point)
        pulumi.set(__self__, "path", path)
        pulumi.set(__self__, "server", server)

    @property
    @pulumi.getter(name="mountPoint")
    def mount_point(self) -> str:
        """
        Destination mount path. The NFS will be mounted for the user under /mnt/nfs/
        """
        return pulumi.get(self, "mount_point")

    @property
    @pulumi.getter
    def path(self) -> str:
        """
        Source path exported from NFS server. Has to start with '/', and combined with the ip address, it indicates the source mount path in the form of `server:path`
        """
        return pulumi.get(self, "path")

    @property
    @pulumi.getter
    def server(self) -> str:
        """
        IP address of the NFS server.
        """
        return pulumi.get(self, "server")


@pulumi.output_type
class GoogleCloudAiplatformV1NotebookEucConfigResponse(dict):
    """
    The euc configuration of NotebookRuntimeTemplate.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bypassActasCheck":
            suggest = "bypass_actas_check"
        elif key == "eucDisabled":
            suggest = "euc_disabled"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NotebookEucConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NotebookEucConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NotebookEucConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bypass_actas_check: bool,
                 euc_disabled: bool):
        """
        The euc configuration of NotebookRuntimeTemplate.
        :param bool bypass_actas_check: Whether ActAs check is bypassed for service account attached to the VM. If false, we need ActAs check for the default Compute Engine Service account. When a Runtime is created, a VM is allocated using Default Compute Engine Service Account. Any user requesting to use this Runtime requires Service Account User (ActAs) permission over this SA. If true, Runtime owner is using EUC and does not require the above permission as VM no longer use default Compute Engine SA, but a P4SA.
        :param bool euc_disabled: Input only. Whether EUC is disabled in this NotebookRuntimeTemplate. In proto3, the default value of a boolean is false. In this way, by default EUC will be enabled for NotebookRuntimeTemplate.
        """
        pulumi.set(__self__, "bypass_actas_check", bypass_actas_check)
        pulumi.set(__self__, "euc_disabled", euc_disabled)

    @property
    @pulumi.getter(name="bypassActasCheck")
    def bypass_actas_check(self) -> bool:
        """
        Whether ActAs check is bypassed for service account attached to the VM. If false, we need ActAs check for the default Compute Engine Service account. When a Runtime is created, a VM is allocated using Default Compute Engine Service Account. Any user requesting to use this Runtime requires Service Account User (ActAs) permission over this SA. If true, Runtime owner is using EUC and does not require the above permission as VM no longer use default Compute Engine SA, but a P4SA.
        """
        return pulumi.get(self, "bypass_actas_check")

    @property
    @pulumi.getter(name="eucDisabled")
    def euc_disabled(self) -> bool:
        """
        Input only. Whether EUC is disabled in this NotebookRuntimeTemplate. In proto3, the default value of a boolean is false. In this way, by default EUC will be enabled for NotebookRuntimeTemplate.
        """
        return pulumi.get(self, "euc_disabled")


@pulumi.output_type
class GoogleCloudAiplatformV1NotebookIdleShutdownConfigResponse(dict):
    """
    The idle shutdown configuration of NotebookRuntimeTemplate, which contains the idle_timeout as required field.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "idleShutdownDisabled":
            suggest = "idle_shutdown_disabled"
        elif key == "idleTimeout":
            suggest = "idle_timeout"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1NotebookIdleShutdownConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1NotebookIdleShutdownConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1NotebookIdleShutdownConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 idle_shutdown_disabled: bool,
                 idle_timeout: str):
        """
        The idle shutdown configuration of NotebookRuntimeTemplate, which contains the idle_timeout as required field.
        :param bool idle_shutdown_disabled: Whether Idle Shutdown is disabled in this NotebookRuntimeTemplate.
        :param str idle_timeout: Duration is accurate to the second. In Notebook, Idle Timeout is accurate to minute so the range of idle_timeout (second) is: 10 * 60 ~ 1440 * 60.
        """
        pulumi.set(__self__, "idle_shutdown_disabled", idle_shutdown_disabled)
        pulumi.set(__self__, "idle_timeout", idle_timeout)

    @property
    @pulumi.getter(name="idleShutdownDisabled")
    def idle_shutdown_disabled(self) -> bool:
        """
        Whether Idle Shutdown is disabled in this NotebookRuntimeTemplate.
        """
        return pulumi.get(self, "idle_shutdown_disabled")

    @property
    @pulumi.getter(name="idleTimeout")
    def idle_timeout(self) -> str:
        """
        Duration is accurate to the second. In Notebook, Idle Timeout is accurate to minute so the range of idle_timeout (second) is: 10 * 60 ~ 1440 * 60.
        """
        return pulumi.get(self, "idle_timeout")


@pulumi.output_type
class GoogleCloudAiplatformV1PersistentDiskSpecResponse(dict):
    """
    Represents the spec of persistent disk options.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "diskSizeGb":
            suggest = "disk_size_gb"
        elif key == "diskType":
            suggest = "disk_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PersistentDiskSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PersistentDiskSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PersistentDiskSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disk_size_gb: str,
                 disk_type: str):
        """
        Represents the spec of persistent disk options.
        :param str disk_size_gb: Size in GB of the disk (default is 100GB).
        :param str disk_type: Type of the disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) "pd-standard" (Persistent Disk Hard Disk Drive) "pd-balanced" (Balanced Persistent Disk) "pd-extreme" (Extreme Persistent Disk)
        """
        pulumi.set(__self__, "disk_size_gb", disk_size_gb)
        pulumi.set(__self__, "disk_type", disk_type)

    @property
    @pulumi.getter(name="diskSizeGb")
    def disk_size_gb(self) -> str:
        """
        Size in GB of the disk (default is 100GB).
        """
        return pulumi.get(self, "disk_size_gb")

    @property
    @pulumi.getter(name="diskType")
    def disk_type(self) -> str:
        """
        Type of the disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) "pd-standard" (Persistent Disk Hard Disk Drive) "pd-balanced" (Balanced Persistent Disk) "pd-extreme" (Extreme Persistent Disk)
        """
        return pulumi.get(self, "disk_type")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineJobDetailResponse(dict):
    """
    The runtime detail of PipelineJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "pipelineContext":
            suggest = "pipeline_context"
        elif key == "pipelineRunContext":
            suggest = "pipeline_run_context"
        elif key == "taskDetails":
            suggest = "task_details"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineJobDetailResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineJobDetailResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineJobDetailResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 pipeline_context: 'outputs.GoogleCloudAiplatformV1ContextResponse',
                 pipeline_run_context: 'outputs.GoogleCloudAiplatformV1ContextResponse',
                 task_details: Sequence['outputs.GoogleCloudAiplatformV1PipelineTaskDetailResponse']):
        """
        The runtime detail of PipelineJob.
        :param 'GoogleCloudAiplatformV1ContextResponse' pipeline_context: The context of the pipeline.
        :param 'GoogleCloudAiplatformV1ContextResponse' pipeline_run_context: The context of the current pipeline run.
        :param Sequence['GoogleCloudAiplatformV1PipelineTaskDetailResponse'] task_details: The runtime details of the tasks under the pipeline.
        """
        pulumi.set(__self__, "pipeline_context", pipeline_context)
        pulumi.set(__self__, "pipeline_run_context", pipeline_run_context)
        pulumi.set(__self__, "task_details", task_details)

    @property
    @pulumi.getter(name="pipelineContext")
    def pipeline_context(self) -> 'outputs.GoogleCloudAiplatformV1ContextResponse':
        """
        The context of the pipeline.
        """
        return pulumi.get(self, "pipeline_context")

    @property
    @pulumi.getter(name="pipelineRunContext")
    def pipeline_run_context(self) -> 'outputs.GoogleCloudAiplatformV1ContextResponse':
        """
        The context of the current pipeline run.
        """
        return pulumi.get(self, "pipeline_run_context")

    @property
    @pulumi.getter(name="taskDetails")
    def task_details(self) -> Sequence['outputs.GoogleCloudAiplatformV1PipelineTaskDetailResponse']:
        """
        The runtime details of the tasks under the pipeline.
        """
        return pulumi.get(self, "task_details")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineJobResponse(dict):
    """
    An instance of a machine learning PipelineJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "createTime":
            suggest = "create_time"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "encryptionSpec":
            suggest = "encryption_spec"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "jobDetail":
            suggest = "job_detail"
        elif key == "pipelineSpec":
            suggest = "pipeline_spec"
        elif key == "reservedIpRanges":
            suggest = "reserved_ip_ranges"
        elif key == "runtimeConfig":
            suggest = "runtime_config"
        elif key == "scheduleName":
            suggest = "schedule_name"
        elif key == "serviceAccount":
            suggest = "service_account"
        elif key == "startTime":
            suggest = "start_time"
        elif key == "templateMetadata":
            suggest = "template_metadata"
        elif key == "templateUri":
            suggest = "template_uri"
        elif key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineJobResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineJobResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineJobResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 create_time: str,
                 display_name: str,
                 encryption_spec: 'outputs.GoogleCloudAiplatformV1EncryptionSpecResponse',
                 end_time: str,
                 error: 'outputs.GoogleRpcStatusResponse',
                 job_detail: 'outputs.GoogleCloudAiplatformV1PipelineJobDetailResponse',
                 labels: Mapping[str, str],
                 name: str,
                 network: str,
                 pipeline_spec: Mapping[str, Any],
                 reserved_ip_ranges: Sequence[str],
                 runtime_config: 'outputs.GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse',
                 schedule_name: str,
                 service_account: str,
                 start_time: str,
                 state: str,
                 template_metadata: 'outputs.GoogleCloudAiplatformV1PipelineTemplateMetadataResponse',
                 template_uri: str,
                 update_time: str):
        """
        An instance of a machine learning PipelineJob.
        :param str create_time: Pipeline creation time.
        :param str display_name: The display name of the Pipeline. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param 'GoogleCloudAiplatformV1EncryptionSpecResponse' encryption_spec: Customer-managed encryption key spec for a pipelineJob. If set, this PipelineJob and all of its sub-resources will be secured by this key.
        :param str end_time: Pipeline end time.
        :param 'GoogleRpcStatusResponse' error: The error that occurred during pipeline execution. Only populated when the pipeline's state is FAILED or CANCELLED.
        :param 'GoogleCloudAiplatformV1PipelineJobDetailResponse' job_detail: The details of pipeline run. Not available in the list view.
        :param Mapping[str, str] labels: The labels with user-defined metadata to organize PipelineJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels. Note there is some reserved label key for Vertex AI Pipelines. - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
        :param str name: The resource name of the PipelineJob.
        :param str network: The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Pipeline Job's workload should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. Private services access must already be configured for the network. Pipeline job will apply the network configuration to the Google Cloud resources being launched, if applied, such as Vertex AI Training or Dataflow job. If left unspecified, the workload is not peered with any network.
        :param Mapping[str, Any] pipeline_spec: The spec of the pipeline.
        :param Sequence[str] reserved_ip_ranges: A list of names for the reserved ip ranges under the VPC network that can be used for this Pipeline Job's workload. If set, we will deploy the Pipeline Job's workload within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        :param 'GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse' runtime_config: Runtime config of the pipeline.
        :param str schedule_name: The schedule resource name. Only returned if the Pipeline is created by Schedule API.
        :param str service_account: The service account that the pipeline workload runs as. If not specified, the Compute Engine default service account in the project will be used. See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account Users starting the pipeline must have the `iam.serviceAccounts.actAs` permission on this service account.
        :param str start_time: Pipeline start time.
        :param str state: The detailed state of the job.
        :param 'GoogleCloudAiplatformV1PipelineTemplateMetadataResponse' template_metadata: Pipeline template metadata. Will fill up fields if PipelineJob.template_uri is from supported template registry.
        :param str template_uri: A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded. Currently, only uri from Vertex Template Registry & Gallery is supported. Reference to https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
        :param str update_time: Timestamp when this PipelineJob was most recently updated.
        """
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "encryption_spec", encryption_spec)
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "error", error)
        pulumi.set(__self__, "job_detail", job_detail)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "pipeline_spec", pipeline_spec)
        pulumi.set(__self__, "reserved_ip_ranges", reserved_ip_ranges)
        pulumi.set(__self__, "runtime_config", runtime_config)
        pulumi.set(__self__, "schedule_name", schedule_name)
        pulumi.set(__self__, "service_account", service_account)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "template_metadata", template_metadata)
        pulumi.set(__self__, "template_uri", template_uri)
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Pipeline creation time.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The display name of the Pipeline. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> 'outputs.GoogleCloudAiplatformV1EncryptionSpecResponse':
        """
        Customer-managed encryption key spec for a pipelineJob. If set, this PipelineJob and all of its sub-resources will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        Pipeline end time.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter
    def error(self) -> 'outputs.GoogleRpcStatusResponse':
        """
        The error that occurred during pipeline execution. Only populated when the pipeline's state is FAILED or CANCELLED.
        """
        return pulumi.get(self, "error")

    @property
    @pulumi.getter(name="jobDetail")
    def job_detail(self) -> 'outputs.GoogleCloudAiplatformV1PipelineJobDetailResponse':
        """
        The details of pipeline run. Not available in the list view.
        """
        return pulumi.get(self, "job_detail")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize PipelineJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels. Note there is some reserved label key for Vertex AI Pipelines. - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The resource name of the PipelineJob.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def network(self) -> str:
        """
        The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Pipeline Job's workload should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. Private services access must already be configured for the network. Pipeline job will apply the network configuration to the Google Cloud resources being launched, if applied, such as Vertex AI Training or Dataflow job. If left unspecified, the workload is not peered with any network.
        """
        return pulumi.get(self, "network")

    @property
    @pulumi.getter(name="pipelineSpec")
    def pipeline_spec(self) -> Mapping[str, Any]:
        """
        The spec of the pipeline.
        """
        return pulumi.get(self, "pipeline_spec")

    @property
    @pulumi.getter(name="reservedIpRanges")
    def reserved_ip_ranges(self) -> Sequence[str]:
        """
        A list of names for the reserved ip ranges under the VPC network that can be used for this Pipeline Job's workload. If set, we will deploy the Pipeline Job's workload within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        """
        return pulumi.get(self, "reserved_ip_ranges")

    @property
    @pulumi.getter(name="runtimeConfig")
    def runtime_config(self) -> 'outputs.GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse':
        """
        Runtime config of the pipeline.
        """
        return pulumi.get(self, "runtime_config")

    @property
    @pulumi.getter(name="scheduleName")
    def schedule_name(self) -> str:
        """
        The schedule resource name. Only returned if the Pipeline is created by Schedule API.
        """
        return pulumi.get(self, "schedule_name")

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> str:
        """
        The service account that the pipeline workload runs as. If not specified, the Compute Engine default service account in the project will be used. See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account Users starting the pipeline must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        return pulumi.get(self, "service_account")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        Pipeline start time.
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The detailed state of the job.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="templateMetadata")
    def template_metadata(self) -> 'outputs.GoogleCloudAiplatformV1PipelineTemplateMetadataResponse':
        """
        Pipeline template metadata. Will fill up fields if PipelineJob.template_uri is from supported template registry.
        """
        return pulumi.get(self, "template_metadata")

    @property
    @pulumi.getter(name="templateUri")
    def template_uri(self) -> str:
        """
        A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded. Currently, only uri from Vertex Template Registry & Gallery is supported. Reference to https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
        """
        return pulumi.get(self, "template_uri")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this PipelineJob was most recently updated.
        """
        return pulumi.get(self, "update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse(dict):
    """
    The type of an input artifact.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "artifactId":
            suggest = "artifact_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 artifact_id: str):
        """
        The type of an input artifact.
        :param str artifact_id: Artifact resource id from MLMD. Which is the last portion of an artifact resource name: `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`. The artifact must stay within the same project, location and default metadatastore as the pipeline.
        """
        pulumi.set(__self__, "artifact_id", artifact_id)

    @property
    @pulumi.getter(name="artifactId")
    def artifact_id(self) -> str:
        """
        Artifact resource id from MLMD. Which is the last portion of an artifact resource name: `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`. The artifact must stay within the same project, location and default metadatastore as the pipeline.
        """
        return pulumi.get(self, "artifact_id")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse(dict):
    """
    The runtime config of a PipelineJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "failurePolicy":
            suggest = "failure_policy"
        elif key == "gcsOutputDirectory":
            suggest = "gcs_output_directory"
        elif key == "inputArtifacts":
            suggest = "input_artifacts"
        elif key == "parameterValues":
            suggest = "parameter_values"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineJobRuntimeConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 failure_policy: str,
                 gcs_output_directory: str,
                 input_artifacts: Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse'],
                 parameter_values: Mapping[str, Any],
                 parameters: Mapping[str, 'outputs.GoogleCloudAiplatformV1ValueResponse']):
        """
        The runtime config of a PipelineJob.
        :param str failure_policy: Represents the failure policy of a pipeline. Currently, the default of a pipeline is that the pipeline will continue to run until no more tasks can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when a task has failed. Any scheduled tasks will continue to completion.
        :param str gcs_output_directory: A path in a Cloud Storage bucket, which will be treated as the root output directory of the pipeline. It is used by the system to generate the paths of output artifacts. The artifact paths are generated with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the specified output directory. The service account specified in this pipeline must have the `storage.objects.get` and `storage.objects.create` permissions for this bucket.
        :param Mapping[str, 'GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse'] input_artifacts: The runtime artifacts of the PipelineJob. The key will be the input artifact name and the value would be one of the InputArtifact.
        :param Mapping[str, Any] parameter_values: The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.
        :param Mapping[str, 'GoogleCloudAiplatformV1ValueResponse'] parameters: Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
        """
        pulumi.set(__self__, "failure_policy", failure_policy)
        pulumi.set(__self__, "gcs_output_directory", gcs_output_directory)
        pulumi.set(__self__, "input_artifacts", input_artifacts)
        pulumi.set(__self__, "parameter_values", parameter_values)
        pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter(name="failurePolicy")
    def failure_policy(self) -> str:
        """
        Represents the failure policy of a pipeline. Currently, the default of a pipeline is that the pipeline will continue to run until no more tasks can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when a task has failed. Any scheduled tasks will continue to completion.
        """
        return pulumi.get(self, "failure_policy")

    @property
    @pulumi.getter(name="gcsOutputDirectory")
    def gcs_output_directory(self) -> str:
        """
        A path in a Cloud Storage bucket, which will be treated as the root output directory of the pipeline. It is used by the system to generate the paths of output artifacts. The artifact paths are generated with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the specified output directory. The service account specified in this pipeline must have the `storage.objects.get` and `storage.objects.create` permissions for this bucket.
        """
        return pulumi.get(self, "gcs_output_directory")

    @property
    @pulumi.getter(name="inputArtifacts")
    def input_artifacts(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactResponse']:
        """
        The runtime artifacts of the PipelineJob. The key will be the input artifact name and the value would be one of the InputArtifact.
        """
        return pulumi.get(self, "input_artifacts")

    @property
    @pulumi.getter(name="parameterValues")
    def parameter_values(self) -> Mapping[str, Any]:
        """
        The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.
        """
        return pulumi.get(self, "parameter_values")

    @property
    @pulumi.getter
    @_utilities.deprecated("""Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.""")
    def parameters(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1ValueResponse']:
        """
        Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse(dict):
    """
    A list of artifact metadata.
    """
    def __init__(__self__, *,
                 artifacts: Sequence['outputs.GoogleCloudAiplatformV1ArtifactResponse']):
        """
        A list of artifact metadata.
        :param Sequence['GoogleCloudAiplatformV1ArtifactResponse'] artifacts: A list of artifact metadata.
        """
        pulumi.set(__self__, "artifacts", artifacts)

    @property
    @pulumi.getter
    def artifacts(self) -> Sequence['outputs.GoogleCloudAiplatformV1ArtifactResponse']:
        """
        A list of artifact metadata.
        """
        return pulumi.get(self, "artifacts")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse(dict):
    """
    A single record of the task status.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 error: 'outputs.GoogleRpcStatusResponse',
                 state: str,
                 update_time: str):
        """
        A single record of the task status.
        :param 'GoogleRpcStatusResponse' error: The error that occurred during the state. May be set when the state is any of the non-final state (PENDING/RUNNING/CANCELLING) or FAILED state. If the state is FAILED, the error here is final and not going to be retried. If the state is a non-final state, the error indicates a system-error being retried.
        :param str state: The state of the task.
        :param str update_time: Update time of this status.
        """
        pulumi.set(__self__, "error", error)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter
    def error(self) -> 'outputs.GoogleRpcStatusResponse':
        """
        The error that occurred during the state. May be set when the state is any of the non-final state (PENDING/RUNNING/CANCELLING) or FAILED state. If the state is FAILED, the error here is final and not going to be retried. If the state is a non-final state, the error indicates a system-error being retried.
        """
        return pulumi.get(self, "error")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The state of the task.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Update time of this status.
        """
        return pulumi.get(self, "update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskDetailResponse(dict):
    """
    The runtime detail of a task execution.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "createTime":
            suggest = "create_time"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "executorDetail":
            suggest = "executor_detail"
        elif key == "parentTaskId":
            suggest = "parent_task_id"
        elif key == "pipelineTaskStatus":
            suggest = "pipeline_task_status"
        elif key == "startTime":
            suggest = "start_time"
        elif key == "taskId":
            suggest = "task_id"
        elif key == "taskName":
            suggest = "task_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineTaskDetailResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineTaskDetailResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineTaskDetailResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 create_time: str,
                 end_time: str,
                 error: 'outputs.GoogleRpcStatusResponse',
                 execution: 'outputs.GoogleCloudAiplatformV1ExecutionResponse',
                 executor_detail: 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse',
                 inputs: Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse'],
                 outputs: Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse'],
                 parent_task_id: str,
                 pipeline_task_status: Sequence['outputs.GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse'],
                 start_time: str,
                 state: str,
                 task_id: str,
                 task_name: str):
        """
        The runtime detail of a task execution.
        :param str create_time: Task create time.
        :param str end_time: Task end time.
        :param 'GoogleRpcStatusResponse' error: The error that occurred during task execution. Only populated when the task's state is FAILED or CANCELLED.
        :param 'GoogleCloudAiplatformV1ExecutionResponse' execution: The execution metadata of the task.
        :param 'GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse' executor_detail: The detailed execution info.
        :param Mapping[str, 'GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse'] inputs: The runtime input artifacts of the task.
        :param Mapping[str, 'GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse'] outputs: The runtime output artifacts of the task.
        :param str parent_task_id: The id of the parent task if the task is within a component scope. Empty if the task is at the root level.
        :param Sequence['GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse'] pipeline_task_status: A list of task status. This field keeps a record of task status evolving over time.
        :param str start_time: Task start time.
        :param str state: State of the task.
        :param str task_id: The system generated ID of the task.
        :param str task_name: The user specified name of the task that is defined in pipeline_spec.
        """
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "error", error)
        pulumi.set(__self__, "execution", execution)
        pulumi.set(__self__, "executor_detail", executor_detail)
        pulumi.set(__self__, "inputs", inputs)
        pulumi.set(__self__, "outputs", outputs)
        pulumi.set(__self__, "parent_task_id", parent_task_id)
        pulumi.set(__self__, "pipeline_task_status", pipeline_task_status)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "task_id", task_id)
        pulumi.set(__self__, "task_name", task_name)

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Task create time.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        Task end time.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter
    def error(self) -> 'outputs.GoogleRpcStatusResponse':
        """
        The error that occurred during task execution. Only populated when the task's state is FAILED or CANCELLED.
        """
        return pulumi.get(self, "error")

    @property
    @pulumi.getter
    def execution(self) -> 'outputs.GoogleCloudAiplatformV1ExecutionResponse':
        """
        The execution metadata of the task.
        """
        return pulumi.get(self, "execution")

    @property
    @pulumi.getter(name="executorDetail")
    def executor_detail(self) -> 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse':
        """
        The detailed execution info.
        """
        return pulumi.get(self, "executor_detail")

    @property
    @pulumi.getter
    def inputs(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse']:
        """
        The runtime input artifacts of the task.
        """
        return pulumi.get(self, "inputs")

    @property
    @pulumi.getter
    def outputs(self) -> Mapping[str, 'outputs.GoogleCloudAiplatformV1PipelineTaskDetailArtifactListResponse']:
        """
        The runtime output artifacts of the task.
        """
        return pulumi.get(self, "outputs")

    @property
    @pulumi.getter(name="parentTaskId")
    def parent_task_id(self) -> str:
        """
        The id of the parent task if the task is within a component scope. Empty if the task is at the root level.
        """
        return pulumi.get(self, "parent_task_id")

    @property
    @pulumi.getter(name="pipelineTaskStatus")
    def pipeline_task_status(self) -> Sequence['outputs.GoogleCloudAiplatformV1PipelineTaskDetailPipelineTaskStatusResponse']:
        """
        A list of task status. This field keeps a record of task status evolving over time.
        """
        return pulumi.get(self, "pipeline_task_status")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        Task start time.
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        State of the task.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="taskId")
    def task_id(self) -> str:
        """
        The system generated ID of the task.
        """
        return pulumi.get(self, "task_id")

    @property
    @pulumi.getter(name="taskName")
    def task_name(self) -> str:
        """
        The user specified name of the task that is defined in pipeline_spec.
        """
        return pulumi.get(self, "task_name")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse(dict):
    """
    The detail of a container execution. It contains the job names of the lifecycle of a container execution.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "failedMainJobs":
            suggest = "failed_main_jobs"
        elif key == "failedPreCachingCheckJobs":
            suggest = "failed_pre_caching_check_jobs"
        elif key == "mainJob":
            suggest = "main_job"
        elif key == "preCachingCheckJob":
            suggest = "pre_caching_check_job"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 failed_main_jobs: Sequence[str],
                 failed_pre_caching_check_jobs: Sequence[str],
                 main_job: str,
                 pre_caching_check_job: str):
        """
        The detail of a container execution. It contains the job names of the lifecycle of a container execution.
        :param Sequence[str] failed_main_jobs: The names of the previously failed CustomJob for the main container executions. The list includes the all attempts in chronological order.
        :param Sequence[str] failed_pre_caching_check_jobs: The names of the previously failed CustomJob for the pre-caching-check container executions. This job will be available if the PipelineJob.pipeline_spec specifies the `pre_caching_check` hook in the lifecycle events. The list includes the all attempts in chronological order.
        :param str main_job: The name of the CustomJob for the main container execution.
        :param str pre_caching_check_job: The name of the CustomJob for the pre-caching-check container execution. This job will be available if the PipelineJob.pipeline_spec specifies the `pre_caching_check` hook in the lifecycle events.
        """
        pulumi.set(__self__, "failed_main_jobs", failed_main_jobs)
        pulumi.set(__self__, "failed_pre_caching_check_jobs", failed_pre_caching_check_jobs)
        pulumi.set(__self__, "main_job", main_job)
        pulumi.set(__self__, "pre_caching_check_job", pre_caching_check_job)

    @property
    @pulumi.getter(name="failedMainJobs")
    def failed_main_jobs(self) -> Sequence[str]:
        """
        The names of the previously failed CustomJob for the main container executions. The list includes the all attempts in chronological order.
        """
        return pulumi.get(self, "failed_main_jobs")

    @property
    @pulumi.getter(name="failedPreCachingCheckJobs")
    def failed_pre_caching_check_jobs(self) -> Sequence[str]:
        """
        The names of the previously failed CustomJob for the pre-caching-check container executions. This job will be available if the PipelineJob.pipeline_spec specifies the `pre_caching_check` hook in the lifecycle events. The list includes the all attempts in chronological order.
        """
        return pulumi.get(self, "failed_pre_caching_check_jobs")

    @property
    @pulumi.getter(name="mainJob")
    def main_job(self) -> str:
        """
        The name of the CustomJob for the main container execution.
        """
        return pulumi.get(self, "main_job")

    @property
    @pulumi.getter(name="preCachingCheckJob")
    def pre_caching_check_job(self) -> str:
        """
        The name of the CustomJob for the pre-caching-check container execution. This job will be available if the PipelineJob.pipeline_spec specifies the `pre_caching_check` hook in the lifecycle events.
        """
        return pulumi.get(self, "pre_caching_check_job")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse(dict):
    """
    The detailed info for a custom job executor.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "failedJobs":
            suggest = "failed_jobs"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 failed_jobs: Sequence[str],
                 job: str):
        """
        The detailed info for a custom job executor.
        :param Sequence[str] failed_jobs: The names of the previously failed CustomJob. The list includes the all attempts in chronological order.
        :param str job: The name of the CustomJob.
        """
        pulumi.set(__self__, "failed_jobs", failed_jobs)
        pulumi.set(__self__, "job", job)

    @property
    @pulumi.getter(name="failedJobs")
    def failed_jobs(self) -> Sequence[str]:
        """
        The names of the previously failed CustomJob. The list includes the all attempts in chronological order.
        """
        return pulumi.get(self, "failed_jobs")

    @property
    @pulumi.getter
    def job(self) -> str:
        """
        The name of the CustomJob.
        """
        return pulumi.get(self, "job")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse(dict):
    """
    The runtime detail of a pipeline executor.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerDetail":
            suggest = "container_detail"
        elif key == "customJobDetail":
            suggest = "custom_job_detail"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PipelineTaskExecutorDetailResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_detail: 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse',
                 custom_job_detail: 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse'):
        """
        The runtime detail of a pipeline executor.
        :param 'GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse' container_detail: The detailed info for a container executor.
        :param 'GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse' custom_job_detail: The detailed info for a custom job executor.
        """
        pulumi.set(__self__, "container_detail", container_detail)
        pulumi.set(__self__, "custom_job_detail", custom_job_detail)

    @property
    @pulumi.getter(name="containerDetail")
    def container_detail(self) -> 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailContainerDetailResponse':
        """
        The detailed info for a container executor.
        """
        return pulumi.get(self, "container_detail")

    @property
    @pulumi.getter(name="customJobDetail")
    def custom_job_detail(self) -> 'outputs.GoogleCloudAiplatformV1PipelineTaskExecutorDetailCustomJobDetailResponse':
        """
        The detailed info for a custom job executor.
        """
        return pulumi.get(self, "custom_job_detail")


@pulumi.output_type
class GoogleCloudAiplatformV1PipelineTemplateMetadataResponse(dict):
    """
    Pipeline template metadata if PipelineJob.template_uri is from supported template registry. Currently, the only supported registry is Artifact Registry.
    """
    def __init__(__self__, *,
                 version: str):
        """
        Pipeline template metadata if PipelineJob.template_uri is from supported template registry. Currently, the only supported registry is Artifact Registry.
        :param str version: The version_name in artifact registry. Will always be presented in output if the PipelineJob.template_uri is from supported template registry. Format is "sha256:abcdef123456...".
        """
        pulumi.set(__self__, "version", version)

    @property
    @pulumi.getter
    def version(self) -> str:
        """
        The version_name in artifact registry. Will always be presented in output if the PipelineJob.template_uri is from supported template registry. Format is "sha256:abcdef123456...".
        """
        return pulumi.get(self, "version")


@pulumi.output_type
class GoogleCloudAiplatformV1PortResponse(dict):
    """
    Represents a network port in a container.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerPort":
            suggest = "container_port"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PortResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PortResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PortResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_port: int):
        """
        Represents a network port in a container.
        :param int container_port: The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.
        """
        pulumi.set(__self__, "container_port", container_port)

    @property
    @pulumi.getter(name="containerPort")
    def container_port(self) -> int:
        """
        The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.
        """
        return pulumi.get(self, "container_port")


@pulumi.output_type
class GoogleCloudAiplatformV1PredefinedSplitResponse(dict):
    """
    Assigns input data to training, validation, and test sets based on the value of a provided key. Supported only for tabular Datasets.
    """
    def __init__(__self__, *,
                 key: str):
        """
        Assigns input data to training, validation, and test sets based on the value of a provided key. Supported only for tabular Datasets.
        :param str key: The key is a name of one of the Dataset's data columns. The value of the key (either the label's value or value in the column) must be one of {`training`, `validation`, `test`}, and it defines to which set the given piece of data is assigned. If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        pulumi.set(__self__, "key", key)

    @property
    @pulumi.getter
    def key(self) -> str:
        """
        The key is a name of one of the Dataset's data columns. The value of the key (either the label's value or value in the column) must be one of {`training`, `validation`, `test`}, and it defines to which set the given piece of data is assigned. If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        return pulumi.get(self, "key")


@pulumi.output_type
class GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigResponse(dict):
    """
    Configuration for logging request-response to a BigQuery table.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryDestination":
            suggest = "bigquery_destination"
        elif key == "samplingRate":
            suggest = "sampling_rate"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_destination: 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse',
                 enabled: bool,
                 sampling_rate: float):
        """
        Configuration for logging request-response to a BigQuery table.
        :param 'GoogleCloudAiplatformV1BigQueryDestinationResponse' bigquery_destination: BigQuery table for logging. If only given a project, a new dataset will be created with name `logging__` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
        :param bool enabled: If logging is enabled or not.
        :param float sampling_rate: Percentage of requests to be logged, expressed as a fraction in range(0,1].
        """
        pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        pulumi.set(__self__, "enabled", enabled)
        pulumi.set(__self__, "sampling_rate", sampling_rate)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> 'outputs.GoogleCloudAiplatformV1BigQueryDestinationResponse':
        """
        BigQuery table for logging. If only given a project, a new dataset will be created with name `logging__` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
        """
        return pulumi.get(self, "bigquery_destination")

    @property
    @pulumi.getter
    def enabled(self) -> bool:
        """
        If logging is enabled or not.
        """
        return pulumi.get(self, "enabled")

    @property
    @pulumi.getter(name="samplingRate")
    def sampling_rate(self) -> float:
        """
        Percentage of requests to be logged, expressed as a fraction in range(0,1].
        """
        return pulumi.get(self, "sampling_rate")


@pulumi.output_type
class GoogleCloudAiplatformV1PredictSchemataResponse(dict):
    """
    Contains the schemata used in Model's predictions and explanations via PredictionService.Predict, PredictionService.Explain and BatchPredictionJob.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "instanceSchemaUri":
            suggest = "instance_schema_uri"
        elif key == "parametersSchemaUri":
            suggest = "parameters_schema_uri"
        elif key == "predictionSchemaUri":
            suggest = "prediction_schema_uri"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PredictSchemataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PredictSchemataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PredictSchemataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 instance_schema_uri: str,
                 parameters_schema_uri: str,
                 prediction_schema_uri: str):
        """
        Contains the schemata used in Model's predictions and explanations via PredictionService.Predict, PredictionService.Explain and BatchPredictionJob.
        :param str instance_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param str parameters_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no parameters are supported, then it is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param str prediction_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        pulumi.set(__self__, "instance_schema_uri", instance_schema_uri)
        pulumi.set(__self__, "parameters_schema_uri", parameters_schema_uri)
        pulumi.set(__self__, "prediction_schema_uri", prediction_schema_uri)

    @property
    @pulumi.getter(name="instanceSchemaUri")
    def instance_schema_uri(self) -> str:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "instance_schema_uri")

    @property
    @pulumi.getter(name="parametersSchemaUri")
    def parameters_schema_uri(self) -> str:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no parameters are supported, then it is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "parameters_schema_uri")

    @property
    @pulumi.getter(name="predictionSchemaUri")
    def prediction_schema_uri(self) -> str:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "prediction_schema_uri")


@pulumi.output_type
class GoogleCloudAiplatformV1PresetsResponse(dict):
    """
    Preset configuration for example-based explanations
    """
    def __init__(__self__, *,
                 modality: str,
                 query: str):
        """
        Preset configuration for example-based explanations
        :param str modality: The modality of the uploaded model, which automatically configures the distance measurement and feature normalization for the underlying example index and queries. If your model does not precisely fit one of these types, it is okay to choose the closest type.
        :param str query: Preset option controlling parameters for speed-precision trade-off when querying for examples. If omitted, defaults to `PRECISE`.
        """
        pulumi.set(__self__, "modality", modality)
        pulumi.set(__self__, "query", query)

    @property
    @pulumi.getter
    def modality(self) -> str:
        """
        The modality of the uploaded model, which automatically configures the distance measurement and feature normalization for the underlying example index and queries. If your model does not precisely fit one of these types, it is okay to choose the closest type.
        """
        return pulumi.get(self, "modality")

    @property
    @pulumi.getter
    def query(self) -> str:
        """
        Preset option controlling parameters for speed-precision trade-off when querying for examples. If omitted, defaults to `PRECISE`.
        """
        return pulumi.get(self, "query")


@pulumi.output_type
class GoogleCloudAiplatformV1PrivateEndpointsResponse(dict):
    """
    PrivateEndpoints proto is used to provide paths for users to send requests privately. To send request via private service access, use predict_http_uri, explain_http_uri or health_http_uri. To send request via private service connect, use service_attachment.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "explainHttpUri":
            suggest = "explain_http_uri"
        elif key == "healthHttpUri":
            suggest = "health_http_uri"
        elif key == "predictHttpUri":
            suggest = "predict_http_uri"
        elif key == "serviceAttachment":
            suggest = "service_attachment"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PrivateEndpointsResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PrivateEndpointsResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PrivateEndpointsResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 explain_http_uri: str,
                 health_http_uri: str,
                 predict_http_uri: str,
                 service_attachment: str):
        """
        PrivateEndpoints proto is used to provide paths for users to send requests privately. To send request via private service access, use predict_http_uri, explain_http_uri or health_http_uri. To send request via private service connect, use service_attachment.
        :param str explain_http_uri: Http(s) path to send explain requests.
        :param str health_http_uri: Http(s) path to send health check requests.
        :param str predict_http_uri: Http(s) path to send prediction requests.
        :param str service_attachment: The name of the service attachment resource. Populated if private service connect is enabled.
        """
        pulumi.set(__self__, "explain_http_uri", explain_http_uri)
        pulumi.set(__self__, "health_http_uri", health_http_uri)
        pulumi.set(__self__, "predict_http_uri", predict_http_uri)
        pulumi.set(__self__, "service_attachment", service_attachment)

    @property
    @pulumi.getter(name="explainHttpUri")
    def explain_http_uri(self) -> str:
        """
        Http(s) path to send explain requests.
        """
        return pulumi.get(self, "explain_http_uri")

    @property
    @pulumi.getter(name="healthHttpUri")
    def health_http_uri(self) -> str:
        """
        Http(s) path to send health check requests.
        """
        return pulumi.get(self, "health_http_uri")

    @property
    @pulumi.getter(name="predictHttpUri")
    def predict_http_uri(self) -> str:
        """
        Http(s) path to send prediction requests.
        """
        return pulumi.get(self, "predict_http_uri")

    @property
    @pulumi.getter(name="serviceAttachment")
    def service_attachment(self) -> str:
        """
        The name of the service attachment resource. Populated if private service connect is enabled.
        """
        return pulumi.get(self, "service_attachment")


@pulumi.output_type
class GoogleCloudAiplatformV1PrivateServiceConnectConfigResponse(dict):
    """
    Represents configuration for private service connect.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enablePrivateServiceConnect":
            suggest = "enable_private_service_connect"
        elif key == "projectAllowlist":
            suggest = "project_allowlist"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PrivateServiceConnectConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PrivateServiceConnectConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PrivateServiceConnectConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 enable_private_service_connect: bool,
                 project_allowlist: Sequence[str]):
        """
        Represents configuration for private service connect.
        :param bool enable_private_service_connect: If true, expose the IndexEndpoint via private service connect.
        :param Sequence[str] project_allowlist: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        pulumi.set(__self__, "project_allowlist", project_allowlist)

    @property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> bool:
        """
        If true, expose the IndexEndpoint via private service connect.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @property
    @pulumi.getter(name="projectAllowlist")
    def project_allowlist(self) -> Sequence[str]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlist")


@pulumi.output_type
class GoogleCloudAiplatformV1ProbeExecActionResponse(dict):
    """
    ExecAction specifies a command to execute.
    """
    def __init__(__self__, *,
                 command: Sequence[str]):
        """
        ExecAction specifies a command to execute.
        :param Sequence[str] command: Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
        """
        pulumi.set(__self__, "command", command)

    @property
    @pulumi.getter
    def command(self) -> Sequence[str]:
        """
        Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
        """
        return pulumi.get(self, "command")


@pulumi.output_type
class GoogleCloudAiplatformV1ProbeResponse(dict):
    """
    Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exec":
            suggest = "exec_"
        elif key == "periodSeconds":
            suggest = "period_seconds"
        elif key == "timeoutSeconds":
            suggest = "timeout_seconds"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ProbeResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ProbeResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ProbeResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exec_: 'outputs.GoogleCloudAiplatformV1ProbeExecActionResponse',
                 period_seconds: int,
                 timeout_seconds: int):
        """
        Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.
        :param 'GoogleCloudAiplatformV1ProbeExecActionResponse' exec_: Exec specifies the action to take.
        :param int period_seconds: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeout_seconds. Maps to Kubernetes probe argument 'periodSeconds'.
        :param int timeout_seconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to period_seconds. Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        pulumi.set(__self__, "exec_", exec_)
        pulumi.set(__self__, "period_seconds", period_seconds)
        pulumi.set(__self__, "timeout_seconds", timeout_seconds)

    @property
    @pulumi.getter(name="exec")
    def exec_(self) -> 'outputs.GoogleCloudAiplatformV1ProbeExecActionResponse':
        """
        Exec specifies the action to take.
        """
        return pulumi.get(self, "exec_")

    @property
    @pulumi.getter(name="periodSeconds")
    def period_seconds(self) -> int:
        """
        How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeout_seconds. Maps to Kubernetes probe argument 'periodSeconds'.
        """
        return pulumi.get(self, "period_seconds")

    @property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> int:
        """
        Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to period_seconds. Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        return pulumi.get(self, "timeout_seconds")


@pulumi.output_type
class GoogleCloudAiplatformV1PythonPackageSpecResponse(dict):
    """
    The spec of a Python packaged code.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "executorImageUri":
            suggest = "executor_image_uri"
        elif key == "packageUris":
            suggest = "package_uris"
        elif key == "pythonModule":
            suggest = "python_module"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1PythonPackageSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1PythonPackageSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1PythonPackageSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 args: Sequence[str],
                 env: Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse'],
                 executor_image_uri: str,
                 package_uris: Sequence[str],
                 python_module: str):
        """
        The spec of a Python packaged code.
        :param Sequence[str] args: Command line arguments to be passed to the Python task.
        :param Sequence['GoogleCloudAiplatformV1EnvVarResponse'] env: Environment variables to be passed to the python module. Maximum limit is 100.
        :param str executor_image_uri: The URI of a container image in Artifact Registry that will run the provided Python package. Vertex AI provides a wide range of executor images with pre-installed packages to meet users' various use cases. See the list of [pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers). You must use an image from this list.
        :param Sequence[str] package_uris: The Google Cloud Storage location of the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.
        :param str python_module: The Python module name to run after installing the packages.
        """
        pulumi.set(__self__, "args", args)
        pulumi.set(__self__, "env", env)
        pulumi.set(__self__, "executor_image_uri", executor_image_uri)
        pulumi.set(__self__, "package_uris", package_uris)
        pulumi.set(__self__, "python_module", python_module)

    @property
    @pulumi.getter
    def args(self) -> Sequence[str]:
        """
        Command line arguments to be passed to the Python task.
        """
        return pulumi.get(self, "args")

    @property
    @pulumi.getter
    def env(self) -> Sequence['outputs.GoogleCloudAiplatformV1EnvVarResponse']:
        """
        Environment variables to be passed to the python module. Maximum limit is 100.
        """
        return pulumi.get(self, "env")

    @property
    @pulumi.getter(name="executorImageUri")
    def executor_image_uri(self) -> str:
        """
        The URI of a container image in Artifact Registry that will run the provided Python package. Vertex AI provides a wide range of executor images with pre-installed packages to meet users' various use cases. See the list of [pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers). You must use an image from this list.
        """
        return pulumi.get(self, "executor_image_uri")

    @property
    @pulumi.getter(name="packageUris")
    def package_uris(self) -> Sequence[str]:
        """
        The Google Cloud Storage location of the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.
        """
        return pulumi.get(self, "package_uris")

    @property
    @pulumi.getter(name="pythonModule")
    def python_module(self) -> str:
        """
        The Python module name to run after installing the packages.
        """
        return pulumi.get(self, "python_module")


@pulumi.output_type
class GoogleCloudAiplatformV1ResourcesConsumedResponse(dict):
    """
    Statistics information about resource consumption.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "replicaHours":
            suggest = "replica_hours"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ResourcesConsumedResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ResourcesConsumedResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ResourcesConsumedResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 replica_hours: float):
        """
        Statistics information about resource consumption.
        :param float replica_hours: The number of replica hours used. Note that many replicas may run in parallel, and additionally any given work may be queued for some time. Therefore this value is not strictly related to wall time.
        """
        pulumi.set(__self__, "replica_hours", replica_hours)

    @property
    @pulumi.getter(name="replicaHours")
    def replica_hours(self) -> float:
        """
        The number of replica hours used. Note that many replicas may run in parallel, and additionally any given work may be queued for some time. Therefore this value is not strictly related to wall time.
        """
        return pulumi.get(self, "replica_hours")


@pulumi.output_type
class GoogleCloudAiplatformV1SampleConfigResponse(dict):
    """
    Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "followingBatchSamplePercentage":
            suggest = "following_batch_sample_percentage"
        elif key == "initialBatchSamplePercentage":
            suggest = "initial_batch_sample_percentage"
        elif key == "sampleStrategy":
            suggest = "sample_strategy"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SampleConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SampleConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SampleConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 following_batch_sample_percentage: int,
                 initial_batch_sample_percentage: int,
                 sample_strategy: str):
        """
        Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        :param int following_batch_sample_percentage: The percentage of data needed to be labeled in each following batch (except the first batch).
        :param int initial_batch_sample_percentage: The percentage of data needed to be labeled in the first batch.
        :param str sample_strategy: Field to choose sampling strategy. Sampling strategy will decide which data should be selected for human labeling in every batch.
        """
        pulumi.set(__self__, "following_batch_sample_percentage", following_batch_sample_percentage)
        pulumi.set(__self__, "initial_batch_sample_percentage", initial_batch_sample_percentage)
        pulumi.set(__self__, "sample_strategy", sample_strategy)

    @property
    @pulumi.getter(name="followingBatchSamplePercentage")
    def following_batch_sample_percentage(self) -> int:
        """
        The percentage of data needed to be labeled in each following batch (except the first batch).
        """
        return pulumi.get(self, "following_batch_sample_percentage")

    @property
    @pulumi.getter(name="initialBatchSamplePercentage")
    def initial_batch_sample_percentage(self) -> int:
        """
        The percentage of data needed to be labeled in the first batch.
        """
        return pulumi.get(self, "initial_batch_sample_percentage")

    @property
    @pulumi.getter(name="sampleStrategy")
    def sample_strategy(self) -> str:
        """
        Field to choose sampling strategy. Sampling strategy will decide which data should be selected for human labeling in every batch.
        """
        return pulumi.get(self, "sample_strategy")


@pulumi.output_type
class GoogleCloudAiplatformV1SampledShapleyAttributionResponse(dict):
    """
    An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "pathCount":
            suggest = "path_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SampledShapleyAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SampledShapleyAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SampledShapleyAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 path_count: int):
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
        :param int path_count: The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 50], inclusively.
        """
        pulumi.set(__self__, "path_count", path_count)

    @property
    @pulumi.getter(name="pathCount")
    def path_count(self) -> int:
        """
        The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 50], inclusively.
        """
        return pulumi.get(self, "path_count")


@pulumi.output_type
class GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse(dict):
    """
    Requests are randomly selected.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "sampleRate":
            suggest = "sample_rate"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 sample_rate: float):
        """
        Requests are randomly selected.
        :param float sample_rate: Sample rate (0, 1]
        """
        pulumi.set(__self__, "sample_rate", sample_rate)

    @property
    @pulumi.getter(name="sampleRate")
    def sample_rate(self) -> float:
        """
        Sample rate (0, 1]
        """
        return pulumi.get(self, "sample_rate")


@pulumi.output_type
class GoogleCloudAiplatformV1SamplingStrategyResponse(dict):
    """
    Sampling Strategy for logging, can be for both training and prediction dataset.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "randomSampleConfig":
            suggest = "random_sample_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SamplingStrategyResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SamplingStrategyResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SamplingStrategyResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 random_sample_config: 'outputs.GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse'):
        """
        Sampling Strategy for logging, can be for both training and prediction dataset.
        :param 'GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse' random_sample_config: Random sample config. Will support more sampling strategies later.
        """
        pulumi.set(__self__, "random_sample_config", random_sample_config)

    @property
    @pulumi.getter(name="randomSampleConfig")
    def random_sample_config(self) -> 'outputs.GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigResponse':
        """
        Random sample config. Will support more sampling strategies later.
        """
        return pulumi.get(self, "random_sample_config")


@pulumi.output_type
class GoogleCloudAiplatformV1SavedQueryResponse(dict):
    """
    A SavedQuery is a view of the dataset. It references a subset of annotations by problem type and filters.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "annotationFilter":
            suggest = "annotation_filter"
        elif key == "annotationSpecCount":
            suggest = "annotation_spec_count"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "displayName":
            suggest = "display_name"
        elif key == "problemType":
            suggest = "problem_type"
        elif key == "supportAutomlTraining":
            suggest = "support_automl_training"
        elif key == "updateTime":
            suggest = "update_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SavedQueryResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SavedQueryResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SavedQueryResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 annotation_filter: str,
                 annotation_spec_count: int,
                 create_time: str,
                 display_name: str,
                 etag: str,
                 metadata: Any,
                 name: str,
                 problem_type: str,
                 support_automl_training: bool,
                 update_time: str):
        """
        A SavedQuery is a view of the dataset. It references a subset of annotations by problem type and filters.
        :param str annotation_filter: Filters on the Annotations in the dataset.
        :param int annotation_spec_count: Number of AnnotationSpecs in the context of the SavedQuery.
        :param str create_time: Timestamp when this SavedQuery was created.
        :param str display_name: The user-defined name of the SavedQuery. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param str etag: Used to perform a consistent read-modify-write update. If not set, a blind "overwrite" update happens.
        :param Any metadata: Some additional information about the SavedQuery.
        :param str name: Resource name of the SavedQuery.
        :param str problem_type: Problem type of the SavedQuery. Allowed values: * IMAGE_CLASSIFICATION_SINGLE_LABEL * IMAGE_CLASSIFICATION_MULTI_LABEL * IMAGE_BOUNDING_POLY * IMAGE_BOUNDING_BOX * TEXT_CLASSIFICATION_SINGLE_LABEL * TEXT_CLASSIFICATION_MULTI_LABEL * TEXT_EXTRACTION * TEXT_SENTIMENT * VIDEO_CLASSIFICATION * VIDEO_OBJECT_TRACKING
        :param bool support_automl_training: If the Annotations belonging to the SavedQuery can be used for AutoML training.
        :param str update_time: Timestamp when SavedQuery was last updated.
        """
        pulumi.set(__self__, "annotation_filter", annotation_filter)
        pulumi.set(__self__, "annotation_spec_count", annotation_spec_count)
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "metadata", metadata)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "problem_type", problem_type)
        pulumi.set(__self__, "support_automl_training", support_automl_training)
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter(name="annotationFilter")
    def annotation_filter(self) -> str:
        """
        Filters on the Annotations in the dataset.
        """
        return pulumi.get(self, "annotation_filter")

    @property
    @pulumi.getter(name="annotationSpecCount")
    def annotation_spec_count(self) -> int:
        """
        Number of AnnotationSpecs in the context of the SavedQuery.
        """
        return pulumi.get(self, "annotation_spec_count")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this SavedQuery was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The user-defined name of the SavedQuery. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        Used to perform a consistent read-modify-write update. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter
    def metadata(self) -> Any:
        """
        Some additional information about the SavedQuery.
        """
        return pulumi.get(self, "metadata")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Resource name of the SavedQuery.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="problemType")
    def problem_type(self) -> str:
        """
        Problem type of the SavedQuery. Allowed values: * IMAGE_CLASSIFICATION_SINGLE_LABEL * IMAGE_CLASSIFICATION_MULTI_LABEL * IMAGE_BOUNDING_POLY * IMAGE_BOUNDING_BOX * TEXT_CLASSIFICATION_SINGLE_LABEL * TEXT_CLASSIFICATION_MULTI_LABEL * TEXT_EXTRACTION * TEXT_SENTIMENT * VIDEO_CLASSIFICATION * VIDEO_OBJECT_TRACKING
        """
        return pulumi.get(self, "problem_type")

    @property
    @pulumi.getter(name="supportAutomlTraining")
    def support_automl_training(self) -> bool:
        """
        If the Annotations belonging to the SavedQuery can be used for AutoML training.
        """
        return pulumi.get(self, "support_automl_training")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when SavedQuery was last updated.
        """
        return pulumi.get(self, "update_time")


@pulumi.output_type
class GoogleCloudAiplatformV1ScheduleRunResponseResponse(dict):
    """
    Status of a scheduled run.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "runResponse":
            suggest = "run_response"
        elif key == "scheduledRunTime":
            suggest = "scheduled_run_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ScheduleRunResponseResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ScheduleRunResponseResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ScheduleRunResponseResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 run_response: str,
                 scheduled_run_time: str):
        """
        Status of a scheduled run.
        :param str run_response: The response of the scheduled run.
        :param str scheduled_run_time: The scheduled run time based on the user-specified schedule.
        """
        pulumi.set(__self__, "run_response", run_response)
        pulumi.set(__self__, "scheduled_run_time", scheduled_run_time)

    @property
    @pulumi.getter(name="runResponse")
    def run_response(self) -> str:
        """
        The response of the scheduled run.
        """
        return pulumi.get(self, "run_response")

    @property
    @pulumi.getter(name="scheduledRunTime")
    def scheduled_run_time(self) -> str:
        """
        The scheduled run time based on the user-specified schedule.
        """
        return pulumi.get(self, "scheduled_run_time")


@pulumi.output_type
class GoogleCloudAiplatformV1SchedulingResponse(dict):
    """
    All parameters related to queuing and scheduling of custom jobs.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "disableRetries":
            suggest = "disable_retries"
        elif key == "restartJobOnWorkerRestart":
            suggest = "restart_job_on_worker_restart"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SchedulingResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SchedulingResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SchedulingResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 disable_retries: bool,
                 restart_job_on_worker_restart: bool,
                 timeout: str):
        """
        All parameters related to queuing and scheduling of custom jobs.
        :param bool disable_retries: Optional. Indicates if the job should retry for internal errors after the job starts running. If true, overrides `Scheduling.restart_job_on_worker_restart` to false.
        :param bool restart_job_on_worker_restart: Restarts the entire CustomJob if a worker gets restarted. This feature can be used by distributed training jobs that are not resilient to workers leaving and joining a job.
        :param str timeout: The maximum job running time. The default is 7 days.
        """
        pulumi.set(__self__, "disable_retries", disable_retries)
        pulumi.set(__self__, "restart_job_on_worker_restart", restart_job_on_worker_restart)
        pulumi.set(__self__, "timeout", timeout)

    @property
    @pulumi.getter(name="disableRetries")
    def disable_retries(self) -> bool:
        """
        Optional. Indicates if the job should retry for internal errors after the job starts running. If true, overrides `Scheduling.restart_job_on_worker_restart` to false.
        """
        return pulumi.get(self, "disable_retries")

    @property
    @pulumi.getter(name="restartJobOnWorkerRestart")
    def restart_job_on_worker_restart(self) -> bool:
        """
        Restarts the entire CustomJob if a worker gets restarted. This feature can be used by distributed training jobs that are not resilient to workers leaving and joining a job.
        """
        return pulumi.get(self, "restart_job_on_worker_restart")

    @property
    @pulumi.getter
    def timeout(self) -> str:
        """
        The maximum job running time. The default is 7 days.
        """
        return pulumi.get(self, "timeout")


@pulumi.output_type
class GoogleCloudAiplatformV1SmoothGradConfigResponse(dict):
    """
    Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "featureNoiseSigma":
            suggest = "feature_noise_sigma"
        elif key == "noiseSigma":
            suggest = "noise_sigma"
        elif key == "noisySampleCount":
            suggest = "noisy_sample_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1SmoothGradConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1SmoothGradConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1SmoothGradConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 feature_noise_sigma: 'outputs.GoogleCloudAiplatformV1FeatureNoiseSigmaResponse',
                 noise_sigma: float,
                 noisy_sample_count: int):
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        :param 'GoogleCloudAiplatformV1FeatureNoiseSigmaResponse' feature_noise_sigma: This is similar to noise_sigma, but provides additional flexibility. A separate noise sigma can be provided for each feature, which is useful if their distributions are different. No noise is added to features that are not set. If this field is unset, noise_sigma will be used for all features.
        :param float noise_sigma: This is a single float value and will be used to add noise to all the features. Use this field when all features are normalized to have the same distribution: scale to range [0, 1], [-1, 1] or z-scoring, where features are normalized to have 0-mean and 1-variance. Learn more about [normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization). For best results the recommended value is about 10% - 20% of the standard deviation of the input feature. Refer to section 3.2 of the SmoothGrad paper: https://arxiv.org/pdf/1706.03825.pdf. Defaults to 0.1. If the distribution is different per feature, set feature_noise_sigma instead for each feature.
        :param int noisy_sample_count: The number of gradient samples to use for approximation. The higher this number, the more accurate the gradient is, but the runtime complexity increases by this factor as well. Valid range of its value is [1, 50]. Defaults to 3.
        """
        pulumi.set(__self__, "feature_noise_sigma", feature_noise_sigma)
        pulumi.set(__self__, "noise_sigma", noise_sigma)
        pulumi.set(__self__, "noisy_sample_count", noisy_sample_count)

    @property
    @pulumi.getter(name="featureNoiseSigma")
    def feature_noise_sigma(self) -> 'outputs.GoogleCloudAiplatformV1FeatureNoiseSigmaResponse':
        """
        This is similar to noise_sigma, but provides additional flexibility. A separate noise sigma can be provided for each feature, which is useful if their distributions are different. No noise is added to features that are not set. If this field is unset, noise_sigma will be used for all features.
        """
        return pulumi.get(self, "feature_noise_sigma")

    @property
    @pulumi.getter(name="noiseSigma")
    def noise_sigma(self) -> float:
        """
        This is a single float value and will be used to add noise to all the features. Use this field when all features are normalized to have the same distribution: scale to range [0, 1], [-1, 1] or z-scoring, where features are normalized to have 0-mean and 1-variance. Learn more about [normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization). For best results the recommended value is about 10% - 20% of the standard deviation of the input feature. Refer to section 3.2 of the SmoothGrad paper: https://arxiv.org/pdf/1706.03825.pdf. Defaults to 0.1. If the distribution is different per feature, set feature_noise_sigma instead for each feature.
        """
        return pulumi.get(self, "noise_sigma")

    @property
    @pulumi.getter(name="noisySampleCount")
    def noisy_sample_count(self) -> int:
        """
        The number of gradient samples to use for approximation. The higher this number, the more accurate the gradient is, but the runtime complexity increases by this factor as well. Valid range of its value is [1, 50]. Defaults to 3.
        """
        return pulumi.get(self, "noisy_sample_count")


@pulumi.output_type
class GoogleCloudAiplatformV1StratifiedSplitResponse(dict):
    """
    Assigns input data to the training, validation, and test sets so that the distribution of values found in the categorical column (as specified by the `key` field) is mirrored within each split. The fraction values determine the relative sizes of the splits. For example, if the specified column has three values, with 50% of the rows having value "A", 25% value "B", and 25% value "C", and the split fractions are specified as 80/10/10, then the training set will constitute 80% of the training data, with about 50% of the training set rows having the value "A" for the specified column, about 25% having the value "B", and about 25% having the value "C". Only the top 500 occurring values are used; any values not in the top 500 values are randomly assigned to a split. If less than three rows contain a specific value, those rows are randomly assigned. Supported only for tabular Datasets.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "testFraction":
            suggest = "test_fraction"
        elif key == "trainingFraction":
            suggest = "training_fraction"
        elif key == "validationFraction":
            suggest = "validation_fraction"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StratifiedSplitResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StratifiedSplitResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StratifiedSplitResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 key: str,
                 test_fraction: float,
                 training_fraction: float,
                 validation_fraction: float):
        """
        Assigns input data to the training, validation, and test sets so that the distribution of values found in the categorical column (as specified by the `key` field) is mirrored within each split. The fraction values determine the relative sizes of the splits. For example, if the specified column has three values, with 50% of the rows having value "A", 25% value "B", and 25% value "C", and the split fractions are specified as 80/10/10, then the training set will constitute 80% of the training data, with about 50% of the training set rows having the value "A" for the specified column, about 25% having the value "B", and about 25% having the value "C". Only the top 500 occurring values are used; any values not in the top 500 values are randomly assigned to a split. If less than three rows contain a specific value, those rows are randomly assigned. Supported only for tabular Datasets.
        :param str key: The key is a name of one of the Dataset's data columns. The key provided must be for a categorical column.
        :param float test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param float training_fraction: The fraction of the input data that is to be used to train the Model.
        :param float validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "test_fraction", test_fraction)
        pulumi.set(__self__, "training_fraction", training_fraction)
        pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter
    def key(self) -> str:
        """
        The key is a name of one of the Dataset's data columns. The key provided must be for a categorical column.
        """
        return pulumi.get(self, "key")

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse(dict):
    """
    Configuration for ConvexAutomatedStoppingSpec. When there are enough completed trials (configured by min_measurement_count), for pending trials with enough measurements and steps, the policy first computes an overestimate of the objective value at max_num_steps according to the slope of the incomplete objective value curve. No prediction can be made if the curve is completely flat. If the overestimation is worse than the best objective value of the completed trials, this pending trial will be early-stopped, but a last measurement will be added to the pending trial with max_num_steps and predicted objective value from the autoregression model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "learningRateParameterName":
            suggest = "learning_rate_parameter_name"
        elif key == "maxStepCount":
            suggest = "max_step_count"
        elif key == "minMeasurementCount":
            suggest = "min_measurement_count"
        elif key == "minStepCount":
            suggest = "min_step_count"
        elif key == "updateAllStoppedTrials":
            suggest = "update_all_stopped_trials"
        elif key == "useElapsedDuration":
            suggest = "use_elapsed_duration"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 learning_rate_parameter_name: str,
                 max_step_count: str,
                 min_measurement_count: str,
                 min_step_count: str,
                 update_all_stopped_trials: bool,
                 use_elapsed_duration: bool):
        """
        Configuration for ConvexAutomatedStoppingSpec. When there are enough completed trials (configured by min_measurement_count), for pending trials with enough measurements and steps, the policy first computes an overestimate of the objective value at max_num_steps according to the slope of the incomplete objective value curve. No prediction can be made if the curve is completely flat. If the overestimation is worse than the best objective value of the completed trials, this pending trial will be early-stopped, but a last measurement will be added to the pending trial with max_num_steps and predicted objective value from the autoregression model.
        :param str learning_rate_parameter_name: The hyper-parameter name used in the tuning job that stands for learning rate. Leave it blank if learning rate is not in a parameter in tuning. The learning_rate is used to estimate the objective value of the ongoing trial.
        :param str max_step_count: Steps used in predicting the final objective for early stopped trials. In general, it's set to be the same as the defined steps in training / tuning. If not defined, it will learn it from the completed trials. When use_steps is false, this field is set to the maximum elapsed seconds.
        :param str min_measurement_count: The minimal number of measurements in a Trial. Early-stopping checks will not trigger if less than min_measurement_count+1 completed trials or pending trials with less than min_measurement_count measurements. If not defined, the default value is 5.
        :param str min_step_count: Minimum number of steps for a trial to complete. Trials which do not have a measurement with step_count > min_step_count won't be considered for early stopping. It's ok to set it to 0, and a trial can be early stopped at any stage. By default, min_step_count is set to be one-tenth of the max_step_count. When use_elapsed_duration is true, this field is set to the minimum elapsed seconds.
        :param bool update_all_stopped_trials: ConvexAutomatedStoppingSpec by default only updates the trials that needs to be early stopped using a newly trained auto-regressive model. When this flag is set to True, all stopped trials from the beginning are potentially updated in terms of their `final_measurement`. Also, note that the training logic of autoregressive models is different in this case. Enabling this option has shown better results and this may be the default option in the future.
        :param bool use_elapsed_duration: This bool determines whether or not the rule is applied based on elapsed_secs or steps. If use_elapsed_duration==false, the early stopping decision is made according to the predicted objective values according to the target steps. If use_elapsed_duration==true, elapsed_secs is used instead of steps. Also, in this case, the parameters max_num_steps and min_num_steps are overloaded to contain max_elapsed_seconds and min_elapsed_seconds.
        """
        pulumi.set(__self__, "learning_rate_parameter_name", learning_rate_parameter_name)
        pulumi.set(__self__, "max_step_count", max_step_count)
        pulumi.set(__self__, "min_measurement_count", min_measurement_count)
        pulumi.set(__self__, "min_step_count", min_step_count)
        pulumi.set(__self__, "update_all_stopped_trials", update_all_stopped_trials)
        pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="learningRateParameterName")
    def learning_rate_parameter_name(self) -> str:
        """
        The hyper-parameter name used in the tuning job that stands for learning rate. Leave it blank if learning rate is not in a parameter in tuning. The learning_rate is used to estimate the objective value of the ongoing trial.
        """
        return pulumi.get(self, "learning_rate_parameter_name")

    @property
    @pulumi.getter(name="maxStepCount")
    def max_step_count(self) -> str:
        """
        Steps used in predicting the final objective for early stopped trials. In general, it's set to be the same as the defined steps in training / tuning. If not defined, it will learn it from the completed trials. When use_steps is false, this field is set to the maximum elapsed seconds.
        """
        return pulumi.get(self, "max_step_count")

    @property
    @pulumi.getter(name="minMeasurementCount")
    def min_measurement_count(self) -> str:
        """
        The minimal number of measurements in a Trial. Early-stopping checks will not trigger if less than min_measurement_count+1 completed trials or pending trials with less than min_measurement_count measurements. If not defined, the default value is 5.
        """
        return pulumi.get(self, "min_measurement_count")

    @property
    @pulumi.getter(name="minStepCount")
    def min_step_count(self) -> str:
        """
        Minimum number of steps for a trial to complete. Trials which do not have a measurement with step_count > min_step_count won't be considered for early stopping. It's ok to set it to 0, and a trial can be early stopped at any stage. By default, min_step_count is set to be one-tenth of the max_step_count. When use_elapsed_duration is true, this field is set to the minimum elapsed seconds.
        """
        return pulumi.get(self, "min_step_count")

    @property
    @pulumi.getter(name="updateAllStoppedTrials")
    def update_all_stopped_trials(self) -> bool:
        """
        ConvexAutomatedStoppingSpec by default only updates the trials that needs to be early stopped using a newly trained auto-regressive model. When this flag is set to True, all stopped trials from the beginning are potentially updated in terms of their `final_measurement`. Also, note that the training logic of autoregressive models is different in this case. Enabling this option has shown better results and this may be the default option in the future.
        """
        return pulumi.get(self, "update_all_stopped_trials")

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> bool:
        """
        This bool determines whether or not the rule is applied based on elapsed_secs or steps. If use_elapsed_duration==false, the early stopping decision is made according to the predicted objective values according to the target steps. If use_elapsed_duration==true, elapsed_secs is used instead of steps. Also, in this case, the parameters max_num_steps and min_num_steps are overloaded to contain max_elapsed_seconds and min_elapsed_seconds.
        """
        return pulumi.get(self, "use_elapsed_duration")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse(dict):
    """
    The decay curve automated stopping rule builds a Gaussian Process Regressor to predict the final objective value of a Trial based on the already completed Trials and the intermediate measurements of the current Trial. Early stopping is requested for the current Trial if there is very low probability to exceed the optimal value found so far.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useElapsedDuration":
            suggest = "use_elapsed_duration"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 use_elapsed_duration: bool):
        """
        The decay curve automated stopping rule builds a Gaussian Process Regressor to predict the final objective value of a Trial based on the already completed Trials and the intermediate measurements of the current Trial. Early stopping is requested for the current Trial if there is very low probability to exceed the optimal value found so far.
        :param bool use_elapsed_duration: True if Measurement.elapsed_duration is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.step_count will be used as the x-axis.
        """
        pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> bool:
        """
        True if Measurement.elapsed_duration is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.step_count will be used as the x-axis.
        """
        return pulumi.get(self, "use_elapsed_duration")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse(dict):
    """
    The median automated stopping rule stops a pending Trial if the Trial's best objective_value is strictly below the median 'performance' of all completed Trials reported up to the Trial's last measurement. Currently, 'performance' refers to the running average of the objective values reported by the Trial in each measurement.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useElapsedDuration":
            suggest = "use_elapsed_duration"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 use_elapsed_duration: bool):
        """
        The median automated stopping rule stops a pending Trial if the Trial's best objective_value is strictly below the median 'performance' of all completed Trials reported up to the Trial's last measurement. Currently, 'performance' refers to the running average of the objective values reported by the Trial in each measurement.
        :param bool use_elapsed_duration: True if median automated stopping rule applies on Measurement.elapsed_duration. It means that elapsed_duration field of latest measurement of current Trial is used to compute median objective value for each completed Trials.
        """
        pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> bool:
        """
        True if median automated stopping rule applies on Measurement.elapsed_duration. It means that elapsed_duration field of latest measurement of current Trial is used to compute median objective value for each completed Trials.
        """
        return pulumi.get(self, "use_elapsed_duration")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecMetricSpecResponse(dict):
    """
    Represents a metric to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "metricId":
            suggest = "metric_id"
        elif key == "safetyConfig":
            suggest = "safety_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecMetricSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecMetricSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecMetricSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 goal: str,
                 metric_id: str,
                 safety_config: 'outputs.GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse'):
        """
        Represents a metric to optimize.
        :param str goal: The optimization goal of the metric.
        :param str metric_id: The ID of the metric. Must not contain whitespaces and must be unique amongst all MetricSpecs.
        :param 'GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse' safety_config: Used for safe search. In the case, the metric will be a safety metric. You must provide a separate metric for objective metric.
        """
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "metric_id", metric_id)
        pulumi.set(__self__, "safety_config", safety_config)

    @property
    @pulumi.getter
    def goal(self) -> str:
        """
        The optimization goal of the metric.
        """
        return pulumi.get(self, "goal")

    @property
    @pulumi.getter(name="metricId")
    def metric_id(self) -> str:
        """
        The ID of the metric. Must not contain whitespaces and must be unique amongst all MetricSpecs.
        """
        return pulumi.get(self, "metric_id")

    @property
    @pulumi.getter(name="safetyConfig")
    def safety_config(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse':
        """
        Used for safe search. In the case, the metric will be a safety metric. You must provide a separate metric for objective metric.
        """
        return pulumi.get(self, "safety_config")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse(dict):
    """
    Used in safe optimization to specify threshold levels and risk tolerance.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "desiredMinSafeTrialsFraction":
            suggest = "desired_min_safe_trials_fraction"
        elif key == "safetyThreshold":
            suggest = "safety_threshold"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 desired_min_safe_trials_fraction: float,
                 safety_threshold: float):
        """
        Used in safe optimization to specify threshold levels and risk tolerance.
        :param float desired_min_safe_trials_fraction: Desired minimum fraction of safe trials (over total number of trials) that should be targeted by the algorithm at any time during the study (best effort). This should be between 0.0 and 1.0 and a value of 0.0 means that there is no minimum and an algorithm proceeds without targeting any specific fraction. A value of 1.0 means that the algorithm attempts to only Suggest safe Trials.
        :param float safety_threshold: Safety threshold (boundary value between safe and unsafe). NOTE that if you leave SafetyMetricConfig unset, a default value of 0 will be used.
        """
        pulumi.set(__self__, "desired_min_safe_trials_fraction", desired_min_safe_trials_fraction)
        pulumi.set(__self__, "safety_threshold", safety_threshold)

    @property
    @pulumi.getter(name="desiredMinSafeTrialsFraction")
    def desired_min_safe_trials_fraction(self) -> float:
        """
        Desired minimum fraction of safe trials (over total number of trials) that should be targeted by the algorithm at any time during the study (best effort). This should be between 0.0 and 1.0 and a value of 0.0 means that there is no minimum and an algorithm proceeds without targeting any specific fraction. A value of 1.0 means that the algorithm attempts to only Suggest safe Trials.
        """
        return pulumi.get(self, "desired_min_safe_trials_fraction")

    @property
    @pulumi.getter(name="safetyThreshold")
    def safety_threshold(self) -> float:
        """
        Safety threshold (boundary value between safe and unsafe). NOTE that if you leave SafetyMetricConfig unset, a default value of 0 will be used.
        """
        return pulumi.get(self, "safety_threshold")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse(dict):
    """
    Value specification for a parameter in `CATEGORICAL` type.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "defaultValue":
            suggest = "default_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 default_value: str,
                 values: Sequence[str]):
        """
        Value specification for a parameter in `CATEGORICAL` type.
        :param str default_value: A default value for a `CATEGORICAL` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param Sequence[str] values: The list of possible categories.
        """
        pulumi.set(__self__, "default_value", default_value)
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> str:
        """
        A default value for a `CATEGORICAL` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        The list of possible categories.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionResponse(dict):
    """
    Represents the spec to match categorical values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[str]):
        """
        Represents the spec to match categorical values from parent parameter.
        :param Sequence[str] values: Matches values of the parent parameter of 'CATEGORICAL' type. All values must exist in `categorical_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        Matches values of the parent parameter of 'CATEGORICAL' type. All values must exist in `categorical_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionResponse(dict):
    """
    Represents the spec to match discrete values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[float]):
        """
        Represents the spec to match discrete values from parent parameter.
        :param Sequence[float] values: Matches values of the parent parameter of 'DISCRETE' type. All values must exist in `discrete_value_spec` of parent parameter. The Epsilon of the value matching is 1e-10.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[float]:
        """
        Matches values of the parent parameter of 'DISCRETE' type. All values must exist in `discrete_value_spec` of parent parameter. The Epsilon of the value matching is 1e-10.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionResponse(dict):
    """
    Represents the spec to match integer values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[str]):
        """
        Represents the spec to match integer values from parent parameter.
        :param Sequence[str] values: Matches values of the parent parameter of 'INTEGER' type. All values must lie in `integer_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        Matches values of the parent parameter of 'INTEGER' type. All values must lie in `integer_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse(dict):
    """
    Represents a parameter spec with condition from its parent parameter.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "parameterSpec":
            suggest = "parameter_spec"
        elif key == "parentCategoricalValues":
            suggest = "parent_categorical_values"
        elif key == "parentDiscreteValues":
            suggest = "parent_discrete_values"
        elif key == "parentIntValues":
            suggest = "parent_int_values"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 parameter_spec: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecResponse',
                 parent_categorical_values: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionResponse',
                 parent_discrete_values: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionResponse',
                 parent_int_values: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionResponse'):
        """
        Represents a parameter spec with condition from its parent parameter.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecResponse' parameter_spec: The spec for a conditional parameter.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionResponse' parent_categorical_values: The spec for matching values from a parent parameter of `CATEGORICAL` type.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionResponse' parent_discrete_values: The spec for matching values from a parent parameter of `DISCRETE` type.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionResponse' parent_int_values: The spec for matching values from a parent parameter of `INTEGER` type.
        """
        pulumi.set(__self__, "parameter_spec", parameter_spec)
        pulumi.set(__self__, "parent_categorical_values", parent_categorical_values)
        pulumi.set(__self__, "parent_discrete_values", parent_discrete_values)
        pulumi.set(__self__, "parent_int_values", parent_int_values)

    @property
    @pulumi.getter(name="parameterSpec")
    def parameter_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecResponse':
        """
        The spec for a conditional parameter.
        """
        return pulumi.get(self, "parameter_spec")

    @property
    @pulumi.getter(name="parentCategoricalValues")
    def parent_categorical_values(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionResponse':
        """
        The spec for matching values from a parent parameter of `CATEGORICAL` type.
        """
        return pulumi.get(self, "parent_categorical_values")

    @property
    @pulumi.getter(name="parentDiscreteValues")
    def parent_discrete_values(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionResponse':
        """
        The spec for matching values from a parent parameter of `DISCRETE` type.
        """
        return pulumi.get(self, "parent_discrete_values")

    @property
    @pulumi.getter(name="parentIntValues")
    def parent_int_values(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionResponse':
        """
        The spec for matching values from a parent parameter of `INTEGER` type.
        """
        return pulumi.get(self, "parent_int_values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse(dict):
    """
    Value specification for a parameter in `DISCRETE` type.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "defaultValue":
            suggest = "default_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 default_value: float,
                 values: Sequence[float]):
        """
        Value specification for a parameter in `DISCRETE` type.
        :param float default_value: A default value for a `DISCRETE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. It automatically rounds to the nearest feasible discrete point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param Sequence[float] values: A list of possible values. The list should be in increasing order and at least 1e-10 apart. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        pulumi.set(__self__, "default_value", default_value)
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> float:
        """
        A default value for a `DISCRETE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. It automatically rounds to the nearest feasible discrete point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @property
    @pulumi.getter
    def values(self) -> Sequence[float]:
        """
        A list of possible values. The list should be in increasing order and at least 1e-10 apart. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse(dict):
    """
    Value specification for a parameter in `DOUBLE` type.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "defaultValue":
            suggest = "default_value"
        elif key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 default_value: float,
                 max_value: float,
                 min_value: float):
        """
        Value specification for a parameter in `DOUBLE` type.
        :param float default_value: A default value for a `DOUBLE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param float max_value: Inclusive maximum value of the parameter.
        :param float min_value: Inclusive minimum value of the parameter.
        """
        pulumi.set(__self__, "default_value", default_value)
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> float:
        """
        A default value for a `DOUBLE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> float:
        """
        Inclusive maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> float:
        """
        Inclusive minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse(dict):
    """
    Value specification for a parameter in `INTEGER` type.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "defaultValue":
            suggest = "default_value"
        elif key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 default_value: str,
                 max_value: str,
                 min_value: str):
        """
        Value specification for a parameter in `INTEGER` type.
        :param str default_value: A default value for an `INTEGER` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param str max_value: Inclusive maximum value of the parameter.
        :param str min_value: Inclusive minimum value of the parameter.
        """
        pulumi.set(__self__, "default_value", default_value)
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> str:
        """
        A default value for an `INTEGER` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> str:
        """
        Inclusive maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> str:
        """
        Inclusive minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecParameterSpecResponse(dict):
    """
    Represents a single parameter to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "categoricalValueSpec":
            suggest = "categorical_value_spec"
        elif key == "conditionalParameterSpecs":
            suggest = "conditional_parameter_specs"
        elif key == "discreteValueSpec":
            suggest = "discrete_value_spec"
        elif key == "doubleValueSpec":
            suggest = "double_value_spec"
        elif key == "integerValueSpec":
            suggest = "integer_value_spec"
        elif key == "parameterId":
            suggest = "parameter_id"
        elif key == "scaleType":
            suggest = "scale_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecParameterSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecParameterSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 categorical_value_spec: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse',
                 conditional_parameter_specs: Sequence['outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse'],
                 discrete_value_spec: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse',
                 double_value_spec: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse',
                 integer_value_spec: 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse',
                 parameter_id: str,
                 scale_type: str):
        """
        Represents a single parameter to optimize.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse' categorical_value_spec: The value spec for a 'CATEGORICAL' parameter.
        :param Sequence['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse'] conditional_parameter_specs: A conditional parameter node is active if the parameter's value matches the conditional node's parent_value_condition. If two items in conditional_parameter_specs have the same name, they must have disjoint parent_value_condition.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse' discrete_value_spec: The value spec for a 'DISCRETE' parameter.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse' double_value_spec: The value spec for a 'DOUBLE' parameter.
        :param 'GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse' integer_value_spec: The value spec for an 'INTEGER' parameter.
        :param str parameter_id: The ID of the parameter. Must not contain whitespaces and must be unique amongst all ParameterSpecs.
        :param str scale_type: How the parameter should be scaled. Leave unset for `CATEGORICAL` parameters.
        """
        pulumi.set(__self__, "categorical_value_spec", categorical_value_spec)
        pulumi.set(__self__, "conditional_parameter_specs", conditional_parameter_specs)
        pulumi.set(__self__, "discrete_value_spec", discrete_value_spec)
        pulumi.set(__self__, "double_value_spec", double_value_spec)
        pulumi.set(__self__, "integer_value_spec", integer_value_spec)
        pulumi.set(__self__, "parameter_id", parameter_id)
        pulumi.set(__self__, "scale_type", scale_type)

    @property
    @pulumi.getter(name="categoricalValueSpec")
    def categorical_value_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecResponse':
        """
        The value spec for a 'CATEGORICAL' parameter.
        """
        return pulumi.get(self, "categorical_value_spec")

    @property
    @pulumi.getter(name="conditionalParameterSpecs")
    def conditional_parameter_specs(self) -> Sequence['outputs.GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecResponse']:
        """
        A conditional parameter node is active if the parameter's value matches the conditional node's parent_value_condition. If two items in conditional_parameter_specs have the same name, they must have disjoint parent_value_condition.
        """
        return pulumi.get(self, "conditional_parameter_specs")

    @property
    @pulumi.getter(name="discreteValueSpec")
    def discrete_value_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecResponse':
        """
        The value spec for a 'DISCRETE' parameter.
        """
        return pulumi.get(self, "discrete_value_spec")

    @property
    @pulumi.getter(name="doubleValueSpec")
    def double_value_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecResponse':
        """
        The value spec for a 'DOUBLE' parameter.
        """
        return pulumi.get(self, "double_value_spec")

    @property
    @pulumi.getter(name="integerValueSpec")
    def integer_value_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecResponse':
        """
        The value spec for an 'INTEGER' parameter.
        """
        return pulumi.get(self, "integer_value_spec")

    @property
    @pulumi.getter(name="parameterId")
    def parameter_id(self) -> str:
        """
        The ID of the parameter. Must not contain whitespaces and must be unique amongst all ParameterSpecs.
        """
        return pulumi.get(self, "parameter_id")

    @property
    @pulumi.getter(name="scaleType")
    def scale_type(self) -> str:
        """
        How the parameter should be scaled. Leave unset for `CATEGORICAL` parameters.
        """
        return pulumi.get(self, "scale_type")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecResponse(dict):
    """
    Represents specification of a Study.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "convexAutomatedStoppingSpec":
            suggest = "convex_automated_stopping_spec"
        elif key == "decayCurveStoppingSpec":
            suggest = "decay_curve_stopping_spec"
        elif key == "measurementSelectionType":
            suggest = "measurement_selection_type"
        elif key == "medianAutomatedStoppingSpec":
            suggest = "median_automated_stopping_spec"
        elif key == "observationNoise":
            suggest = "observation_noise"
        elif key == "studyStoppingConfig":
            suggest = "study_stopping_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 algorithm: str,
                 convex_automated_stopping_spec: 'outputs.GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse',
                 decay_curve_stopping_spec: 'outputs.GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse',
                 measurement_selection_type: str,
                 median_automated_stopping_spec: 'outputs.GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse',
                 metrics: Sequence['outputs.GoogleCloudAiplatformV1StudySpecMetricSpecResponse'],
                 observation_noise: str,
                 parameters: Sequence['outputs.GoogleCloudAiplatformV1StudySpecParameterSpecResponse'],
                 study_stopping_config: 'outputs.GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse'):
        """
        Represents specification of a Study.
        :param str algorithm: The search algorithm specified for the Study.
        :param 'GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse' convex_automated_stopping_spec: The automated early stopping spec using convex stopping rule.
        :param 'GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse' decay_curve_stopping_spec: The automated early stopping spec using decay curve rule.
        :param str measurement_selection_type: Describe which measurement selection type will be used
        :param 'GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse' median_automated_stopping_spec: The automated early stopping spec using median rule.
        :param Sequence['GoogleCloudAiplatformV1StudySpecMetricSpecResponse'] metrics: Metric specs for the Study.
        :param str observation_noise: The observation noise level of the study. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param Sequence['GoogleCloudAiplatformV1StudySpecParameterSpecResponse'] parameters: The set of parameters to tune.
        :param 'GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse' study_stopping_config: Conditions for automated stopping of a Study. Enable automated stopping by configuring at least one condition.
        """
        pulumi.set(__self__, "algorithm", algorithm)
        pulumi.set(__self__, "convex_automated_stopping_spec", convex_automated_stopping_spec)
        pulumi.set(__self__, "decay_curve_stopping_spec", decay_curve_stopping_spec)
        pulumi.set(__self__, "measurement_selection_type", measurement_selection_type)
        pulumi.set(__self__, "median_automated_stopping_spec", median_automated_stopping_spec)
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "observation_noise", observation_noise)
        pulumi.set(__self__, "parameters", parameters)
        pulumi.set(__self__, "study_stopping_config", study_stopping_config)

    @property
    @pulumi.getter
    def algorithm(self) -> str:
        """
        The search algorithm specified for the Study.
        """
        return pulumi.get(self, "algorithm")

    @property
    @pulumi.getter(name="convexAutomatedStoppingSpec")
    def convex_automated_stopping_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecResponse':
        """
        The automated early stopping spec using convex stopping rule.
        """
        return pulumi.get(self, "convex_automated_stopping_spec")

    @property
    @pulumi.getter(name="decayCurveStoppingSpec")
    def decay_curve_stopping_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecResponse':
        """
        The automated early stopping spec using decay curve rule.
        """
        return pulumi.get(self, "decay_curve_stopping_spec")

    @property
    @pulumi.getter(name="measurementSelectionType")
    def measurement_selection_type(self) -> str:
        """
        Describe which measurement selection type will be used
        """
        return pulumi.get(self, "measurement_selection_type")

    @property
    @pulumi.getter(name="medianAutomatedStoppingSpec")
    def median_automated_stopping_spec(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecResponse':
        """
        The automated early stopping spec using median rule.
        """
        return pulumi.get(self, "median_automated_stopping_spec")

    @property
    @pulumi.getter
    def metrics(self) -> Sequence['outputs.GoogleCloudAiplatformV1StudySpecMetricSpecResponse']:
        """
        Metric specs for the Study.
        """
        return pulumi.get(self, "metrics")

    @property
    @pulumi.getter(name="observationNoise")
    def observation_noise(self) -> str:
        """
        The observation noise level of the study. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "observation_noise")

    @property
    @pulumi.getter
    def parameters(self) -> Sequence['outputs.GoogleCloudAiplatformV1StudySpecParameterSpecResponse']:
        """
        The set of parameters to tune.
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="studyStoppingConfig")
    def study_stopping_config(self) -> 'outputs.GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse':
        """
        Conditions for automated stopping of a Study. Enable automated stopping by configuring at least one condition.
        """
        return pulumi.get(self, "study_stopping_config")


@pulumi.output_type
class GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse(dict):
    """
    The configuration (stopping conditions) for automated stopping of a Study. Conditions include trial budgets, time budgets, and convergence detection.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxDurationNoProgress":
            suggest = "max_duration_no_progress"
        elif key == "maxNumTrials":
            suggest = "max_num_trials"
        elif key == "maxNumTrialsNoProgress":
            suggest = "max_num_trials_no_progress"
        elif key == "maximumRuntimeConstraint":
            suggest = "maximum_runtime_constraint"
        elif key == "minNumTrials":
            suggest = "min_num_trials"
        elif key == "minimumRuntimeConstraint":
            suggest = "minimum_runtime_constraint"
        elif key == "shouldStopAsap":
            suggest = "should_stop_asap"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudySpecStudyStoppingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_duration_no_progress: str,
                 max_num_trials: int,
                 max_num_trials_no_progress: int,
                 maximum_runtime_constraint: 'outputs.GoogleCloudAiplatformV1StudyTimeConstraintResponse',
                 min_num_trials: int,
                 minimum_runtime_constraint: 'outputs.GoogleCloudAiplatformV1StudyTimeConstraintResponse',
                 should_stop_asap: bool):
        """
        The configuration (stopping conditions) for automated stopping of a Study. Conditions include trial budgets, time budgets, and convergence detection.
        :param str max_duration_no_progress: If the objective value has not improved for this much time, stop the study. WARNING: Effective only for single-objective studies.
        :param int max_num_trials: If there are more than this many trials, stop the study.
        :param int max_num_trials_no_progress: If the objective value has not improved for this many consecutive trials, stop the study. WARNING: Effective only for single-objective studies.
        :param 'GoogleCloudAiplatformV1StudyTimeConstraintResponse' maximum_runtime_constraint: If the specified time or duration has passed, stop the study.
        :param int min_num_trials: If there are fewer than this many COMPLETED trials, do not stop the study.
        :param 'GoogleCloudAiplatformV1StudyTimeConstraintResponse' minimum_runtime_constraint: Each "stopping rule" in this proto specifies an "if" condition. Before Vizier would generate a new suggestion, it first checks each specified stopping rule, from top to bottom in this list. Note that the first few rules (e.g. minimum_runtime_constraint, min_num_trials) will prevent other stopping rules from being evaluated until they are met. For example, setting `min_num_trials=5` and `always_stop_after= 1 hour` means that the Study will ONLY stop after it has 5 COMPLETED trials, even if more than an hour has passed since its creation. It follows the first applicable rule (whose "if" condition is satisfied) to make a stopping decision. If none of the specified rules are applicable, then Vizier decides that the study should not stop. If Vizier decides that the study should stop, the study enters STOPPING state (or STOPPING_ASAP if should_stop_asap = true). IMPORTANT: The automatic study state transition happens precisely as described above; that is, deleting trials or updating StudyConfig NEVER automatically moves the study state back to ACTIVE. If you want to _resume_ a Study that was stopped, 1) change the stopping conditions if necessary, 2) activate the study, and then 3) ask for suggestions. If the specified time or duration has not passed, do not stop the study.
        :param bool should_stop_asap: If true, a Study enters STOPPING_ASAP whenever it would normally enters STOPPING state. The bottom line is: set to true if you want to interrupt on-going evaluations of Trials as soon as the study stopping condition is met. (Please see Study.State documentation for the source of truth).
        """
        pulumi.set(__self__, "max_duration_no_progress", max_duration_no_progress)
        pulumi.set(__self__, "max_num_trials", max_num_trials)
        pulumi.set(__self__, "max_num_trials_no_progress", max_num_trials_no_progress)
        pulumi.set(__self__, "maximum_runtime_constraint", maximum_runtime_constraint)
        pulumi.set(__self__, "min_num_trials", min_num_trials)
        pulumi.set(__self__, "minimum_runtime_constraint", minimum_runtime_constraint)
        pulumi.set(__self__, "should_stop_asap", should_stop_asap)

    @property
    @pulumi.getter(name="maxDurationNoProgress")
    def max_duration_no_progress(self) -> str:
        """
        If the objective value has not improved for this much time, stop the study. WARNING: Effective only for single-objective studies.
        """
        return pulumi.get(self, "max_duration_no_progress")

    @property
    @pulumi.getter(name="maxNumTrials")
    def max_num_trials(self) -> int:
        """
        If there are more than this many trials, stop the study.
        """
        return pulumi.get(self, "max_num_trials")

    @property
    @pulumi.getter(name="maxNumTrialsNoProgress")
    def max_num_trials_no_progress(self) -> int:
        """
        If the objective value has not improved for this many consecutive trials, stop the study. WARNING: Effective only for single-objective studies.
        """
        return pulumi.get(self, "max_num_trials_no_progress")

    @property
    @pulumi.getter(name="maximumRuntimeConstraint")
    def maximum_runtime_constraint(self) -> 'outputs.GoogleCloudAiplatformV1StudyTimeConstraintResponse':
        """
        If the specified time or duration has passed, stop the study.
        """
        return pulumi.get(self, "maximum_runtime_constraint")

    @property
    @pulumi.getter(name="minNumTrials")
    def min_num_trials(self) -> int:
        """
        If there are fewer than this many COMPLETED trials, do not stop the study.
        """
        return pulumi.get(self, "min_num_trials")

    @property
    @pulumi.getter(name="minimumRuntimeConstraint")
    def minimum_runtime_constraint(self) -> 'outputs.GoogleCloudAiplatformV1StudyTimeConstraintResponse':
        """
        Each "stopping rule" in this proto specifies an "if" condition. Before Vizier would generate a new suggestion, it first checks each specified stopping rule, from top to bottom in this list. Note that the first few rules (e.g. minimum_runtime_constraint, min_num_trials) will prevent other stopping rules from being evaluated until they are met. For example, setting `min_num_trials=5` and `always_stop_after= 1 hour` means that the Study will ONLY stop after it has 5 COMPLETED trials, even if more than an hour has passed since its creation. It follows the first applicable rule (whose "if" condition is satisfied) to make a stopping decision. If none of the specified rules are applicable, then Vizier decides that the study should not stop. If Vizier decides that the study should stop, the study enters STOPPING state (or STOPPING_ASAP if should_stop_asap = true). IMPORTANT: The automatic study state transition happens precisely as described above; that is, deleting trials or updating StudyConfig NEVER automatically moves the study state back to ACTIVE. If you want to _resume_ a Study that was stopped, 1) change the stopping conditions if necessary, 2) activate the study, and then 3) ask for suggestions. If the specified time or duration has not passed, do not stop the study.
        """
        return pulumi.get(self, "minimum_runtime_constraint")

    @property
    @pulumi.getter(name="shouldStopAsap")
    def should_stop_asap(self) -> bool:
        """
        If true, a Study enters STOPPING_ASAP whenever it would normally enters STOPPING state. The bottom line is: set to true if you want to interrupt on-going evaluations of Trials as soon as the study stopping condition is met. (Please see Study.State documentation for the source of truth).
        """
        return pulumi.get(self, "should_stop_asap")


@pulumi.output_type
class GoogleCloudAiplatformV1StudyTimeConstraintResponse(dict):
    """
    Time-based Constraint for Study
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "endTime":
            suggest = "end_time"
        elif key == "maxDuration":
            suggest = "max_duration"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1StudyTimeConstraintResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1StudyTimeConstraintResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1StudyTimeConstraintResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 end_time: str,
                 max_duration: str):
        """
        Time-based Constraint for Study
        :param str end_time: Compares the wallclock time to this time. Must use UTC timezone.
        :param str max_duration: Counts the wallclock time passed since the creation of this Study.
        """
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "max_duration", max_duration)

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        Compares the wallclock time to this time. Must use UTC timezone.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter(name="maxDuration")
    def max_duration(self) -> str:
        """
        Counts the wallclock time passed since the creation of this Study.
        """
        return pulumi.get(self, "max_duration")


@pulumi.output_type
class GoogleCloudAiplatformV1TensorboardTimeSeriesMetadataResponse(dict):
    """
    Describes metadata for a TensorboardTimeSeries.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxBlobSequenceLength":
            suggest = "max_blob_sequence_length"
        elif key == "maxStep":
            suggest = "max_step"
        elif key == "maxWallTime":
            suggest = "max_wall_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1TensorboardTimeSeriesMetadataResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1TensorboardTimeSeriesMetadataResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1TensorboardTimeSeriesMetadataResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_blob_sequence_length: str,
                 max_step: str,
                 max_wall_time: str):
        """
        Describes metadata for a TensorboardTimeSeries.
        :param str max_blob_sequence_length: The largest blob sequence length (number of blobs) of all data points in this time series, if its ValueType is BLOB_SEQUENCE.
        :param str max_step: Max step index of all data points within a TensorboardTimeSeries.
        :param str max_wall_time: Max wall clock timestamp of all data points within a TensorboardTimeSeries.
        """
        pulumi.set(__self__, "max_blob_sequence_length", max_blob_sequence_length)
        pulumi.set(__self__, "max_step", max_step)
        pulumi.set(__self__, "max_wall_time", max_wall_time)

    @property
    @pulumi.getter(name="maxBlobSequenceLength")
    def max_blob_sequence_length(self) -> str:
        """
        The largest blob sequence length (number of blobs) of all data points in this time series, if its ValueType is BLOB_SEQUENCE.
        """
        return pulumi.get(self, "max_blob_sequence_length")

    @property
    @pulumi.getter(name="maxStep")
    def max_step(self) -> str:
        """
        Max step index of all data points within a TensorboardTimeSeries.
        """
        return pulumi.get(self, "max_step")

    @property
    @pulumi.getter(name="maxWallTime")
    def max_wall_time(self) -> str:
        """
        Max wall clock timestamp of all data points within a TensorboardTimeSeries.
        """
        return pulumi.get(self, "max_wall_time")


@pulumi.output_type
class GoogleCloudAiplatformV1ThresholdConfigResponse(dict):
    """
    The config for feature monitoring threshold.
    """
    def __init__(__self__, *,
                 value: float):
        """
        The config for feature monitoring threshold.
        :param float value: Specify a threshold value that can trigger the alert. If this threshold config is for feature distribution distance: 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> float:
        """
        Specify a threshold value that can trigger the alert. If this threshold config is for feature distribution distance: 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudAiplatformV1TimestampSplitResponse(dict):
    """
    Assigns input data to training, validation, and test sets based on a provided timestamps. The youngest data pieces are assigned to training set, next to validation set, and the oldest to the test set. Supported only for tabular Datasets.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "testFraction":
            suggest = "test_fraction"
        elif key == "trainingFraction":
            suggest = "training_fraction"
        elif key == "validationFraction":
            suggest = "validation_fraction"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1TimestampSplitResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1TimestampSplitResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1TimestampSplitResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 key: str,
                 test_fraction: float,
                 training_fraction: float,
                 validation_fraction: float):
        """
        Assigns input data to training, validation, and test sets based on a provided timestamps. The youngest data pieces are assigned to training set, next to validation set, and the oldest to the test set. Supported only for tabular Datasets.
        :param str key: The key is a name of one of the Dataset's data columns. The values of the key (the values in the column) must be in RFC 3339 `date-time` format, where `time-offset` = `"Z"` (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        :param float test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param float training_fraction: The fraction of the input data that is to be used to train the Model.
        :param float validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        pulumi.set(__self__, "key", key)
        pulumi.set(__self__, "test_fraction", test_fraction)
        pulumi.set(__self__, "training_fraction", training_fraction)
        pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter
    def key(self) -> str:
        """
        The key is a name of one of the Dataset's data columns. The values of the key (the values in the column) must be in RFC 3339 `date-time` format, where `time-offset` = `"Z"` (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        return pulumi.get(self, "key")

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> float:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")


@pulumi.output_type
class GoogleCloudAiplatformV1TrainingConfigResponse(dict):
    """
    CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "timeoutTrainingMilliHours":
            suggest = "timeout_training_milli_hours"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1TrainingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1TrainingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1TrainingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 timeout_training_milli_hours: str):
        """
        CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        :param str timeout_training_milli_hours: The timeout hours for the CMLE training job, expressed in milli hours i.e. 1,000 value in this field means 1 hour.
        """
        pulumi.set(__self__, "timeout_training_milli_hours", timeout_training_milli_hours)

    @property
    @pulumi.getter(name="timeoutTrainingMilliHours")
    def timeout_training_milli_hours(self) -> str:
        """
        The timeout hours for the CMLE training job, expressed in milli hours i.e. 1,000 value in this field means 1 hour.
        """
        return pulumi.get(self, "timeout_training_milli_hours")


@pulumi.output_type
class GoogleCloudAiplatformV1TrialParameterResponse(dict):
    """
    A message representing a parameter to be tuned.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "parameterId":
            suggest = "parameter_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1TrialParameterResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1TrialParameterResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1TrialParameterResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 parameter_id: str,
                 value: Any):
        """
        A message representing a parameter to be tuned.
        :param str parameter_id: The ID of the parameter. The parameter should be defined in StudySpec's Parameters.
        :param Any value: The value of the parameter. `number_value` will be set if a parameter defined in StudySpec is in type 'INTEGER', 'DOUBLE' or 'DISCRETE'. `string_value` will be set if a parameter defined in StudySpec is in type 'CATEGORICAL'.
        """
        pulumi.set(__self__, "parameter_id", parameter_id)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter(name="parameterId")
    def parameter_id(self) -> str:
        """
        The ID of the parameter. The parameter should be defined in StudySpec's Parameters.
        """
        return pulumi.get(self, "parameter_id")

    @property
    @pulumi.getter
    def value(self) -> Any:
        """
        The value of the parameter. `number_value` will be set if a parameter defined in StudySpec is in type 'INTEGER', 'DOUBLE' or 'DISCRETE'. `string_value` will be set if a parameter defined in StudySpec is in type 'CATEGORICAL'.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudAiplatformV1TrialResponse(dict):
    """
    A message representing a Trial. A Trial contains a unique set of Parameters that has been or will be evaluated, along with the objective metrics got by running the Trial.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "clientId":
            suggest = "client_id"
        elif key == "customJob":
            suggest = "custom_job"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "finalMeasurement":
            suggest = "final_measurement"
        elif key == "infeasibleReason":
            suggest = "infeasible_reason"
        elif key == "startTime":
            suggest = "start_time"
        elif key == "webAccessUris":
            suggest = "web_access_uris"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1TrialResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1TrialResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1TrialResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 client_id: str,
                 custom_job: str,
                 end_time: str,
                 final_measurement: 'outputs.GoogleCloudAiplatformV1MeasurementResponse',
                 infeasible_reason: str,
                 measurements: Sequence['outputs.GoogleCloudAiplatformV1MeasurementResponse'],
                 name: str,
                 parameters: Sequence['outputs.GoogleCloudAiplatformV1TrialParameterResponse'],
                 start_time: str,
                 state: str,
                 web_access_uris: Mapping[str, str]):
        """
        A message representing a Trial. A Trial contains a unique set of Parameters that has been or will be evaluated, along with the objective metrics got by running the Trial.
        :param str client_id: The identifier of the client that originally requested this Trial. Each client is identified by a unique client_id. When a client asks for a suggestion, Vertex AI Vizier will assign it a Trial. The client should evaluate the Trial, complete it, and report back to Vertex AI Vizier. If suggestion is asked again by same client_id before the Trial is completed, the same Trial will be returned. Multiple clients with different client_ids can ask for suggestions simultaneously, each of them will get their own Trial.
        :param str custom_job: The CustomJob name linked to the Trial. It's set for a HyperparameterTuningJob's Trial.
        :param str end_time: Time when the Trial's status changed to `SUCCEEDED` or `INFEASIBLE`.
        :param 'GoogleCloudAiplatformV1MeasurementResponse' final_measurement: The final measurement containing the objective value.
        :param str infeasible_reason: A human readable string describing why the Trial is infeasible. This is set only if Trial state is `INFEASIBLE`.
        :param Sequence['GoogleCloudAiplatformV1MeasurementResponse'] measurements: A list of measurements that are strictly lexicographically ordered by their induced tuples (steps, elapsed_duration). These are used for early stopping computations.
        :param str name: Resource name of the Trial assigned by the service.
        :param Sequence['GoogleCloudAiplatformV1TrialParameterResponse'] parameters: The parameters of the Trial.
        :param str start_time: Time when the Trial was started.
        :param str state: The detailed state of the Trial.
        :param Mapping[str, str] web_access_uris: URIs for accessing [interactive shells](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) (one URI for each training node). Only available if this trial is part of a HyperparameterTuningJob and the job's trial_job_spec.enable_web_access field is `true`. The keys are names of each node used for the trial; for example, `workerpool0-0` for the primary node, `workerpool1-0` for the first node in the second worker pool, and `workerpool1-1` for the second node in the second worker pool. The values are the URIs for each node's interactive shell.
        """
        pulumi.set(__self__, "client_id", client_id)
        pulumi.set(__self__, "custom_job", custom_job)
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "final_measurement", final_measurement)
        pulumi.set(__self__, "infeasible_reason", infeasible_reason)
        pulumi.set(__self__, "measurements", measurements)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "parameters", parameters)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "web_access_uris", web_access_uris)

    @property
    @pulumi.getter(name="clientId")
    def client_id(self) -> str:
        """
        The identifier of the client that originally requested this Trial. Each client is identified by a unique client_id. When a client asks for a suggestion, Vertex AI Vizier will assign it a Trial. The client should evaluate the Trial, complete it, and report back to Vertex AI Vizier. If suggestion is asked again by same client_id before the Trial is completed, the same Trial will be returned. Multiple clients with different client_ids can ask for suggestions simultaneously, each of them will get their own Trial.
        """
        return pulumi.get(self, "client_id")

    @property
    @pulumi.getter(name="customJob")
    def custom_job(self) -> str:
        """
        The CustomJob name linked to the Trial. It's set for a HyperparameterTuningJob's Trial.
        """
        return pulumi.get(self, "custom_job")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        Time when the Trial's status changed to `SUCCEEDED` or `INFEASIBLE`.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter(name="finalMeasurement")
    def final_measurement(self) -> 'outputs.GoogleCloudAiplatformV1MeasurementResponse':
        """
        The final measurement containing the objective value.
        """
        return pulumi.get(self, "final_measurement")

    @property
    @pulumi.getter(name="infeasibleReason")
    def infeasible_reason(self) -> str:
        """
        A human readable string describing why the Trial is infeasible. This is set only if Trial state is `INFEASIBLE`.
        """
        return pulumi.get(self, "infeasible_reason")

    @property
    @pulumi.getter
    def measurements(self) -> Sequence['outputs.GoogleCloudAiplatformV1MeasurementResponse']:
        """
        A list of measurements that are strictly lexicographically ordered by their induced tuples (steps, elapsed_duration). These are used for early stopping computations.
        """
        return pulumi.get(self, "measurements")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Resource name of the Trial assigned by the service.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def parameters(self) -> Sequence['outputs.GoogleCloudAiplatformV1TrialParameterResponse']:
        """
        The parameters of the Trial.
        """
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        Time when the Trial was started.
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The detailed state of the Trial.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="webAccessUris")
    def web_access_uris(self) -> Mapping[str, str]:
        """
        URIs for accessing [interactive shells](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) (one URI for each training node). Only available if this trial is part of a HyperparameterTuningJob and the job's trial_job_spec.enable_web_access field is `true`. The keys are names of each node used for the trial; for example, `workerpool0-0` for the primary node, `workerpool1-0` for the first node in the second worker pool, and `workerpool1-1` for the second node in the second worker pool. The values are the URIs for each node's interactive shell.
        """
        return pulumi.get(self, "web_access_uris")


@pulumi.output_type
class GoogleCloudAiplatformV1UnmanagedContainerModelResponse(dict):
    """
    Contains model information necessary to perform batch prediction without requiring a full model import.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "artifactUri":
            suggest = "artifact_uri"
        elif key == "containerSpec":
            suggest = "container_spec"
        elif key == "predictSchemata":
            suggest = "predict_schemata"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1UnmanagedContainerModelResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1UnmanagedContainerModelResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1UnmanagedContainerModelResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 artifact_uri: str,
                 container_spec: 'outputs.GoogleCloudAiplatformV1ModelContainerSpecResponse',
                 predict_schemata: 'outputs.GoogleCloudAiplatformV1PredictSchemataResponse'):
        """
        Contains model information necessary to perform batch prediction without requiring a full model import.
        :param str artifact_uri: The path to the directory containing the Model artifact and any of its supporting files.
        :param 'GoogleCloudAiplatformV1ModelContainerSpecResponse' container_spec: Input only. The specification of the container that is to be used when deploying this Model.
        :param 'GoogleCloudAiplatformV1PredictSchemataResponse' predict_schemata: Contains the schemata used in Model's predictions and explanations
        """
        pulumi.set(__self__, "artifact_uri", artifact_uri)
        pulumi.set(__self__, "container_spec", container_spec)
        pulumi.set(__self__, "predict_schemata", predict_schemata)

    @property
    @pulumi.getter(name="artifactUri")
    def artifact_uri(self) -> str:
        """
        The path to the directory containing the Model artifact and any of its supporting files.
        """
        return pulumi.get(self, "artifact_uri")

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> 'outputs.GoogleCloudAiplatformV1ModelContainerSpecResponse':
        """
        Input only. The specification of the container that is to be used when deploying this Model.
        """
        return pulumi.get(self, "container_spec")

    @property
    @pulumi.getter(name="predictSchemata")
    def predict_schemata(self) -> 'outputs.GoogleCloudAiplatformV1PredictSchemataResponse':
        """
        Contains the schemata used in Model's predictions and explanations
        """
        return pulumi.get(self, "predict_schemata")


@pulumi.output_type
class GoogleCloudAiplatformV1ValueResponse(dict):
    """
    Value is the value of the field.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "doubleValue":
            suggest = "double_value"
        elif key == "intValue":
            suggest = "int_value"
        elif key == "stringValue":
            suggest = "string_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1ValueResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1ValueResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1ValueResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 double_value: float,
                 int_value: str,
                 string_value: str):
        """
        Value is the value of the field.
        :param float double_value: A double value.
        :param str int_value: An integer value.
        :param str string_value: A string value.
        """
        pulumi.set(__self__, "double_value", double_value)
        pulumi.set(__self__, "int_value", int_value)
        pulumi.set(__self__, "string_value", string_value)

    @property
    @pulumi.getter(name="doubleValue")
    def double_value(self) -> float:
        """
        A double value.
        """
        return pulumi.get(self, "double_value")

    @property
    @pulumi.getter(name="intValue")
    def int_value(self) -> str:
        """
        An integer value.
        """
        return pulumi.get(self, "int_value")

    @property
    @pulumi.getter(name="stringValue")
    def string_value(self) -> str:
        """
        A string value.
        """
        return pulumi.get(self, "string_value")


@pulumi.output_type
class GoogleCloudAiplatformV1WorkerPoolSpecResponse(dict):
    """
    Represents the spec of a worker pool in a job.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerSpec":
            suggest = "container_spec"
        elif key == "diskSpec":
            suggest = "disk_spec"
        elif key == "machineSpec":
            suggest = "machine_spec"
        elif key == "nfsMounts":
            suggest = "nfs_mounts"
        elif key == "pythonPackageSpec":
            suggest = "python_package_spec"
        elif key == "replicaCount":
            suggest = "replica_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1WorkerPoolSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1WorkerPoolSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1WorkerPoolSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_spec: 'outputs.GoogleCloudAiplatformV1ContainerSpecResponse',
                 disk_spec: 'outputs.GoogleCloudAiplatformV1DiskSpecResponse',
                 machine_spec: 'outputs.GoogleCloudAiplatformV1MachineSpecResponse',
                 nfs_mounts: Sequence['outputs.GoogleCloudAiplatformV1NfsMountResponse'],
                 python_package_spec: 'outputs.GoogleCloudAiplatformV1PythonPackageSpecResponse',
                 replica_count: str):
        """
        Represents the spec of a worker pool in a job.
        :param 'GoogleCloudAiplatformV1ContainerSpecResponse' container_spec: The custom container task.
        :param 'GoogleCloudAiplatformV1DiskSpecResponse' disk_spec: Disk spec.
        :param 'GoogleCloudAiplatformV1MachineSpecResponse' machine_spec: Optional. Immutable. The specification of a single machine.
        :param Sequence['GoogleCloudAiplatformV1NfsMountResponse'] nfs_mounts: Optional. List of NFS mount spec.
        :param 'GoogleCloudAiplatformV1PythonPackageSpecResponse' python_package_spec: The Python packaged task.
        :param str replica_count: Optional. The number of worker replicas to use for this worker pool.
        """
        pulumi.set(__self__, "container_spec", container_spec)
        pulumi.set(__self__, "disk_spec", disk_spec)
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "nfs_mounts", nfs_mounts)
        pulumi.set(__self__, "python_package_spec", python_package_spec)
        pulumi.set(__self__, "replica_count", replica_count)

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> 'outputs.GoogleCloudAiplatformV1ContainerSpecResponse':
        """
        The custom container task.
        """
        return pulumi.get(self, "container_spec")

    @property
    @pulumi.getter(name="diskSpec")
    def disk_spec(self) -> 'outputs.GoogleCloudAiplatformV1DiskSpecResponse':
        """
        Disk spec.
        """
        return pulumi.get(self, "disk_spec")

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> 'outputs.GoogleCloudAiplatformV1MachineSpecResponse':
        """
        Optional. Immutable. The specification of a single machine.
        """
        return pulumi.get(self, "machine_spec")

    @property
    @pulumi.getter(name="nfsMounts")
    def nfs_mounts(self) -> Sequence['outputs.GoogleCloudAiplatformV1NfsMountResponse']:
        """
        Optional. List of NFS mount spec.
        """
        return pulumi.get(self, "nfs_mounts")

    @property
    @pulumi.getter(name="pythonPackageSpec")
    def python_package_spec(self) -> 'outputs.GoogleCloudAiplatformV1PythonPackageSpecResponse':
        """
        The Python packaged task.
        """
        return pulumi.get(self, "python_package_spec")

    @property
    @pulumi.getter(name="replicaCount")
    def replica_count(self) -> str:
        """
        Optional. The number of worker replicas to use for this worker pool.
        """
        return pulumi.get(self, "replica_count")


@pulumi.output_type
class GoogleCloudAiplatformV1XraiAttributionResponse(dict):
    """
    An explanation method that redistributes Integrated Gradients attributions to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Supported only by image Models.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "blurBaselineConfig":
            suggest = "blur_baseline_config"
        elif key == "smoothGradConfig":
            suggest = "smooth_grad_config"
        elif key == "stepCount":
            suggest = "step_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudAiplatformV1XraiAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudAiplatformV1XraiAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudAiplatformV1XraiAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 blur_baseline_config: 'outputs.GoogleCloudAiplatformV1BlurBaselineConfigResponse',
                 smooth_grad_config: 'outputs.GoogleCloudAiplatformV1SmoothGradConfigResponse',
                 step_count: int):
        """
        An explanation method that redistributes Integrated Gradients attributions to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Supported only by image Models.
        :param 'GoogleCloudAiplatformV1BlurBaselineConfigResponse' blur_baseline_config: Config for XRAI with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param 'GoogleCloudAiplatformV1SmoothGradConfigResponse' smooth_grad_config: Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        :param int step_count: The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        pulumi.set(__self__, "blur_baseline_config", blur_baseline_config)
        pulumi.set(__self__, "smooth_grad_config", smooth_grad_config)
        pulumi.set(__self__, "step_count", step_count)

    @property
    @pulumi.getter(name="blurBaselineConfig")
    def blur_baseline_config(self) -> 'outputs.GoogleCloudAiplatformV1BlurBaselineConfigResponse':
        """
        Config for XRAI with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        """
        return pulumi.get(self, "blur_baseline_config")

    @property
    @pulumi.getter(name="smoothGradConfig")
    def smooth_grad_config(self) -> 'outputs.GoogleCloudAiplatformV1SmoothGradConfigResponse':
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        return pulumi.get(self, "smooth_grad_config")

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> int:
        """
        The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        return pulumi.get(self, "step_count")


@pulumi.output_type
class GoogleIamV1BindingResponse(dict):
    """
    Associates `members`, or principals, with a `role`.
    """
    def __init__(__self__, *,
                 condition: 'outputs.GoogleTypeExprResponse',
                 members: Sequence[str],
                 role: str):
        """
        Associates `members`, or principals, with a `role`.
        :param 'GoogleTypeExprResponse' condition: The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        :param Sequence[str] members: Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        :param str role: Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        pulumi.set(__self__, "condition", condition)
        pulumi.set(__self__, "members", members)
        pulumi.set(__self__, "role", role)

    @property
    @pulumi.getter
    def condition(self) -> 'outputs.GoogleTypeExprResponse':
        """
        The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        """
        return pulumi.get(self, "condition")

    @property
    @pulumi.getter
    def members(self) -> Sequence[str]:
        """
        Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        """
        return pulumi.get(self, "members")

    @property
    @pulumi.getter
    def role(self) -> str:
        """
        Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        return pulumi.get(self, "role")


@pulumi.output_type
class GoogleRpcStatusResponse(dict):
    """
    The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors).
    """
    def __init__(__self__, *,
                 code: int,
                 details: Sequence[Mapping[str, Any]],
                 message: str):
        """
        The `Status` type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by [gRPC](https://github.com/grpc). Each `Status` message contains three pieces of data: error code, error message, and error details. You can find out more about this error model and how to work with it in the [API Design Guide](https://cloud.google.com/apis/design/errors).
        :param int code: The status code, which should be an enum value of google.rpc.Code.
        :param Sequence[Mapping[str, Any]] details: A list of messages that carry the error details. There is a common set of message types for APIs to use.
        :param str message: A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
        """
        pulumi.set(__self__, "code", code)
        pulumi.set(__self__, "details", details)
        pulumi.set(__self__, "message", message)

    @property
    @pulumi.getter
    def code(self) -> int:
        """
        The status code, which should be an enum value of google.rpc.Code.
        """
        return pulumi.get(self, "code")

    @property
    @pulumi.getter
    def details(self) -> Sequence[Mapping[str, Any]]:
        """
        A list of messages that carry the error details. There is a common set of message types for APIs to use.
        """
        return pulumi.get(self, "details")

    @property
    @pulumi.getter
    def message(self) -> str:
        """
        A developer-facing error message, which should be in English. Any user-facing error message should be localized and sent in the google.rpc.Status.details field, or localized by the client.
        """
        return pulumi.get(self, "message")


@pulumi.output_type
class GoogleTypeExprResponse(dict):
    """
    Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec. Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
    """
    def __init__(__self__, *,
                 description: str,
                 expression: str,
                 location: str,
                 title: str):
        """
        Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec. Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
        :param str description: Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        :param str expression: Textual representation of an expression in Common Expression Language syntax.
        :param str location: Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        :param str title: Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "location", location)
        pulumi.set(__self__, "title", title)

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter
    def expression(self) -> str:
        """
        Textual representation of an expression in Common Expression Language syntax.
        """
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def location(self) -> str:
        """
        Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def title(self) -> str:
        """
        Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        return pulumi.get(self, "title")


@pulumi.output_type
class GoogleTypeMoneyResponse(dict):
    """
    Represents an amount of money with its currency type.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "currencyCode":
            suggest = "currency_code"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleTypeMoneyResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleTypeMoneyResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleTypeMoneyResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 currency_code: str,
                 nanos: int,
                 units: str):
        """
        Represents an amount of money with its currency type.
        :param str currency_code: The three-letter currency code defined in ISO 4217.
        :param int nanos: Number of nano (10^-9) units of the amount. The value must be between -999,999,999 and +999,999,999 inclusive. If `units` is positive, `nanos` must be positive or zero. If `units` is zero, `nanos` can be positive, zero, or negative. If `units` is negative, `nanos` must be negative or zero. For example $-1.75 is represented as `units`=-1 and `nanos`=-750,000,000.
        :param str units: The whole units of the amount. For example if `currencyCode` is `"USD"`, then 1 unit is one US dollar.
        """
        pulumi.set(__self__, "currency_code", currency_code)
        pulumi.set(__self__, "nanos", nanos)
        pulumi.set(__self__, "units", units)

    @property
    @pulumi.getter(name="currencyCode")
    def currency_code(self) -> str:
        """
        The three-letter currency code defined in ISO 4217.
        """
        return pulumi.get(self, "currency_code")

    @property
    @pulumi.getter
    def nanos(self) -> int:
        """
        Number of nano (10^-9) units of the amount. The value must be between -999,999,999 and +999,999,999 inclusive. If `units` is positive, `nanos` must be positive or zero. If `units` is zero, `nanos` can be positive, zero, or negative. If `units` is negative, `nanos` must be negative or zero. For example $-1.75 is represented as `units`=-1 and `nanos`=-750,000,000.
        """
        return pulumi.get(self, "nanos")

    @property
    @pulumi.getter
    def units(self) -> str:
        """
        The whole units of the amount. For example if `currencyCode` is `"USD"`, then 1 unit is one US dollar.
        """
        return pulumi.get(self, "units")


