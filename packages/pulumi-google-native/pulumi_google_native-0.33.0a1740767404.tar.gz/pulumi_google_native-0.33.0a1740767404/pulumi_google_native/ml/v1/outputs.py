# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from ... import _utilities
from . import outputs
from ._enums import *

__all__ = [
    'GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse',
    'GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse',
    'GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse',
    'GoogleCloudMlV1_Measurement_MetricResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_CategoricalValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_DiscreteValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentCategoricalValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentDiscreteValueSpecResponse',
    'GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentIntValueSpecResponse',
    'GoogleCloudMlV1_StudyConfig_MetricSpecResponse',
    'GoogleCloudMlV1_StudyConfig_ParameterSpecResponse',
    'GoogleCloudMlV1_Trial_ParameterResponse',
    'GoogleCloudMlV1__AcceleratorConfigResponse',
    'GoogleCloudMlV1__AutoScalingResponse',
    'GoogleCloudMlV1__AutomatedStoppingConfigResponse',
    'GoogleCloudMlV1__BuiltInAlgorithmOutputResponse',
    'GoogleCloudMlV1__ContainerPortResponse',
    'GoogleCloudMlV1__ContainerSpecResponse',
    'GoogleCloudMlV1__DiskConfigResponse',
    'GoogleCloudMlV1__EncryptionConfigResponse',
    'GoogleCloudMlV1__EnvVarResponse',
    'GoogleCloudMlV1__ExplanationConfigResponse',
    'GoogleCloudMlV1__HyperparameterOutputResponse',
    'GoogleCloudMlV1__HyperparameterSpecResponse',
    'GoogleCloudMlV1__IntegratedGradientsAttributionResponse',
    'GoogleCloudMlV1__ManualScalingResponse',
    'GoogleCloudMlV1__MeasurementResponse',
    'GoogleCloudMlV1__MetricSpecResponse',
    'GoogleCloudMlV1__ParameterSpecResponse',
    'GoogleCloudMlV1__PredictionInputResponse',
    'GoogleCloudMlV1__PredictionOutputResponse',
    'GoogleCloudMlV1__ReplicaConfigResponse',
    'GoogleCloudMlV1__RequestLoggingConfigResponse',
    'GoogleCloudMlV1__RouteMapResponse',
    'GoogleCloudMlV1__SampledShapleyAttributionResponse',
    'GoogleCloudMlV1__SchedulingResponse',
    'GoogleCloudMlV1__StudyConfigResponse',
    'GoogleCloudMlV1__TrainingInputResponse',
    'GoogleCloudMlV1__TrainingOutputResponse',
    'GoogleCloudMlV1__VersionResponse',
    'GoogleCloudMlV1__XraiAttributionResponse',
    'GoogleIamV1__AuditConfigResponse',
    'GoogleIamV1__AuditLogConfigResponse',
    'GoogleIamV1__BindingResponse',
    'GoogleType__ExprResponse',
]

@pulumi.output_type
class GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useElapsedTime":
            suggest = "use_elapsed_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 use_elapsed_time: bool):
        """
        :param bool use_elapsed_time: If true, measurement.elapsed_time is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.steps will be used as the x-axis.
        """
        pulumi.set(__self__, "use_elapsed_time", use_elapsed_time)

    @property
    @pulumi.getter(name="useElapsedTime")
    def use_elapsed_time(self) -> bool:
        """
        If true, measurement.elapsed_time is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.steps will be used as the x-axis.
        """
        return pulumi.get(self, "use_elapsed_time")


@pulumi.output_type
class GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse(dict):
    """
    The median automated stopping rule stops a pending trial if the trial's best objective_value is strictly below the median 'performance' of all completed trials reported up to the trial's last measurement. Currently, 'performance' refers to the running average of the objective values reported by the trial in each measurement.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "useElapsedTime":
            suggest = "use_elapsed_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 use_elapsed_time: bool):
        """
        The median automated stopping rule stops a pending trial if the trial's best objective_value is strictly below the median 'performance' of all completed trials reported up to the trial's last measurement. Currently, 'performance' refers to the running average of the objective values reported by the trial in each measurement.
        :param bool use_elapsed_time: If true, the median automated stopping rule applies to measurement.use_elapsed_time, which means the elapsed_time field of the current trial's latest measurement is used to compute the median objective value for each completed trial.
        """
        pulumi.set(__self__, "use_elapsed_time", use_elapsed_time)

    @property
    @pulumi.getter(name="useElapsedTime")
    def use_elapsed_time(self) -> bool:
        """
        If true, the median automated stopping rule applies to measurement.use_elapsed_time, which means the elapsed_time field of the current trial's latest measurement is used to compute the median objective value for each completed trial.
        """
        return pulumi.get(self, "use_elapsed_time")


@pulumi.output_type
class GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse(dict):
    """
    An observed value of a metric.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "objectiveValue":
            suggest = "objective_value"
        elif key == "trainingStep":
            suggest = "training_step"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 objective_value: float,
                 training_step: str):
        """
        An observed value of a metric.
        :param float objective_value: The objective value at this training step.
        :param str training_step: The global training step for this metric.
        """
        pulumi.set(__self__, "objective_value", objective_value)
        pulumi.set(__self__, "training_step", training_step)

    @property
    @pulumi.getter(name="objectiveValue")
    def objective_value(self) -> float:
        """
        The objective value at this training step.
        """
        return pulumi.get(self, "objective_value")

    @property
    @pulumi.getter(name="trainingStep")
    def training_step(self) -> str:
        """
        The global training step for this metric.
        """
        return pulumi.get(self, "training_step")


@pulumi.output_type
class GoogleCloudMlV1_Measurement_MetricResponse(dict):
    """
    A message representing a metric in the measurement.
    """
    def __init__(__self__, *,
                 metric: str,
                 value: float):
        """
        A message representing a metric in the measurement.
        :param str metric: Metric name.
        :param float value: The value for this metric.
        """
        pulumi.set(__self__, "metric", metric)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def metric(self) -> str:
        """
        Metric name.
        """
        return pulumi.get(self, "metric")

    @property
    @pulumi.getter
    def value(self) -> float:
        """
        The value for this metric.
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_CategoricalValueSpecResponse(dict):
    def __init__(__self__, *,
                 values: Sequence[str]):
        """
        :param Sequence[str] values: Must be specified if type is `CATEGORICAL`. The list of possible categories.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        Must be specified if type is `CATEGORICAL`. The list of possible categories.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_DiscreteValueSpecResponse(dict):
    def __init__(__self__, *,
                 values: Sequence[float]):
        """
        :param Sequence[float] values: Must be specified if type is `DISCRETE`. A list of feasible points. The list should be in strictly increasing order. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[float]:
        """
        Must be specified if type is `DISCRETE`. A list of feasible points. The list should be in strictly increasing order. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_value: float,
                 min_value: float):
        """
        :param float max_value: Must be specified if type is `DOUBLE`. Maximum value of the parameter.
        :param float min_value: Must be specified if type is `DOUBLE`. Minimum value of the parameter.
        """
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> float:
        """
        Must be specified if type is `DOUBLE`. Maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> float:
        """
        Must be specified if type is `DOUBLE`. Minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse(dict):
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_value: str,
                 min_value: str):
        """
        :param str max_value: Must be specified if type is `INTEGER`. Maximum value of the parameter.
        :param str min_value: Must be specified if type is `INTEGER`. Minimum value of the parameter.
        """
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> str:
        """
        Must be specified if type is `INTEGER`. Maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> str:
        """
        Must be specified if type is `INTEGER`. Minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentCategoricalValueSpecResponse(dict):
    """
    Represents the spec to match categorical values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[str]):
        """
        Represents the spec to match categorical values from parent parameter.
        :param Sequence[str] values: Matches values of the parent parameter with type 'CATEGORICAL'. All values must exist in `categorical_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        Matches values of the parent parameter with type 'CATEGORICAL'. All values must exist in `categorical_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentDiscreteValueSpecResponse(dict):
    """
    Represents the spec to match discrete values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[float]):
        """
        Represents the spec to match discrete values from parent parameter.
        :param Sequence[float] values: Matches values of the parent parameter with type 'DISCRETE'. All values must exist in `discrete_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[float]:
        """
        Matches values of the parent parameter with type 'DISCRETE'. All values must exist in `discrete_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentIntValueSpecResponse(dict):
    """
    Represents the spec to match integer values from parent parameter.
    """
    def __init__(__self__, *,
                 values: Sequence[str]):
        """
        Represents the spec to match integer values from parent parameter.
        :param Sequence[str] values: Matches values of the parent parameter with type 'INTEGER'. All values must lie in `integer_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> Sequence[str]:
        """
        Matches values of the parent parameter with type 'INTEGER'. All values must lie in `integer_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfig_MetricSpecResponse(dict):
    """
    Represents a metric to optimize.
    """
    def __init__(__self__, *,
                 goal: str,
                 metric: str):
        """
        Represents a metric to optimize.
        :param str goal: The optimization goal of the metric.
        :param str metric: The name of the metric.
        """
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "metric", metric)

    @property
    @pulumi.getter
    def goal(self) -> str:
        """
        The optimization goal of the metric.
        """
        return pulumi.get(self, "goal")

    @property
    @pulumi.getter
    def metric(self) -> str:
        """
        The name of the metric.
        """
        return pulumi.get(self, "metric")


@pulumi.output_type
class GoogleCloudMlV1_StudyConfig_ParameterSpecResponse(dict):
    """
    Represents a single parameter to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "categoricalValueSpec":
            suggest = "categorical_value_spec"
        elif key == "childParameterSpecs":
            suggest = "child_parameter_specs"
        elif key == "discreteValueSpec":
            suggest = "discrete_value_spec"
        elif key == "doubleValueSpec":
            suggest = "double_value_spec"
        elif key == "integerValueSpec":
            suggest = "integer_value_spec"
        elif key == "parentCategoricalValues":
            suggest = "parent_categorical_values"
        elif key == "parentDiscreteValues":
            suggest = "parent_discrete_values"
        elif key == "parentIntValues":
            suggest = "parent_int_values"
        elif key == "scaleType":
            suggest = "scale_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_StudyConfig_ParameterSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_StudyConfig_ParameterSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_StudyConfig_ParameterSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 categorical_value_spec: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_CategoricalValueSpecResponse',
                 child_parameter_specs: Sequence['outputs.GoogleCloudMlV1_StudyConfig_ParameterSpecResponse'],
                 discrete_value_spec: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_DiscreteValueSpecResponse',
                 double_value_spec: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse',
                 integer_value_spec: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse',
                 parameter: str,
                 parent_categorical_values: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentCategoricalValueSpecResponse',
                 parent_discrete_values: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentDiscreteValueSpecResponse',
                 parent_int_values: 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentIntValueSpecResponse',
                 scale_type: str,
                 type: str):
        """
        Represents a single parameter to optimize.
        :param 'GoogleCloudMlV1_StudyConfigParameterSpec_CategoricalValueSpecResponse' categorical_value_spec: The value spec for a 'CATEGORICAL' parameter.
        :param Sequence['GoogleCloudMlV1_StudyConfig_ParameterSpecResponse'] child_parameter_specs: A child node is active if the parameter's value matches the child node's matching_parent_values. If two items in child_parameter_specs have the same name, they must have disjoint matching_parent_values.
        :param 'GoogleCloudMlV1_StudyConfigParameterSpec_DiscreteValueSpecResponse' discrete_value_spec: The value spec for a 'DISCRETE' parameter.
        :param 'GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse' double_value_spec: The value spec for a 'DOUBLE' parameter.
        :param 'GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse' integer_value_spec: The value spec for an 'INTEGER' parameter.
        :param str parameter: The parameter name must be unique amongst all ParameterSpecs.
        :param str scale_type: How the parameter should be scaled. Leave unset for categorical parameters.
        :param str type: The type of the parameter.
        """
        pulumi.set(__self__, "categorical_value_spec", categorical_value_spec)
        pulumi.set(__self__, "child_parameter_specs", child_parameter_specs)
        pulumi.set(__self__, "discrete_value_spec", discrete_value_spec)
        pulumi.set(__self__, "double_value_spec", double_value_spec)
        pulumi.set(__self__, "integer_value_spec", integer_value_spec)
        pulumi.set(__self__, "parameter", parameter)
        pulumi.set(__self__, "parent_categorical_values", parent_categorical_values)
        pulumi.set(__self__, "parent_discrete_values", parent_discrete_values)
        pulumi.set(__self__, "parent_int_values", parent_int_values)
        pulumi.set(__self__, "scale_type", scale_type)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="categoricalValueSpec")
    def categorical_value_spec(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_CategoricalValueSpecResponse':
        """
        The value spec for a 'CATEGORICAL' parameter.
        """
        return pulumi.get(self, "categorical_value_spec")

    @property
    @pulumi.getter(name="childParameterSpecs")
    def child_parameter_specs(self) -> Sequence['outputs.GoogleCloudMlV1_StudyConfig_ParameterSpecResponse']:
        """
        A child node is active if the parameter's value matches the child node's matching_parent_values. If two items in child_parameter_specs have the same name, they must have disjoint matching_parent_values.
        """
        return pulumi.get(self, "child_parameter_specs")

    @property
    @pulumi.getter(name="discreteValueSpec")
    def discrete_value_spec(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_DiscreteValueSpecResponse':
        """
        The value spec for a 'DISCRETE' parameter.
        """
        return pulumi.get(self, "discrete_value_spec")

    @property
    @pulumi.getter(name="doubleValueSpec")
    def double_value_spec(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_DoubleValueSpecResponse':
        """
        The value spec for a 'DOUBLE' parameter.
        """
        return pulumi.get(self, "double_value_spec")

    @property
    @pulumi.getter(name="integerValueSpec")
    def integer_value_spec(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_IntegerValueSpecResponse':
        """
        The value spec for an 'INTEGER' parameter.
        """
        return pulumi.get(self, "integer_value_spec")

    @property
    @pulumi.getter
    def parameter(self) -> str:
        """
        The parameter name must be unique amongst all ParameterSpecs.
        """
        return pulumi.get(self, "parameter")

    @property
    @pulumi.getter(name="parentCategoricalValues")
    def parent_categorical_values(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentCategoricalValueSpecResponse':
        return pulumi.get(self, "parent_categorical_values")

    @property
    @pulumi.getter(name="parentDiscreteValues")
    def parent_discrete_values(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentDiscreteValueSpecResponse':
        return pulumi.get(self, "parent_discrete_values")

    @property
    @pulumi.getter(name="parentIntValues")
    def parent_int_values(self) -> 'outputs.GoogleCloudMlV1_StudyConfigParameterSpec_MatchingParentIntValueSpecResponse':
        return pulumi.get(self, "parent_int_values")

    @property
    @pulumi.getter(name="scaleType")
    def scale_type(self) -> str:
        """
        How the parameter should be scaled. Leave unset for categorical parameters.
        """
        return pulumi.get(self, "scale_type")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        The type of the parameter.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GoogleCloudMlV1_Trial_ParameterResponse(dict):
    """
    A message representing a parameter to be tuned. Contains the name of the parameter and the suggested value to use for this trial.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "floatValue":
            suggest = "float_value"
        elif key == "intValue":
            suggest = "int_value"
        elif key == "stringValue":
            suggest = "string_value"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1_Trial_ParameterResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1_Trial_ParameterResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1_Trial_ParameterResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 float_value: float,
                 int_value: str,
                 parameter: str,
                 string_value: str):
        """
        A message representing a parameter to be tuned. Contains the name of the parameter and the suggested value to use for this trial.
        :param float float_value: Must be set if ParameterType is DOUBLE or DISCRETE.
        :param str int_value: Must be set if ParameterType is INTEGER
        :param str parameter: The name of the parameter.
        :param str string_value: Must be set if ParameterTypeis CATEGORICAL
        """
        pulumi.set(__self__, "float_value", float_value)
        pulumi.set(__self__, "int_value", int_value)
        pulumi.set(__self__, "parameter", parameter)
        pulumi.set(__self__, "string_value", string_value)

    @property
    @pulumi.getter(name="floatValue")
    def float_value(self) -> float:
        """
        Must be set if ParameterType is DOUBLE or DISCRETE.
        """
        return pulumi.get(self, "float_value")

    @property
    @pulumi.getter(name="intValue")
    def int_value(self) -> str:
        """
        Must be set if ParameterType is INTEGER
        """
        return pulumi.get(self, "int_value")

    @property
    @pulumi.getter
    def parameter(self) -> str:
        """
        The name of the parameter.
        """
        return pulumi.get(self, "parameter")

    @property
    @pulumi.getter(name="stringValue")
    def string_value(self) -> str:
        """
        Must be set if ParameterTypeis CATEGORICAL
        """
        return pulumi.get(self, "string_value")


@pulumi.output_type
class GoogleCloudMlV1__AcceleratorConfigResponse(dict):
    """
    Represents a hardware accelerator request config. Note that the AcceleratorConfig can be used in both Jobs and Versions. Learn more about [accelerators for training](/ml-engine/docs/using-gpus) and [accelerators for online prediction](/ml-engine/docs/machine-types-online-prediction#gpus).
    """
    def __init__(__self__, *,
                 count: str,
                 type: str):
        """
        Represents a hardware accelerator request config. Note that the AcceleratorConfig can be used in both Jobs and Versions. Learn more about [accelerators for training](/ml-engine/docs/using-gpus) and [accelerators for online prediction](/ml-engine/docs/machine-types-online-prediction#gpus).
        :param str count: The number of accelerators to attach to each machine running the job.
        :param str type: The type of accelerator to use.
        """
        pulumi.set(__self__, "count", count)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter
    def count(self) -> str:
        """
        The number of accelerators to attach to each machine running the job.
        """
        return pulumi.get(self, "count")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        The type of accelerator to use.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GoogleCloudMlV1__AutoScalingResponse(dict):
    """
    Options for automatically scaling a model.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxNodes":
            suggest = "max_nodes"
        elif key == "minNodes":
            suggest = "min_nodes"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__AutoScalingResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__AutoScalingResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__AutoScalingResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_nodes: int,
                 metrics: Sequence['outputs.GoogleCloudMlV1__MetricSpecResponse'],
                 min_nodes: int):
        """
        Options for automatically scaling a model.
        :param int max_nodes: The maximum number of nodes to scale this model under load. The actual value will depend on resource quota and availability.
        :param Sequence['GoogleCloudMlV1__MetricSpecResponse'] metrics: MetricSpec contains the specifications to use to calculate the desired nodes count.
        :param int min_nodes: Optional. The minimum number of nodes to allocate for this model. These nodes are always up, starting from the time the model is deployed. Therefore, the cost of operating this model will be at least `rate` * `min_nodes` * number of hours since last billing cycle, where `rate` is the cost per node-hour as documented in the [pricing guide](/ml-engine/docs/pricing), even if no predictions are performed. There is additional cost for each prediction performed. Unlike manual scaling, if the load gets too heavy for the nodes that are up, the service will automatically add nodes to handle the increased load as well as scale back as traffic drops, always maintaining at least `min_nodes`. You will be charged for the time in which additional nodes are used. If `min_nodes` is not specified and AutoScaling is used with a [legacy (MLS1) machine type](/ml-engine/docs/machine-types-online-prediction), `min_nodes` defaults to 0, in which case, when traffic to a model stops (and after a cool-down period), nodes will be shut down and no charges will be incurred until traffic to the model resumes. If `min_nodes` is not specified and AutoScaling is used with a [Compute Engine (N1) machine type](/ml-engine/docs/machine-types-online-prediction), `min_nodes` defaults to 1. `min_nodes` must be at least 1 for use with a Compute Engine machine type. You can set `min_nodes` when creating the model version, and you can also update `min_nodes` for an existing version: update_body.json: { 'autoScaling': { 'minNodes': 5 } } HTTP request: PATCH https://ml.googleapis.com/v1/{name=projects/*/models/*/versions/*}?update_mask=autoScaling.minNodes -d @./update_body.json 
        """
        pulumi.set(__self__, "max_nodes", max_nodes)
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "min_nodes", min_nodes)

    @property
    @pulumi.getter(name="maxNodes")
    def max_nodes(self) -> int:
        """
        The maximum number of nodes to scale this model under load. The actual value will depend on resource quota and availability.
        """
        return pulumi.get(self, "max_nodes")

    @property
    @pulumi.getter
    def metrics(self) -> Sequence['outputs.GoogleCloudMlV1__MetricSpecResponse']:
        """
        MetricSpec contains the specifications to use to calculate the desired nodes count.
        """
        return pulumi.get(self, "metrics")

    @property
    @pulumi.getter(name="minNodes")
    def min_nodes(self) -> int:
        """
        Optional. The minimum number of nodes to allocate for this model. These nodes are always up, starting from the time the model is deployed. Therefore, the cost of operating this model will be at least `rate` * `min_nodes` * number of hours since last billing cycle, where `rate` is the cost per node-hour as documented in the [pricing guide](/ml-engine/docs/pricing), even if no predictions are performed. There is additional cost for each prediction performed. Unlike manual scaling, if the load gets too heavy for the nodes that are up, the service will automatically add nodes to handle the increased load as well as scale back as traffic drops, always maintaining at least `min_nodes`. You will be charged for the time in which additional nodes are used. If `min_nodes` is not specified and AutoScaling is used with a [legacy (MLS1) machine type](/ml-engine/docs/machine-types-online-prediction), `min_nodes` defaults to 0, in which case, when traffic to a model stops (and after a cool-down period), nodes will be shut down and no charges will be incurred until traffic to the model resumes. If `min_nodes` is not specified and AutoScaling is used with a [Compute Engine (N1) machine type](/ml-engine/docs/machine-types-online-prediction), `min_nodes` defaults to 1. `min_nodes` must be at least 1 for use with a Compute Engine machine type. You can set `min_nodes` when creating the model version, and you can also update `min_nodes` for an existing version: update_body.json: { 'autoScaling': { 'minNodes': 5 } } HTTP request: PATCH https://ml.googleapis.com/v1/{name=projects/*/models/*/versions/*}?update_mask=autoScaling.minNodes -d @./update_body.json 
        """
        return pulumi.get(self, "min_nodes")


@pulumi.output_type
class GoogleCloudMlV1__AutomatedStoppingConfigResponse(dict):
    """
    Configuration for Automated Early Stopping of Trials. If no implementation_config is set, automated early stopping will not be run.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "decayCurveStoppingConfig":
            suggest = "decay_curve_stopping_config"
        elif key == "medianAutomatedStoppingConfig":
            suggest = "median_automated_stopping_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__AutomatedStoppingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__AutomatedStoppingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__AutomatedStoppingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 decay_curve_stopping_config: 'outputs.GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse',
                 median_automated_stopping_config: 'outputs.GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse'):
        """
        Configuration for Automated Early Stopping of Trials. If no implementation_config is set, automated early stopping will not be run.
        """
        pulumi.set(__self__, "decay_curve_stopping_config", decay_curve_stopping_config)
        pulumi.set(__self__, "median_automated_stopping_config", median_automated_stopping_config)

    @property
    @pulumi.getter(name="decayCurveStoppingConfig")
    def decay_curve_stopping_config(self) -> 'outputs.GoogleCloudMlV1_AutomatedStoppingConfig_DecayCurveAutomatedStoppingConfigResponse':
        return pulumi.get(self, "decay_curve_stopping_config")

    @property
    @pulumi.getter(name="medianAutomatedStoppingConfig")
    def median_automated_stopping_config(self) -> 'outputs.GoogleCloudMlV1_AutomatedStoppingConfig_MedianAutomatedStoppingConfigResponse':
        return pulumi.get(self, "median_automated_stopping_config")


@pulumi.output_type
class GoogleCloudMlV1__BuiltInAlgorithmOutputResponse(dict):
    """
    Represents output related to a built-in algorithm Job.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "modelPath":
            suggest = "model_path"
        elif key == "pythonVersion":
            suggest = "python_version"
        elif key == "runtimeVersion":
            suggest = "runtime_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__BuiltInAlgorithmOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__BuiltInAlgorithmOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__BuiltInAlgorithmOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 framework: str,
                 model_path: str,
                 python_version: str,
                 runtime_version: str):
        """
        Represents output related to a built-in algorithm Job.
        :param str framework: Framework on which the built-in algorithm was trained.
        :param str model_path: The Cloud Storage path to the `model/` directory where the training job saves the trained model. Only set for successful jobs that don't use hyperparameter tuning.
        :param str python_version: Python version on which the built-in algorithm was trained.
        :param str runtime_version: AI Platform runtime version on which the built-in algorithm was trained.
        """
        pulumi.set(__self__, "framework", framework)
        pulumi.set(__self__, "model_path", model_path)
        pulumi.set(__self__, "python_version", python_version)
        pulumi.set(__self__, "runtime_version", runtime_version)

    @property
    @pulumi.getter
    def framework(self) -> str:
        """
        Framework on which the built-in algorithm was trained.
        """
        return pulumi.get(self, "framework")

    @property
    @pulumi.getter(name="modelPath")
    def model_path(self) -> str:
        """
        The Cloud Storage path to the `model/` directory where the training job saves the trained model. Only set for successful jobs that don't use hyperparameter tuning.
        """
        return pulumi.get(self, "model_path")

    @property
    @pulumi.getter(name="pythonVersion")
    def python_version(self) -> str:
        """
        Python version on which the built-in algorithm was trained.
        """
        return pulumi.get(self, "python_version")

    @property
    @pulumi.getter(name="runtimeVersion")
    def runtime_version(self) -> str:
        """
        AI Platform runtime version on which the built-in algorithm was trained.
        """
        return pulumi.get(self, "runtime_version")


@pulumi.output_type
class GoogleCloudMlV1__ContainerPortResponse(dict):
    """
    Represents a network port in a single container. This message is a subset of the [Kubernetes ContainerPort v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#containerport-v1-core).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "containerPort":
            suggest = "container_port"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__ContainerPortResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__ContainerPortResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__ContainerPortResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 container_port: int):
        """
        Represents a network port in a single container. This message is a subset of the [Kubernetes ContainerPort v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#containerport-v1-core).
        :param int container_port: Number of the port to expose on the container. This must be a valid port number: 0 < PORT_NUMBER < 65536.
        """
        pulumi.set(__self__, "container_port", container_port)

    @property
    @pulumi.getter(name="containerPort")
    def container_port(self) -> int:
        """
        Number of the port to expose on the container. This must be a valid port number: 0 < PORT_NUMBER < 65536.
        """
        return pulumi.get(self, "container_port")


@pulumi.output_type
class GoogleCloudMlV1__ContainerSpecResponse(dict):
    """
    Specification of a custom container for serving predictions. This message is a subset of the [Kubernetes Container v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
    """
    def __init__(__self__, *,
                 args: Sequence[str],
                 command: Sequence[str],
                 env: Sequence['outputs.GoogleCloudMlV1__EnvVarResponse'],
                 image: str,
                 ports: Sequence['outputs.GoogleCloudMlV1__ContainerPortResponse']):
        """
        Specification of a custom container for serving predictions. This message is a subset of the [Kubernetes Container v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        :param Sequence[str] args: Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `commmand` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the [Docker documentation about how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        :param Sequence[str] command: Immutable. Specifies the command that runs when the container starts. This overrides the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the [Docker documentation about how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        :param Sequence['GoogleCloudMlV1__EnvVarResponse'] env: Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        :param str image: URI of the Docker image to be used as the custom container for serving predictions. This URI must identify [an image in Artifact Registry](/artifact-registry/docs/overview) and begin with the hostname `{REGION}-docker.pkg.dev`, where `{REGION}` is replaced by the region that matches AI Platform Prediction [regional endpoint](/ai-platform/prediction/docs/regional-endpoints) that you are using. For example, if you are using the `us-central1-ml.googleapis.com` endpoint, then this URI must begin with `us-central1-docker.pkg.dev`. To use a custom container, the [AI Platform Google-managed service account](/ai-platform/prediction/docs/custom-service-account#default) must have permission to pull (read) the Docker image at this URI. The AI Platform Google-managed service account has the following format: `service-{PROJECT_NUMBER}@cloud-ml.google.com.iam.gserviceaccount.com` {PROJECT_NUMBER} is replaced by your Google Cloud project number. By default, this service account has necessary permissions to pull an Artifact Registry image in the same Google Cloud project where you are using AI Platform Prediction. In this case, no configuration is necessary. If you want to use an image from a different Google Cloud project, learn how to [grant the Artifact Registry Reader (roles/artifactregistry.reader) role for a repository](/artifact-registry/docs/access-control#grant-repo) to your projet's AI Platform Google-managed service account. To learn about the requirements for the Docker image itself, read [Custom container requirements](/ai-platform/prediction/docs/custom-container-requirements).
        :param Sequence['GoogleCloudMlV1__ContainerPortResponse'] ports: Immutable. List of ports to expose from the container. AI Platform Prediction sends any prediction requests that it receives to the first port on this list. AI Platform Prediction also sends [liveness and health checks](/ai-platform/prediction/docs/custom-container-requirements#health) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` AI Platform Prediction does not use ports other than the first one listed. This field corresponds to the `ports` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        """
        pulumi.set(__self__, "args", args)
        pulumi.set(__self__, "command", command)
        pulumi.set(__self__, "env", env)
        pulumi.set(__self__, "image", image)
        pulumi.set(__self__, "ports", ports)

    @property
    @pulumi.getter
    def args(self) -> Sequence[str]:
        """
        Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `commmand` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the [Docker documentation about how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        """
        return pulumi.get(self, "args")

    @property
    @pulumi.getter
    def command(self) -> Sequence[str]:
        """
        Immutable. Specifies the command that runs when the container starts. This overrides the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the [Docker documentation about how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        """
        return pulumi.get(self, "command")

    @property
    @pulumi.getter
    def env(self) -> Sequence['outputs.GoogleCloudMlV1__EnvVarResponse']:
        """
        Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        """
        return pulumi.get(self, "env")

    @property
    @pulumi.getter
    def image(self) -> str:
        """
        URI of the Docker image to be used as the custom container for serving predictions. This URI must identify [an image in Artifact Registry](/artifact-registry/docs/overview) and begin with the hostname `{REGION}-docker.pkg.dev`, where `{REGION}` is replaced by the region that matches AI Platform Prediction [regional endpoint](/ai-platform/prediction/docs/regional-endpoints) that you are using. For example, if you are using the `us-central1-ml.googleapis.com` endpoint, then this URI must begin with `us-central1-docker.pkg.dev`. To use a custom container, the [AI Platform Google-managed service account](/ai-platform/prediction/docs/custom-service-account#default) must have permission to pull (read) the Docker image at this URI. The AI Platform Google-managed service account has the following format: `service-{PROJECT_NUMBER}@cloud-ml.google.com.iam.gserviceaccount.com` {PROJECT_NUMBER} is replaced by your Google Cloud project number. By default, this service account has necessary permissions to pull an Artifact Registry image in the same Google Cloud project where you are using AI Platform Prediction. In this case, no configuration is necessary. If you want to use an image from a different Google Cloud project, learn how to [grant the Artifact Registry Reader (roles/artifactregistry.reader) role for a repository](/artifact-registry/docs/access-control#grant-repo) to your projet's AI Platform Google-managed service account. To learn about the requirements for the Docker image itself, read [Custom container requirements](/ai-platform/prediction/docs/custom-container-requirements).
        """
        return pulumi.get(self, "image")

    @property
    @pulumi.getter
    def ports(self) -> Sequence['outputs.GoogleCloudMlV1__ContainerPortResponse']:
        """
        Immutable. List of ports to expose from the container. AI Platform Prediction sends any prediction requests that it receives to the first port on this list. AI Platform Prediction also sends [liveness and health checks](/ai-platform/prediction/docs/custom-container-requirements#health) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` AI Platform Prediction does not use ports other than the first one listed. This field corresponds to the `ports` field of the [Kubernetes Containers v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#container-v1-core).
        """
        return pulumi.get(self, "ports")


@pulumi.output_type
class GoogleCloudMlV1__DiskConfigResponse(dict):
    """
    Represents the config of disk options.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bootDiskSizeGb":
            suggest = "boot_disk_size_gb"
        elif key == "bootDiskType":
            suggest = "boot_disk_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__DiskConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__DiskConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__DiskConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 boot_disk_size_gb: int,
                 boot_disk_type: str):
        """
        Represents the config of disk options.
        :param int boot_disk_size_gb: Size in GB of the boot disk (default is 100GB).
        :param str boot_disk_type: Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        pulumi.set(__self__, "boot_disk_type", boot_disk_type)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> int:
        """
        Size in GB of the boot disk (default is 100GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> str:
        """
        Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")


@pulumi.output_type
class GoogleCloudMlV1__EncryptionConfigResponse(dict):
    """
    Represents a custom encryption key configuration that can be applied to a resource.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "kmsKeyName":
            suggest = "kms_key_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__EncryptionConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__EncryptionConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__EncryptionConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 kms_key_name: str):
        """
        Represents a custom encryption key configuration that can be applied to a resource.
        :param str kms_key_name: The Cloud KMS resource identifier of the customer-managed encryption key used to protect a resource, such as a training job. It has the following format: `projects/{PROJECT_ID}/locations/{REGION}/keyRings/{KEY_RING_NAME}/cryptoKeys/{KEY_NAME}`
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> str:
        """
        The Cloud KMS resource identifier of the customer-managed encryption key used to protect a resource, such as a training job. It has the following format: `projects/{PROJECT_ID}/locations/{REGION}/keyRings/{KEY_RING_NAME}/cryptoKeys/{KEY_NAME}`
        """
        return pulumi.get(self, "kms_key_name")


@pulumi.output_type
class GoogleCloudMlV1__EnvVarResponse(dict):
    """
    Represents an environment variable to be made available in a container. This message is a subset of the [Kubernetes EnvVar v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#envvar-v1-core).
    """
    def __init__(__self__, *,
                 name: str,
                 value: str):
        """
        Represents an environment variable to be made available in a container. This message is a subset of the [Kubernetes EnvVar v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#envvar-v1-core).
        :param str name: Name of the environment variable. Must be a [valid C identifier](https://github.com/kubernetes/kubernetes/blob/v1.18.8/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go#L258) and must not begin with the prefix `AIP_`.
        :param str value: Value of the environment variable. Defaults to an empty string. In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set earlier in the same env field as where this message occurs. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $(VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME)
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Name of the environment variable. Must be a [valid C identifier](https://github.com/kubernetes/kubernetes/blob/v1.18.8/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go#L258) and must not begin with the prefix `AIP_`.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def value(self) -> str:
        """
        Value of the environment variable. Defaults to an empty string. In this field, you can reference [environment variables set by AI Platform Prediction](/ai-platform/prediction/docs/custom-container-requirements#aip-variables) and environment variables set earlier in the same env field as where this message occurs. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $(VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME)
        """
        return pulumi.get(self, "value")


@pulumi.output_type
class GoogleCloudMlV1__ExplanationConfigResponse(dict):
    """
    Message holding configuration options for explaining model predictions. There are three feature attribution methods supported for TensorFlow models: integrated gradients, sampled Shapley, and XRAI. [Learn more about feature attributions.](/ai-platform/prediction/docs/ai-explanations/overview)
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "integratedGradientsAttribution":
            suggest = "integrated_gradients_attribution"
        elif key == "sampledShapleyAttribution":
            suggest = "sampled_shapley_attribution"
        elif key == "xraiAttribution":
            suggest = "xrai_attribution"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__ExplanationConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__ExplanationConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__ExplanationConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 integrated_gradients_attribution: 'outputs.GoogleCloudMlV1__IntegratedGradientsAttributionResponse',
                 sampled_shapley_attribution: 'outputs.GoogleCloudMlV1__SampledShapleyAttributionResponse',
                 xrai_attribution: 'outputs.GoogleCloudMlV1__XraiAttributionResponse'):
        """
        Message holding configuration options for explaining model predictions. There are three feature attribution methods supported for TensorFlow models: integrated gradients, sampled Shapley, and XRAI. [Learn more about feature attributions.](/ai-platform/prediction/docs/ai-explanations/overview)
        :param 'GoogleCloudMlV1__IntegratedGradientsAttributionResponse' integrated_gradients_attribution: Attributes credit by computing the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param 'GoogleCloudMlV1__SampledShapleyAttributionResponse' sampled_shapley_attribution: An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
        :param 'GoogleCloudMlV1__XraiAttributionResponse' xrai_attribution: Attributes credit by computing the XRAI taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Currently only implemented for models with natural image inputs.
        """
        pulumi.set(__self__, "integrated_gradients_attribution", integrated_gradients_attribution)
        pulumi.set(__self__, "sampled_shapley_attribution", sampled_shapley_attribution)
        pulumi.set(__self__, "xrai_attribution", xrai_attribution)

    @property
    @pulumi.getter(name="integratedGradientsAttribution")
    def integrated_gradients_attribution(self) -> 'outputs.GoogleCloudMlV1__IntegratedGradientsAttributionResponse':
        """
        Attributes credit by computing the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        """
        return pulumi.get(self, "integrated_gradients_attribution")

    @property
    @pulumi.getter(name="sampledShapleyAttribution")
    def sampled_shapley_attribution(self) -> 'outputs.GoogleCloudMlV1__SampledShapleyAttributionResponse':
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
        """
        return pulumi.get(self, "sampled_shapley_attribution")

    @property
    @pulumi.getter(name="xraiAttribution")
    def xrai_attribution(self) -> 'outputs.GoogleCloudMlV1__XraiAttributionResponse':
        """
        Attributes credit by computing the XRAI taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Currently only implemented for models with natural image inputs.
        """
        return pulumi.get(self, "xrai_attribution")


@pulumi.output_type
class GoogleCloudMlV1__HyperparameterOutputResponse(dict):
    """
    Represents the result of a single hyperparameter tuning trial from a training job. The TrainingOutput object that is returned on successful completion of a training job with hyperparameter tuning includes a list of HyperparameterOutput objects, one for each successful trial.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "allMetrics":
            suggest = "all_metrics"
        elif key == "builtInAlgorithmOutput":
            suggest = "built_in_algorithm_output"
        elif key == "endTime":
            suggest = "end_time"
        elif key == "finalMetric":
            suggest = "final_metric"
        elif key == "isTrialStoppedEarly":
            suggest = "is_trial_stopped_early"
        elif key == "startTime":
            suggest = "start_time"
        elif key == "trialId":
            suggest = "trial_id"
        elif key == "webAccessUris":
            suggest = "web_access_uris"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__HyperparameterOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__HyperparameterOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__HyperparameterOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 all_metrics: Sequence['outputs.GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse'],
                 built_in_algorithm_output: 'outputs.GoogleCloudMlV1__BuiltInAlgorithmOutputResponse',
                 end_time: str,
                 final_metric: 'outputs.GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse',
                 hyperparameters: Mapping[str, str],
                 is_trial_stopped_early: bool,
                 start_time: str,
                 state: str,
                 trial_id: str,
                 web_access_uris: Mapping[str, str]):
        """
        Represents the result of a single hyperparameter tuning trial from a training job. The TrainingOutput object that is returned on successful completion of a training job with hyperparameter tuning includes a list of HyperparameterOutput objects, one for each successful trial.
        :param Sequence['GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse'] all_metrics: All recorded object metrics for this trial. This field is not currently populated.
        :param 'GoogleCloudMlV1__BuiltInAlgorithmOutputResponse' built_in_algorithm_output: Details related to built-in algorithms jobs. Only set for trials of built-in algorithms jobs that have succeeded.
        :param str end_time: End time for the trial.
        :param 'GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse' final_metric: The final objective metric seen for this trial.
        :param Mapping[str, str] hyperparameters: The hyperparameters given to this trial.
        :param bool is_trial_stopped_early: True if the trial is stopped early.
        :param str start_time: Start time for the trial.
        :param str state: The detailed state of the trial.
        :param str trial_id: The trial id for these results.
        :param Mapping[str, str] web_access_uris: URIs for accessing [interactive shells](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) (one URI for each training node). Only available if this trial is part of a hyperparameter tuning job and the job's training_input.enable_web_access is `true`. The keys are names of each node in the training job; for example, `master-replica-0` for the master node, `worker-replica-0` for the first worker, and `ps-replica-0` for the first parameter server. The values are the URIs for each node's interactive shell.
        """
        pulumi.set(__self__, "all_metrics", all_metrics)
        pulumi.set(__self__, "built_in_algorithm_output", built_in_algorithm_output)
        pulumi.set(__self__, "end_time", end_time)
        pulumi.set(__self__, "final_metric", final_metric)
        pulumi.set(__self__, "hyperparameters", hyperparameters)
        pulumi.set(__self__, "is_trial_stopped_early", is_trial_stopped_early)
        pulumi.set(__self__, "start_time", start_time)
        pulumi.set(__self__, "state", state)
        pulumi.set(__self__, "trial_id", trial_id)
        pulumi.set(__self__, "web_access_uris", web_access_uris)

    @property
    @pulumi.getter(name="allMetrics")
    def all_metrics(self) -> Sequence['outputs.GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse']:
        """
        All recorded object metrics for this trial. This field is not currently populated.
        """
        return pulumi.get(self, "all_metrics")

    @property
    @pulumi.getter(name="builtInAlgorithmOutput")
    def built_in_algorithm_output(self) -> 'outputs.GoogleCloudMlV1__BuiltInAlgorithmOutputResponse':
        """
        Details related to built-in algorithms jobs. Only set for trials of built-in algorithms jobs that have succeeded.
        """
        return pulumi.get(self, "built_in_algorithm_output")

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> str:
        """
        End time for the trial.
        """
        return pulumi.get(self, "end_time")

    @property
    @pulumi.getter(name="finalMetric")
    def final_metric(self) -> 'outputs.GoogleCloudMlV1_HyperparameterOutput_HyperparameterMetricResponse':
        """
        The final objective metric seen for this trial.
        """
        return pulumi.get(self, "final_metric")

    @property
    @pulumi.getter
    def hyperparameters(self) -> Mapping[str, str]:
        """
        The hyperparameters given to this trial.
        """
        return pulumi.get(self, "hyperparameters")

    @property
    @pulumi.getter(name="isTrialStoppedEarly")
    def is_trial_stopped_early(self) -> bool:
        """
        True if the trial is stopped early.
        """
        return pulumi.get(self, "is_trial_stopped_early")

    @property
    @pulumi.getter(name="startTime")
    def start_time(self) -> str:
        """
        Start time for the trial.
        """
        return pulumi.get(self, "start_time")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The detailed state of the trial.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="trialId")
    def trial_id(self) -> str:
        """
        The trial id for these results.
        """
        return pulumi.get(self, "trial_id")

    @property
    @pulumi.getter(name="webAccessUris")
    def web_access_uris(self) -> Mapping[str, str]:
        """
        URIs for accessing [interactive shells](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) (one URI for each training node). Only available if this trial is part of a hyperparameter tuning job and the job's training_input.enable_web_access is `true`. The keys are names of each node in the training job; for example, `master-replica-0` for the master node, `worker-replica-0` for the first worker, and `ps-replica-0` for the first parameter server. The values are the URIs for each node's interactive shell.
        """
        return pulumi.get(self, "web_access_uris")


@pulumi.output_type
class GoogleCloudMlV1__HyperparameterSpecResponse(dict):
    """
    Represents a set of hyperparameters to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableTrialEarlyStopping":
            suggest = "enable_trial_early_stopping"
        elif key == "hyperparameterMetricTag":
            suggest = "hyperparameter_metric_tag"
        elif key == "maxFailedTrials":
            suggest = "max_failed_trials"
        elif key == "maxParallelTrials":
            suggest = "max_parallel_trials"
        elif key == "maxTrials":
            suggest = "max_trials"
        elif key == "resumePreviousJobId":
            suggest = "resume_previous_job_id"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__HyperparameterSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__HyperparameterSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__HyperparameterSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 algorithm: str,
                 enable_trial_early_stopping: bool,
                 goal: str,
                 hyperparameter_metric_tag: str,
                 max_failed_trials: int,
                 max_parallel_trials: int,
                 max_trials: int,
                 params: Sequence['outputs.GoogleCloudMlV1__ParameterSpecResponse'],
                 resume_previous_job_id: str):
        """
        Represents a set of hyperparameters to optimize.
        :param str algorithm: Optional. The search algorithm specified for the hyperparameter tuning job. Uses the default AI Platform hyperparameter tuning algorithm if unspecified.
        :param bool enable_trial_early_stopping: Optional. Indicates if the hyperparameter tuning job enables auto trial early stopping.
        :param str goal: The type of goal to use for tuning. Available types are `MAXIMIZE` and `MINIMIZE`. Defaults to `MAXIMIZE`.
        :param str hyperparameter_metric_tag: Optional. The TensorFlow summary tag name to use for optimizing trials. For current versions of TensorFlow, this tag name should exactly match what is shown in TensorBoard, including all scopes. For versions of TensorFlow prior to 0.12, this should be only the tag passed to tf.Summary. By default, "training/hptuning/metric" will be used.
        :param int max_failed_trials: Optional. The number of failed trials that need to be seen before failing the hyperparameter tuning job. You can specify this field to override the default failing criteria for AI Platform hyperparameter tuning jobs. Defaults to zero, which means the service decides when a hyperparameter job should fail.
        :param int max_parallel_trials: Optional. The number of training trials to run concurrently. You can reduce the time it takes to perform hyperparameter tuning by adding trials in parallel. However, each trail only benefits from the information gained in completed trials. That means that a trial does not get access to the results of trials running at the same time, which could reduce the quality of the overall optimization. Each trial will use the same scale tier and machine types. Defaults to one.
        :param int max_trials: Optional. How many training trials should be attempted to optimize the specified hyperparameters. Defaults to one.
        :param Sequence['GoogleCloudMlV1__ParameterSpecResponse'] params: The set of parameters to tune.
        :param str resume_previous_job_id: Optional. The prior hyperparameter tuning job id that users hope to continue with. The job id will be used to find the corresponding vizier study guid and resume the study.
        """
        pulumi.set(__self__, "algorithm", algorithm)
        pulumi.set(__self__, "enable_trial_early_stopping", enable_trial_early_stopping)
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "hyperparameter_metric_tag", hyperparameter_metric_tag)
        pulumi.set(__self__, "max_failed_trials", max_failed_trials)
        pulumi.set(__self__, "max_parallel_trials", max_parallel_trials)
        pulumi.set(__self__, "max_trials", max_trials)
        pulumi.set(__self__, "params", params)
        pulumi.set(__self__, "resume_previous_job_id", resume_previous_job_id)

    @property
    @pulumi.getter
    def algorithm(self) -> str:
        """
        Optional. The search algorithm specified for the hyperparameter tuning job. Uses the default AI Platform hyperparameter tuning algorithm if unspecified.
        """
        return pulumi.get(self, "algorithm")

    @property
    @pulumi.getter(name="enableTrialEarlyStopping")
    def enable_trial_early_stopping(self) -> bool:
        """
        Optional. Indicates if the hyperparameter tuning job enables auto trial early stopping.
        """
        return pulumi.get(self, "enable_trial_early_stopping")

    @property
    @pulumi.getter
    def goal(self) -> str:
        """
        The type of goal to use for tuning. Available types are `MAXIMIZE` and `MINIMIZE`. Defaults to `MAXIMIZE`.
        """
        return pulumi.get(self, "goal")

    @property
    @pulumi.getter(name="hyperparameterMetricTag")
    def hyperparameter_metric_tag(self) -> str:
        """
        Optional. The TensorFlow summary tag name to use for optimizing trials. For current versions of TensorFlow, this tag name should exactly match what is shown in TensorBoard, including all scopes. For versions of TensorFlow prior to 0.12, this should be only the tag passed to tf.Summary. By default, "training/hptuning/metric" will be used.
        """
        return pulumi.get(self, "hyperparameter_metric_tag")

    @property
    @pulumi.getter(name="maxFailedTrials")
    def max_failed_trials(self) -> int:
        """
        Optional. The number of failed trials that need to be seen before failing the hyperparameter tuning job. You can specify this field to override the default failing criteria for AI Platform hyperparameter tuning jobs. Defaults to zero, which means the service decides when a hyperparameter job should fail.
        """
        return pulumi.get(self, "max_failed_trials")

    @property
    @pulumi.getter(name="maxParallelTrials")
    def max_parallel_trials(self) -> int:
        """
        Optional. The number of training trials to run concurrently. You can reduce the time it takes to perform hyperparameter tuning by adding trials in parallel. However, each trail only benefits from the information gained in completed trials. That means that a trial does not get access to the results of trials running at the same time, which could reduce the quality of the overall optimization. Each trial will use the same scale tier and machine types. Defaults to one.
        """
        return pulumi.get(self, "max_parallel_trials")

    @property
    @pulumi.getter(name="maxTrials")
    def max_trials(self) -> int:
        """
        Optional. How many training trials should be attempted to optimize the specified hyperparameters. Defaults to one.
        """
        return pulumi.get(self, "max_trials")

    @property
    @pulumi.getter
    def params(self) -> Sequence['outputs.GoogleCloudMlV1__ParameterSpecResponse']:
        """
        The set of parameters to tune.
        """
        return pulumi.get(self, "params")

    @property
    @pulumi.getter(name="resumePreviousJobId")
    def resume_previous_job_id(self) -> str:
        """
        Optional. The prior hyperparameter tuning job id that users hope to continue with. The job id will be used to find the corresponding vizier study guid and resume the study.
        """
        return pulumi.get(self, "resume_previous_job_id")


@pulumi.output_type
class GoogleCloudMlV1__IntegratedGradientsAttributionResponse(dict):
    """
    Attributes credit by computing the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "numIntegralSteps":
            suggest = "num_integral_steps"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__IntegratedGradientsAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__IntegratedGradientsAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__IntegratedGradientsAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 num_integral_steps: int):
        """
        Attributes credit by computing the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param int num_integral_steps: Number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range.
        """
        pulumi.set(__self__, "num_integral_steps", num_integral_steps)

    @property
    @pulumi.getter(name="numIntegralSteps")
    def num_integral_steps(self) -> int:
        """
        Number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range.
        """
        return pulumi.get(self, "num_integral_steps")


@pulumi.output_type
class GoogleCloudMlV1__ManualScalingResponse(dict):
    """
    Options for manually scaling a model.
    """
    def __init__(__self__, *,
                 nodes: int):
        """
        Options for manually scaling a model.
        :param int nodes: The number of nodes to allocate for this model. These nodes are always up, starting from the time the model is deployed, so the cost of operating this model will be proportional to `nodes` * number of hours since last billing cycle plus the cost for each prediction performed.
        """
        pulumi.set(__self__, "nodes", nodes)

    @property
    @pulumi.getter
    def nodes(self) -> int:
        """
        The number of nodes to allocate for this model. These nodes are always up, starting from the time the model is deployed, so the cost of operating this model will be proportional to `nodes` * number of hours since last billing cycle plus the cost for each prediction performed.
        """
        return pulumi.get(self, "nodes")


@pulumi.output_type
class GoogleCloudMlV1__MeasurementResponse(dict):
    """
    A message representing a measurement.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "elapsedTime":
            suggest = "elapsed_time"
        elif key == "stepCount":
            suggest = "step_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__MeasurementResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__MeasurementResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__MeasurementResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 elapsed_time: str,
                 metrics: Sequence['outputs.GoogleCloudMlV1_Measurement_MetricResponse'],
                 step_count: str):
        """
        A message representing a measurement.
        :param str elapsed_time: Time that the trial has been running at the point of this measurement.
        :param Sequence['GoogleCloudMlV1_Measurement_MetricResponse'] metrics: Provides a list of metrics that act as inputs into the objective function.
        :param str step_count: The number of steps a machine learning model has been trained for. Must be non-negative.
        """
        pulumi.set(__self__, "elapsed_time", elapsed_time)
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "step_count", step_count)

    @property
    @pulumi.getter(name="elapsedTime")
    def elapsed_time(self) -> str:
        """
        Time that the trial has been running at the point of this measurement.
        """
        return pulumi.get(self, "elapsed_time")

    @property
    @pulumi.getter
    def metrics(self) -> Sequence['outputs.GoogleCloudMlV1_Measurement_MetricResponse']:
        """
        Provides a list of metrics that act as inputs into the objective function.
        """
        return pulumi.get(self, "metrics")

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> str:
        """
        The number of steps a machine learning model has been trained for. Must be non-negative.
        """
        return pulumi.get(self, "step_count")


@pulumi.output_type
class GoogleCloudMlV1__MetricSpecResponse(dict):
    """
    MetricSpec contains the specifications to use to calculate the desired nodes count when autoscaling is enabled.
    """
    def __init__(__self__, *,
                 name: str,
                 target: int):
        """
        MetricSpec contains the specifications to use to calculate the desired nodes count when autoscaling is enabled.
        :param str name: metric name.
        :param int target: Target specifies the target value for the given metric; once real metric deviates from the threshold by a certain percentage, the node count changes.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "target", target)

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        metric name.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter
    def target(self) -> int:
        """
        Target specifies the target value for the given metric; once real metric deviates from the threshold by a certain percentage, the node count changes.
        """
        return pulumi.get(self, "target")


@pulumi.output_type
class GoogleCloudMlV1__ParameterSpecResponse(dict):
    """
    Represents a single hyperparameter to optimize.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "categoricalValues":
            suggest = "categorical_values"
        elif key == "discreteValues":
            suggest = "discrete_values"
        elif key == "maxValue":
            suggest = "max_value"
        elif key == "minValue":
            suggest = "min_value"
        elif key == "parameterName":
            suggest = "parameter_name"
        elif key == "scaleType":
            suggest = "scale_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__ParameterSpecResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__ParameterSpecResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__ParameterSpecResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 categorical_values: Sequence[str],
                 discrete_values: Sequence[float],
                 max_value: float,
                 min_value: float,
                 parameter_name: str,
                 scale_type: str,
                 type: str):
        """
        Represents a single hyperparameter to optimize.
        :param Sequence[str] categorical_values: Required if type is `CATEGORICAL`. The list of possible categories.
        :param Sequence[float] discrete_values: Required if type is `DISCRETE`. A list of feasible points. The list should be in strictly increasing order. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        :param float max_value: Required if type is `DOUBLE` or `INTEGER`. This field should be unset if type is `CATEGORICAL`. This value should be integers if type is `INTEGER`.
        :param float min_value: Required if type is `DOUBLE` or `INTEGER`. This field should be unset if type is `CATEGORICAL`. This value should be integers if type is INTEGER.
        :param str parameter_name: The parameter name must be unique amongst all ParameterConfigs in a HyperparameterSpec message. E.g., "learning_rate".
        :param str scale_type: Optional. How the parameter should be scaled to the hypercube. Leave unset for categorical parameters. Some kind of scaling is strongly recommended for real or integral parameters (e.g., `UNIT_LINEAR_SCALE`).
        :param str type: The type of the parameter.
        """
        pulumi.set(__self__, "categorical_values", categorical_values)
        pulumi.set(__self__, "discrete_values", discrete_values)
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)
        pulumi.set(__self__, "parameter_name", parameter_name)
        pulumi.set(__self__, "scale_type", scale_type)
        pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="categoricalValues")
    def categorical_values(self) -> Sequence[str]:
        """
        Required if type is `CATEGORICAL`. The list of possible categories.
        """
        return pulumi.get(self, "categorical_values")

    @property
    @pulumi.getter(name="discreteValues")
    def discrete_values(self) -> Sequence[float]:
        """
        Required if type is `DISCRETE`. A list of feasible points. The list should be in strictly increasing order. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        return pulumi.get(self, "discrete_values")

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> float:
        """
        Required if type is `DOUBLE` or `INTEGER`. This field should be unset if type is `CATEGORICAL`. This value should be integers if type is `INTEGER`.
        """
        return pulumi.get(self, "max_value")

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> float:
        """
        Required if type is `DOUBLE` or `INTEGER`. This field should be unset if type is `CATEGORICAL`. This value should be integers if type is INTEGER.
        """
        return pulumi.get(self, "min_value")

    @property
    @pulumi.getter(name="parameterName")
    def parameter_name(self) -> str:
        """
        The parameter name must be unique amongst all ParameterConfigs in a HyperparameterSpec message. E.g., "learning_rate".
        """
        return pulumi.get(self, "parameter_name")

    @property
    @pulumi.getter(name="scaleType")
    def scale_type(self) -> str:
        """
        Optional. How the parameter should be scaled to the hypercube. Leave unset for categorical parameters. Some kind of scaling is strongly recommended for real or integral parameters (e.g., `UNIT_LINEAR_SCALE`).
        """
        return pulumi.get(self, "scale_type")

    @property
    @pulumi.getter
    def type(self) -> str:
        """
        The type of the parameter.
        """
        return pulumi.get(self, "type")


@pulumi.output_type
class GoogleCloudMlV1__PredictionInputResponse(dict):
    """
    Represents input parameters for a prediction job.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "batchSize":
            suggest = "batch_size"
        elif key == "dataFormat":
            suggest = "data_format"
        elif key == "inputPaths":
            suggest = "input_paths"
        elif key == "maxWorkerCount":
            suggest = "max_worker_count"
        elif key == "modelName":
            suggest = "model_name"
        elif key == "outputDataFormat":
            suggest = "output_data_format"
        elif key == "outputPath":
            suggest = "output_path"
        elif key == "runtimeVersion":
            suggest = "runtime_version"
        elif key == "signatureName":
            suggest = "signature_name"
        elif key == "versionName":
            suggest = "version_name"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__PredictionInputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__PredictionInputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__PredictionInputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 batch_size: str,
                 data_format: str,
                 input_paths: Sequence[str],
                 max_worker_count: str,
                 model_name: str,
                 output_data_format: str,
                 output_path: str,
                 region: str,
                 runtime_version: str,
                 signature_name: str,
                 uri: str,
                 version_name: str):
        """
        Represents input parameters for a prediction job.
        :param str batch_size: Optional. Number of records per batch, defaults to 64. The service will buffer batch_size number of records in memory before invoking one Tensorflow prediction call internally. So take the record size and memory available into consideration when setting this parameter.
        :param str data_format: The format of the input data files.
        :param Sequence[str] input_paths: The Cloud Storage location of the input data files. May contain wildcards.
        :param str max_worker_count: Optional. The maximum number of workers to be used for parallel processing. Defaults to 10 if not specified.
        :param str model_name: Use this field if you want to use the default version for the specified model. The string must use the following format: `"projects/YOUR_PROJECT/models/YOUR_MODEL"`
        :param str output_data_format: Optional. Format of the output data files, defaults to JSON.
        :param str output_path: The output Google Cloud Storage location.
        :param str region: The Google Compute Engine region to run the prediction job in. See the available regions for AI Platform services.
        :param str runtime_version: Optional. The AI Platform runtime version to use for this batch prediction. If not set, AI Platform will pick the runtime version used during the CreateVersion request for this model version, or choose the latest stable version when model version information is not available such as when the model is specified by uri.
        :param str signature_name: Optional. The name of the signature defined in the SavedModel to use for this job. Please refer to [SavedModel](https://tensorflow.github.io/serving/serving_basic.html) for information about how to use signatures. Defaults to [DEFAULT_SERVING_SIGNATURE_DEF_KEY](https://www.tensorflow.org/api_docs/python/tf/saved_model/signature_constants) , which is "serving_default".
        :param str uri: Use this field if you want to specify a Google Cloud Storage path for the model to use.
        :param str version_name: Use this field if you want to specify a version of the model to use. The string is formatted the same way as `model_version`, with the addition of the version information: `"projects/YOUR_PROJECT/models/YOUR_MODEL/versions/YOUR_VERSION"`
        """
        pulumi.set(__self__, "batch_size", batch_size)
        pulumi.set(__self__, "data_format", data_format)
        pulumi.set(__self__, "input_paths", input_paths)
        pulumi.set(__self__, "max_worker_count", max_worker_count)
        pulumi.set(__self__, "model_name", model_name)
        pulumi.set(__self__, "output_data_format", output_data_format)
        pulumi.set(__self__, "output_path", output_path)
        pulumi.set(__self__, "region", region)
        pulumi.set(__self__, "runtime_version", runtime_version)
        pulumi.set(__self__, "signature_name", signature_name)
        pulumi.set(__self__, "uri", uri)
        pulumi.set(__self__, "version_name", version_name)

    @property
    @pulumi.getter(name="batchSize")
    def batch_size(self) -> str:
        """
        Optional. Number of records per batch, defaults to 64. The service will buffer batch_size number of records in memory before invoking one Tensorflow prediction call internally. So take the record size and memory available into consideration when setting this parameter.
        """
        return pulumi.get(self, "batch_size")

    @property
    @pulumi.getter(name="dataFormat")
    def data_format(self) -> str:
        """
        The format of the input data files.
        """
        return pulumi.get(self, "data_format")

    @property
    @pulumi.getter(name="inputPaths")
    def input_paths(self) -> Sequence[str]:
        """
        The Cloud Storage location of the input data files. May contain wildcards.
        """
        return pulumi.get(self, "input_paths")

    @property
    @pulumi.getter(name="maxWorkerCount")
    def max_worker_count(self) -> str:
        """
        Optional. The maximum number of workers to be used for parallel processing. Defaults to 10 if not specified.
        """
        return pulumi.get(self, "max_worker_count")

    @property
    @pulumi.getter(name="modelName")
    def model_name(self) -> str:
        """
        Use this field if you want to use the default version for the specified model. The string must use the following format: `"projects/YOUR_PROJECT/models/YOUR_MODEL"`
        """
        return pulumi.get(self, "model_name")

    @property
    @pulumi.getter(name="outputDataFormat")
    def output_data_format(self) -> str:
        """
        Optional. Format of the output data files, defaults to JSON.
        """
        return pulumi.get(self, "output_data_format")

    @property
    @pulumi.getter(name="outputPath")
    def output_path(self) -> str:
        """
        The output Google Cloud Storage location.
        """
        return pulumi.get(self, "output_path")

    @property
    @pulumi.getter
    def region(self) -> str:
        """
        The Google Compute Engine region to run the prediction job in. See the available regions for AI Platform services.
        """
        return pulumi.get(self, "region")

    @property
    @pulumi.getter(name="runtimeVersion")
    def runtime_version(self) -> str:
        """
        Optional. The AI Platform runtime version to use for this batch prediction. If not set, AI Platform will pick the runtime version used during the CreateVersion request for this model version, or choose the latest stable version when model version information is not available such as when the model is specified by uri.
        """
        return pulumi.get(self, "runtime_version")

    @property
    @pulumi.getter(name="signatureName")
    def signature_name(self) -> str:
        """
        Optional. The name of the signature defined in the SavedModel to use for this job. Please refer to [SavedModel](https://tensorflow.github.io/serving/serving_basic.html) for information about how to use signatures. Defaults to [DEFAULT_SERVING_SIGNATURE_DEF_KEY](https://www.tensorflow.org/api_docs/python/tf/saved_model/signature_constants) , which is "serving_default".
        """
        return pulumi.get(self, "signature_name")

    @property
    @pulumi.getter
    def uri(self) -> str:
        """
        Use this field if you want to specify a Google Cloud Storage path for the model to use.
        """
        return pulumi.get(self, "uri")

    @property
    @pulumi.getter(name="versionName")
    def version_name(self) -> str:
        """
        Use this field if you want to specify a version of the model to use. The string is formatted the same way as `model_version`, with the addition of the version information: `"projects/YOUR_PROJECT/models/YOUR_MODEL/versions/YOUR_VERSION"`
        """
        return pulumi.get(self, "version_name")


@pulumi.output_type
class GoogleCloudMlV1__PredictionOutputResponse(dict):
    """
    Represents results of a prediction job.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "errorCount":
            suggest = "error_count"
        elif key == "nodeHours":
            suggest = "node_hours"
        elif key == "outputPath":
            suggest = "output_path"
        elif key == "predictionCount":
            suggest = "prediction_count"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__PredictionOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__PredictionOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__PredictionOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 error_count: str,
                 node_hours: float,
                 output_path: str,
                 prediction_count: str):
        """
        Represents results of a prediction job.
        :param str error_count: The number of data instances which resulted in errors.
        :param float node_hours: Node hours used by the batch prediction job.
        :param str output_path: The output Google Cloud Storage location provided at the job creation time.
        :param str prediction_count: The number of generated predictions.
        """
        pulumi.set(__self__, "error_count", error_count)
        pulumi.set(__self__, "node_hours", node_hours)
        pulumi.set(__self__, "output_path", output_path)
        pulumi.set(__self__, "prediction_count", prediction_count)

    @property
    @pulumi.getter(name="errorCount")
    def error_count(self) -> str:
        """
        The number of data instances which resulted in errors.
        """
        return pulumi.get(self, "error_count")

    @property
    @pulumi.getter(name="nodeHours")
    def node_hours(self) -> float:
        """
        Node hours used by the batch prediction job.
        """
        return pulumi.get(self, "node_hours")

    @property
    @pulumi.getter(name="outputPath")
    def output_path(self) -> str:
        """
        The output Google Cloud Storage location provided at the job creation time.
        """
        return pulumi.get(self, "output_path")

    @property
    @pulumi.getter(name="predictionCount")
    def prediction_count(self) -> str:
        """
        The number of generated predictions.
        """
        return pulumi.get(self, "prediction_count")


@pulumi.output_type
class GoogleCloudMlV1__ReplicaConfigResponse(dict):
    """
    Represents the configuration for a replica in a cluster.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorConfig":
            suggest = "accelerator_config"
        elif key == "containerArgs":
            suggest = "container_args"
        elif key == "containerCommand":
            suggest = "container_command"
        elif key == "diskConfig":
            suggest = "disk_config"
        elif key == "imageUri":
            suggest = "image_uri"
        elif key == "tpuTfVersion":
            suggest = "tpu_tf_version"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__ReplicaConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__ReplicaConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__ReplicaConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_config: 'outputs.GoogleCloudMlV1__AcceleratorConfigResponse',
                 container_args: Sequence[str],
                 container_command: Sequence[str],
                 disk_config: 'outputs.GoogleCloudMlV1__DiskConfigResponse',
                 image_uri: str,
                 tpu_tf_version: str):
        """
        Represents the configuration for a replica in a cluster.
        :param 'GoogleCloudMlV1__AcceleratorConfigResponse' accelerator_config: Represents the type and number of accelerators used by the replica. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu)
        :param Sequence[str] container_args: Arguments to the entrypoint command. The following rules apply for container_command and container_args: - If you do not supply command or args: The defaults defined in the Docker image are used. - If you supply a command but no args: The default EntryPoint and the default Cmd defined in the Docker image are ignored. Your command is run without any arguments. - If you supply only args: The default Entrypoint defined in the Docker image is run with the args that you supplied. - If you supply a command and args: The default Entrypoint and the default Cmd defined in the Docker image are ignored. Your command is run with your args. It cannot be set if custom container image is not provided. Note that this field and [TrainingInput.args] are mutually exclusive, i.e., both cannot be set at the same time.
        :param Sequence[str] container_command: The command with which the replica's custom container is run. If provided, it will override default ENTRYPOINT of the docker image. If not provided, the docker image's ENTRYPOINT is used. It cannot be set if custom container image is not provided. Note that this field and [TrainingInput.args] are mutually exclusive, i.e., both cannot be set at the same time.
        :param 'GoogleCloudMlV1__DiskConfigResponse' disk_config: Represents the configuration of disk options.
        :param str image_uri: The Docker image to run on the replica. This image must be in Container Registry. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        :param str tpu_tf_version: The AI Platform runtime version that includes a TensorFlow version matching the one used in the custom container. This field is required if the replica is a TPU worker that uses a custom container. Otherwise, do not specify this field. This must be a [runtime version that currently supports training with TPUs](/ml-engine/docs/tensorflow/runtime-version-list#tpu-support). Note that the version of TensorFlow included in a runtime version may differ from the numbering of the runtime version itself, because it may have a different [patch version](https://www.tensorflow.org/guide/version_compat#semantic_versioning_20). In this field, you must specify the runtime version (TensorFlow minor version). For example, if your custom container runs TensorFlow `1.x.y`, specify `1.x`.
        """
        pulumi.set(__self__, "accelerator_config", accelerator_config)
        pulumi.set(__self__, "container_args", container_args)
        pulumi.set(__self__, "container_command", container_command)
        pulumi.set(__self__, "disk_config", disk_config)
        pulumi.set(__self__, "image_uri", image_uri)
        pulumi.set(__self__, "tpu_tf_version", tpu_tf_version)

    @property
    @pulumi.getter(name="acceleratorConfig")
    def accelerator_config(self) -> 'outputs.GoogleCloudMlV1__AcceleratorConfigResponse':
        """
        Represents the type and number of accelerators used by the replica. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu)
        """
        return pulumi.get(self, "accelerator_config")

    @property
    @pulumi.getter(name="containerArgs")
    def container_args(self) -> Sequence[str]:
        """
        Arguments to the entrypoint command. The following rules apply for container_command and container_args: - If you do not supply command or args: The defaults defined in the Docker image are used. - If you supply a command but no args: The default EntryPoint and the default Cmd defined in the Docker image are ignored. Your command is run without any arguments. - If you supply only args: The default Entrypoint defined in the Docker image is run with the args that you supplied. - If you supply a command and args: The default Entrypoint and the default Cmd defined in the Docker image are ignored. Your command is run with your args. It cannot be set if custom container image is not provided. Note that this field and [TrainingInput.args] are mutually exclusive, i.e., both cannot be set at the same time.
        """
        return pulumi.get(self, "container_args")

    @property
    @pulumi.getter(name="containerCommand")
    def container_command(self) -> Sequence[str]:
        """
        The command with which the replica's custom container is run. If provided, it will override default ENTRYPOINT of the docker image. If not provided, the docker image's ENTRYPOINT is used. It cannot be set if custom container image is not provided. Note that this field and [TrainingInput.args] are mutually exclusive, i.e., both cannot be set at the same time.
        """
        return pulumi.get(self, "container_command")

    @property
    @pulumi.getter(name="diskConfig")
    def disk_config(self) -> 'outputs.GoogleCloudMlV1__DiskConfigResponse':
        """
        Represents the configuration of disk options.
        """
        return pulumi.get(self, "disk_config")

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> str:
        """
        The Docker image to run on the replica. This image must be in Container Registry. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        """
        return pulumi.get(self, "image_uri")

    @property
    @pulumi.getter(name="tpuTfVersion")
    def tpu_tf_version(self) -> str:
        """
        The AI Platform runtime version that includes a TensorFlow version matching the one used in the custom container. This field is required if the replica is a TPU worker that uses a custom container. Otherwise, do not specify this field. This must be a [runtime version that currently supports training with TPUs](/ml-engine/docs/tensorflow/runtime-version-list#tpu-support). Note that the version of TensorFlow included in a runtime version may differ from the numbering of the runtime version itself, because it may have a different [patch version](https://www.tensorflow.org/guide/version_compat#semantic_versioning_20). In this field, you must specify the runtime version (TensorFlow minor version). For example, if your custom container runs TensorFlow `1.x.y`, specify `1.x`.
        """
        return pulumi.get(self, "tpu_tf_version")


@pulumi.output_type
class GoogleCloudMlV1__RequestLoggingConfigResponse(dict):
    """
    Configuration for logging request-response pairs to a BigQuery table. Online prediction requests to a model version and the responses to these requests are converted to raw strings and saved to the specified BigQuery table. Logging is constrained by [BigQuery quotas and limits](/bigquery/quotas). If your project exceeds BigQuery quotas or limits, AI Platform Prediction does not log request-response pairs, but it continues to serve predictions. If you are using [continuous evaluation](/ml-engine/docs/continuous-evaluation/), you do not need to specify this configuration manually. Setting up continuous evaluation automatically enables logging of request-response pairs.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "bigqueryTableName":
            suggest = "bigquery_table_name"
        elif key == "samplingPercentage":
            suggest = "sampling_percentage"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__RequestLoggingConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__RequestLoggingConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__RequestLoggingConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 bigquery_table_name: str,
                 sampling_percentage: float):
        """
        Configuration for logging request-response pairs to a BigQuery table. Online prediction requests to a model version and the responses to these requests are converted to raw strings and saved to the specified BigQuery table. Logging is constrained by [BigQuery quotas and limits](/bigquery/quotas). If your project exceeds BigQuery quotas or limits, AI Platform Prediction does not log request-response pairs, but it continues to serve predictions. If you are using [continuous evaluation](/ml-engine/docs/continuous-evaluation/), you do not need to specify this configuration manually. Setting up continuous evaluation automatically enables logging of request-response pairs.
        :param str bigquery_table_name: Fully qualified BigQuery table name in the following format: " project_id.dataset_name.table_name" The specified table must already exist, and the "Cloud ML Service Agent" for your project must have permission to write to it. The table must have the following [schema](/bigquery/docs/schemas): Field nameType Mode model STRING REQUIRED model_version STRING REQUIRED time TIMESTAMP REQUIRED raw_data STRING REQUIRED raw_prediction STRING NULLABLE groundtruth STRING NULLABLE 
        :param float sampling_percentage: Percentage of requests to be logged, expressed as a fraction from 0 to 1. For example, if you want to log 10% of requests, enter `0.1`. The sampling window is the lifetime of the model version. Defaults to 0.
        """
        pulumi.set(__self__, "bigquery_table_name", bigquery_table_name)
        pulumi.set(__self__, "sampling_percentage", sampling_percentage)

    @property
    @pulumi.getter(name="bigqueryTableName")
    def bigquery_table_name(self) -> str:
        """
        Fully qualified BigQuery table name in the following format: " project_id.dataset_name.table_name" The specified table must already exist, and the "Cloud ML Service Agent" for your project must have permission to write to it. The table must have the following [schema](/bigquery/docs/schemas): Field nameType Mode model STRING REQUIRED model_version STRING REQUIRED time TIMESTAMP REQUIRED raw_data STRING REQUIRED raw_prediction STRING NULLABLE groundtruth STRING NULLABLE 
        """
        return pulumi.get(self, "bigquery_table_name")

    @property
    @pulumi.getter(name="samplingPercentage")
    def sampling_percentage(self) -> float:
        """
        Percentage of requests to be logged, expressed as a fraction from 0 to 1. For example, if you want to log 10% of requests, enter `0.1`. The sampling window is the lifetime of the model version. Defaults to 0.
        """
        return pulumi.get(self, "sampling_percentage")


@pulumi.output_type
class GoogleCloudMlV1__RouteMapResponse(dict):
    """
    Specifies HTTP paths served by a custom container. AI Platform Prediction sends requests to these paths on the container; the custom container must run an HTTP server that responds to these requests with appropriate responses. Read [Custom container requirements](/ai-platform/prediction/docs/custom-container-requirements) for details on how to create your container image to meet these requirements.
    """
    def __init__(__self__, *,
                 health: str,
                 predict: str):
        """
        Specifies HTTP paths served by a custom container. AI Platform Prediction sends requests to these paths on the container; the custom container must run an HTTP server that responds to these requests with appropriate responses. Read [Custom container requirements](/ai-platform/prediction/docs/custom-container-requirements) for details on how to create your container image to meet these requirements.
        :param str health: HTTP path on the container to send health checkss to. AI Platform Prediction intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](/ai-platform/prediction/docs/custom-container-requirements#checks). For example, if you set this field to `/bar`, then AI Platform Prediction intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of Version.container.ports. If you don't specify this field, it defaults to the following value: /v1/models/ MODEL/versions/VERSION The placeholders in this value are replaced as follows: * MODEL: The name of the parent Model. This does not include the "projects/PROJECT_ID/models/" prefix that the API returns in output; it is the bare model name, as provided to projects.models.create. * VERSION: The name of the model version. This does not include the "projects/PROJECT_ID /models/MODEL/versions/" prefix that the API returns in output; it is the bare version name, as provided to projects.models.versions.create.
        :param str predict: HTTP path on the container to send prediction requests to. AI Platform Prediction forwards requests sent using projects.predict to this path on the container's IP address and port. AI Platform Prediction then returns the container's response in the API response. For example, if you set this field to `/foo`, then when AI Platform Prediction receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of Version.container.ports. If you don't specify this field, it defaults to the following value: /v1/models/MODEL/versions/VERSION:predict The placeholders in this value are replaced as follows: * MODEL: The name of the parent Model. This does not include the "projects/PROJECT_ID/models/" prefix that the API returns in output; it is the bare model name, as provided to projects.models.create. * VERSION: The name of the model version. This does not include the "projects/PROJECT_ID/models/MODEL/versions/" prefix that the API returns in output; it is the bare version name, as provided to projects.models.versions.create.
        """
        pulumi.set(__self__, "health", health)
        pulumi.set(__self__, "predict", predict)

    @property
    @pulumi.getter
    def health(self) -> str:
        """
        HTTP path on the container to send health checkss to. AI Platform Prediction intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](/ai-platform/prediction/docs/custom-container-requirements#checks). For example, if you set this field to `/bar`, then AI Platform Prediction intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of Version.container.ports. If you don't specify this field, it defaults to the following value: /v1/models/ MODEL/versions/VERSION The placeholders in this value are replaced as follows: * MODEL: The name of the parent Model. This does not include the "projects/PROJECT_ID/models/" prefix that the API returns in output; it is the bare model name, as provided to projects.models.create. * VERSION: The name of the model version. This does not include the "projects/PROJECT_ID /models/MODEL/versions/" prefix that the API returns in output; it is the bare version name, as provided to projects.models.versions.create.
        """
        return pulumi.get(self, "health")

    @property
    @pulumi.getter
    def predict(self) -> str:
        """
        HTTP path on the container to send prediction requests to. AI Platform Prediction forwards requests sent using projects.predict to this path on the container's IP address and port. AI Platform Prediction then returns the container's response in the API response. For example, if you set this field to `/foo`, then when AI Platform Prediction receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of Version.container.ports. If you don't specify this field, it defaults to the following value: /v1/models/MODEL/versions/VERSION:predict The placeholders in this value are replaced as follows: * MODEL: The name of the parent Model. This does not include the "projects/PROJECT_ID/models/" prefix that the API returns in output; it is the bare model name, as provided to projects.models.create. * VERSION: The name of the model version. This does not include the "projects/PROJECT_ID/models/MODEL/versions/" prefix that the API returns in output; it is the bare version name, as provided to projects.models.versions.create.
        """
        return pulumi.get(self, "predict")


@pulumi.output_type
class GoogleCloudMlV1__SampledShapleyAttributionResponse(dict):
    """
    An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "numPaths":
            suggest = "num_paths"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__SampledShapleyAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__SampledShapleyAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__SampledShapleyAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 num_paths: int):
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
        :param int num_paths: The number of feature permutations to consider when approximating the Shapley values.
        """
        pulumi.set(__self__, "num_paths", num_paths)

    @property
    @pulumi.getter(name="numPaths")
    def num_paths(self) -> int:
        """
        The number of feature permutations to consider when approximating the Shapley values.
        """
        return pulumi.get(self, "num_paths")


@pulumi.output_type
class GoogleCloudMlV1__SchedulingResponse(dict):
    """
    All parameters related to scheduling of training jobs.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "maxRunningTime":
            suggest = "max_running_time"
        elif key == "maxWaitTime":
            suggest = "max_wait_time"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__SchedulingResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__SchedulingResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__SchedulingResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 max_running_time: str,
                 max_wait_time: str,
                 priority: int):
        """
        All parameters related to scheduling of training jobs.
        :param str max_running_time: Optional. The maximum job running time, expressed in seconds. The field can contain up to nine fractional digits, terminated by `s`. If not specified, this field defaults to `604800s` (seven days). If the training job is still running after this duration, AI Platform Training cancels it. The duration is measured from when the job enters the `RUNNING` state; therefore it does not overlap with the duration limited by Scheduling.max_wait_time. For example, if you want to ensure your job runs for no more than 2 hours, set this field to `7200s` (2 hours * 60 minutes / hour * 60 seconds / minute). If you submit your training job using the `gcloud` tool, you can [specify this field in a `config.yaml` file](/ai-platform/training/docs/training-jobs#formatting_your_configuration_parameters). For example: ```yaml trainingInput: scheduling: maxRunningTime: 7200s ```
        :param str max_wait_time: Optional. The maximum job wait time, expressed in seconds. The field can contain up to nine fractional digits, terminated by `s`. If not specified, there is no limit to the wait time. The minimum for this field is `1800s` (30 minutes). If the training job has not entered the `RUNNING` state after this duration, AI Platform Training cancels it. After the job begins running, it can no longer be cancelled due to the maximum wait time. Therefore the duration limited by this field does not overlap with the duration limited by Scheduling.max_running_time. For example, if the job temporarily stops running and retries due to a [VM restart](/ai-platform/training/docs/overview#restarts), this cannot lead to a maximum wait time cancellation. However, independently of this constraint, AI Platform Training might stop a job if there are too many retries due to exhausted resources in a region. The following example describes how you might use this field: To cancel your job if it doesn't start running within 1 hour, set this field to `3600s` (1 hour * 60 minutes / hour * 60 seconds / minute). If the job is still in the `QUEUED` or `PREPARING` state after an hour of waiting, AI Platform Training cancels the job. If you submit your training job using the `gcloud` tool, you can [specify this field in a `config.yaml` file](/ai-platform/training/docs/training-jobs#formatting_your_configuration_parameters). For example: ```yaml trainingInput: scheduling: maxWaitTime: 3600s ```
        :param int priority: Optional. Job scheduling will be based on this priority, which in the range [0, 1000]. The bigger the number, the higher the priority. Default to 0 if not set. If there are multiple jobs requesting same type of accelerators, the high priority job will be scheduled prior to ones with low priority.
        """
        pulumi.set(__self__, "max_running_time", max_running_time)
        pulumi.set(__self__, "max_wait_time", max_wait_time)
        pulumi.set(__self__, "priority", priority)

    @property
    @pulumi.getter(name="maxRunningTime")
    def max_running_time(self) -> str:
        """
        Optional. The maximum job running time, expressed in seconds. The field can contain up to nine fractional digits, terminated by `s`. If not specified, this field defaults to `604800s` (seven days). If the training job is still running after this duration, AI Platform Training cancels it. The duration is measured from when the job enters the `RUNNING` state; therefore it does not overlap with the duration limited by Scheduling.max_wait_time. For example, if you want to ensure your job runs for no more than 2 hours, set this field to `7200s` (2 hours * 60 minutes / hour * 60 seconds / minute). If you submit your training job using the `gcloud` tool, you can [specify this field in a `config.yaml` file](/ai-platform/training/docs/training-jobs#formatting_your_configuration_parameters). For example: ```yaml trainingInput: scheduling: maxRunningTime: 7200s ```
        """
        return pulumi.get(self, "max_running_time")

    @property
    @pulumi.getter(name="maxWaitTime")
    def max_wait_time(self) -> str:
        """
        Optional. The maximum job wait time, expressed in seconds. The field can contain up to nine fractional digits, terminated by `s`. If not specified, there is no limit to the wait time. The minimum for this field is `1800s` (30 minutes). If the training job has not entered the `RUNNING` state after this duration, AI Platform Training cancels it. After the job begins running, it can no longer be cancelled due to the maximum wait time. Therefore the duration limited by this field does not overlap with the duration limited by Scheduling.max_running_time. For example, if the job temporarily stops running and retries due to a [VM restart](/ai-platform/training/docs/overview#restarts), this cannot lead to a maximum wait time cancellation. However, independently of this constraint, AI Platform Training might stop a job if there are too many retries due to exhausted resources in a region. The following example describes how you might use this field: To cancel your job if it doesn't start running within 1 hour, set this field to `3600s` (1 hour * 60 minutes / hour * 60 seconds / minute). If the job is still in the `QUEUED` or `PREPARING` state after an hour of waiting, AI Platform Training cancels the job. If you submit your training job using the `gcloud` tool, you can [specify this field in a `config.yaml` file](/ai-platform/training/docs/training-jobs#formatting_your_configuration_parameters). For example: ```yaml trainingInput: scheduling: maxWaitTime: 3600s ```
        """
        return pulumi.get(self, "max_wait_time")

    @property
    @pulumi.getter
    def priority(self) -> int:
        """
        Optional. Job scheduling will be based on this priority, which in the range [0, 1000]. The bigger the number, the higher the priority. Default to 0 if not set. If there are multiple jobs requesting same type of accelerators, the high priority job will be scheduled prior to ones with low priority.
        """
        return pulumi.get(self, "priority")


@pulumi.output_type
class GoogleCloudMlV1__StudyConfigResponse(dict):
    """
    Represents configuration of a study.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "automatedStoppingConfig":
            suggest = "automated_stopping_config"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__StudyConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__StudyConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__StudyConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 algorithm: str,
                 automated_stopping_config: 'outputs.GoogleCloudMlV1__AutomatedStoppingConfigResponse',
                 metrics: Sequence['outputs.GoogleCloudMlV1_StudyConfig_MetricSpecResponse'],
                 parameters: Sequence['outputs.GoogleCloudMlV1_StudyConfig_ParameterSpecResponse']):
        """
        Represents configuration of a study.
        :param str algorithm: The search algorithm specified for the study.
        :param 'GoogleCloudMlV1__AutomatedStoppingConfigResponse' automated_stopping_config: Configuration for automated stopping of unpromising Trials.
        :param Sequence['GoogleCloudMlV1_StudyConfig_MetricSpecResponse'] metrics: Metric specs for the study.
        :param Sequence['GoogleCloudMlV1_StudyConfig_ParameterSpecResponse'] parameters: The set of parameters to tune.
        """
        pulumi.set(__self__, "algorithm", algorithm)
        pulumi.set(__self__, "automated_stopping_config", automated_stopping_config)
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter
    def algorithm(self) -> str:
        """
        The search algorithm specified for the study.
        """
        return pulumi.get(self, "algorithm")

    @property
    @pulumi.getter(name="automatedStoppingConfig")
    def automated_stopping_config(self) -> 'outputs.GoogleCloudMlV1__AutomatedStoppingConfigResponse':
        """
        Configuration for automated stopping of unpromising Trials.
        """
        return pulumi.get(self, "automated_stopping_config")

    @property
    @pulumi.getter
    def metrics(self) -> Sequence['outputs.GoogleCloudMlV1_StudyConfig_MetricSpecResponse']:
        """
        Metric specs for the study.
        """
        return pulumi.get(self, "metrics")

    @property
    @pulumi.getter
    def parameters(self) -> Sequence['outputs.GoogleCloudMlV1_StudyConfig_ParameterSpecResponse']:
        """
        The set of parameters to tune.
        """
        return pulumi.get(self, "parameters")


@pulumi.output_type
class GoogleCloudMlV1__TrainingInputResponse(dict):
    """
    Represents input parameters for a training job. When using the gcloud command to submit your training job, you can specify the input parameters as command-line arguments and/or in a YAML configuration file referenced from the --config command-line argument. For details, see the guide to [submitting a training job](/ai-platform/training/docs/training-jobs).
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "enableWebAccess":
            suggest = "enable_web_access"
        elif key == "encryptionConfig":
            suggest = "encryption_config"
        elif key == "evaluatorConfig":
            suggest = "evaluator_config"
        elif key == "evaluatorCount":
            suggest = "evaluator_count"
        elif key == "evaluatorType":
            suggest = "evaluator_type"
        elif key == "jobDir":
            suggest = "job_dir"
        elif key == "masterConfig":
            suggest = "master_config"
        elif key == "masterType":
            suggest = "master_type"
        elif key == "packageUris":
            suggest = "package_uris"
        elif key == "parameterServerConfig":
            suggest = "parameter_server_config"
        elif key == "parameterServerCount":
            suggest = "parameter_server_count"
        elif key == "parameterServerType":
            suggest = "parameter_server_type"
        elif key == "pythonModule":
            suggest = "python_module"
        elif key == "pythonVersion":
            suggest = "python_version"
        elif key == "runtimeVersion":
            suggest = "runtime_version"
        elif key == "scaleTier":
            suggest = "scale_tier"
        elif key == "serviceAccount":
            suggest = "service_account"
        elif key == "useChiefInTfConfig":
            suggest = "use_chief_in_tf_config"
        elif key == "workerConfig":
            suggest = "worker_config"
        elif key == "workerCount":
            suggest = "worker_count"
        elif key == "workerType":
            suggest = "worker_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__TrainingInputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__TrainingInputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__TrainingInputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 args: Sequence[str],
                 enable_web_access: bool,
                 encryption_config: 'outputs.GoogleCloudMlV1__EncryptionConfigResponse',
                 evaluator_config: 'outputs.GoogleCloudMlV1__ReplicaConfigResponse',
                 evaluator_count: str,
                 evaluator_type: str,
                 hyperparameters: 'outputs.GoogleCloudMlV1__HyperparameterSpecResponse',
                 job_dir: str,
                 master_config: 'outputs.GoogleCloudMlV1__ReplicaConfigResponse',
                 master_type: str,
                 network: str,
                 package_uris: Sequence[str],
                 parameter_server_config: 'outputs.GoogleCloudMlV1__ReplicaConfigResponse',
                 parameter_server_count: str,
                 parameter_server_type: str,
                 python_module: str,
                 python_version: str,
                 region: str,
                 runtime_version: str,
                 scale_tier: str,
                 scheduling: 'outputs.GoogleCloudMlV1__SchedulingResponse',
                 service_account: str,
                 use_chief_in_tf_config: bool,
                 worker_config: 'outputs.GoogleCloudMlV1__ReplicaConfigResponse',
                 worker_count: str,
                 worker_type: str):
        """
        Represents input parameters for a training job. When using the gcloud command to submit your training job, you can specify the input parameters as command-line arguments and/or in a YAML configuration file referenced from the --config command-line argument. For details, see the guide to [submitting a training job](/ai-platform/training/docs/training-jobs).
        :param Sequence[str] args: Optional. Command-line arguments passed to the training application when it starts. If your job uses a custom container, then the arguments are passed to the container's `ENTRYPOINT` command.
        :param bool enable_web_access: Optional. Whether you want AI Platform Training to enable [interactive shell access](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by TrainingOutput.web_access_uris or HyperparameterOutput.web_access_uris (within TrainingOutput.trials).
        :param 'GoogleCloudMlV1__EncryptionConfigResponse' encryption_config: Optional. Options for using customer-managed encryption keys (CMEK) to protect resources created by a training job, instead of using Google's default encryption. If this is set, then all resources created by the training job will be encrypted with the customer-managed encryption key that you specify. [Learn how and when to use CMEK with AI Platform Training](/ai-platform/training/docs/cmek).
        :param 'GoogleCloudMlV1__ReplicaConfigResponse' evaluator_config: Optional. The configuration for evaluators. You should only set `evaluatorConfig.acceleratorConfig` if `evaluatorType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `evaluatorConfig.imageUri` only if you build a custom image for your evaluator. If `evaluatorConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        :param str evaluator_count: Optional. The number of evaluator replicas to use for the training job. Each replica in the cluster will be of the type specified in `evaluator_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `evaluator_type`. The default value is zero.
        :param str evaluator_type: Optional. Specifies the type of virtual machine to use for your training job's evaluator nodes. The supported values are the same as those described in the entry for `masterType`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. This value must be present when `scaleTier` is set to `CUSTOM` and `evaluatorCount` is greater than zero.
        :param 'GoogleCloudMlV1__HyperparameterSpecResponse' hyperparameters: Optional. The set of Hyperparameters to tune.
        :param str job_dir: Optional. A Google Cloud Storage path in which to store training outputs and other data needed for training. This path is passed to your TensorFlow program as the '--job-dir' command-line argument. The benefit of specifying this field is that Cloud ML validates the path for use in training.
        :param 'GoogleCloudMlV1__ReplicaConfigResponse' master_config: Optional. The configuration for your master worker. You should only set `masterConfig.acceleratorConfig` if `masterType` is set to a Compute Engine machine type. Learn about [restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `masterConfig.imageUri` only if you build a custom image. Only one of `masterConfig.imageUri` and `runtimeVersion` should be set. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        :param str master_type: Optional. Specifies the type of virtual machine to use for your training job's master worker. You must specify this field when `scaleTier` is set to `CUSTOM`. You can use certain Compute Engine machine types directly in this field. See the [list of compatible Compute Engine machine types](/ai-platform/training/docs/machine-types#compute-engine-machine-types). Alternatively, you can use the certain legacy machine types in this field. See the [list of legacy machine types](/ai-platform/training/docs/machine-types#legacy-machine-types). Finally, if you want to use a TPU for training, specify `cloud_tpu` in this field. Learn more about the [special configuration options for training with TPUs](/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine).
        :param str network: Optional. The full name of the [Compute Engine network](/vpc/docs/vpc) to which the Job is peered. For example, `projects/12345/global/networks/myVPC`. The format of this field is `projects/{project}/global/networks/{network}`, where {project} is a project number (like `12345`) and {network} is network name. Private services access must already be configured for the network. If left unspecified, the Job is not peered with any network. [Learn about using VPC Network Peering.](/ai-platform/training/docs/vpc-peering).
        :param Sequence[str] package_uris: The Google Cloud Storage location of the packages with the training program and any additional dependencies. The maximum number of package URIs is 100.
        :param 'GoogleCloudMlV1__ReplicaConfigResponse' parameter_server_config: Optional. The configuration for parameter servers. You should only set `parameterServerConfig.acceleratorConfig` if `parameterServerType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `parameterServerConfig.imageUri` only if you build a custom image for your parameter server. If `parameterServerConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        :param str parameter_server_count: Optional. The number of parameter server replicas to use for the training job. Each replica in the cluster will be of the type specified in `parameter_server_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `parameter_server_type`. The default value is zero.
        :param str parameter_server_type: Optional. Specifies the type of virtual machine to use for your training job's parameter server. The supported values are the same as those described in the entry for `master_type`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. This value must be present when `scaleTier` is set to `CUSTOM` and `parameter_server_count` is greater than zero.
        :param str python_module: The Python module name to run after installing the packages.
        :param str python_version: Optional. The version of Python used in training. You must either specify this field or specify `masterConfig.imageUri`. The following Python versions are available: * Python '3.7' is available when `runtime_version` is set to '1.15' or later. * Python '3.5' is available when `runtime_version` is set to a version from '1.4' to '1.14'. * Python '2.7' is available when `runtime_version` is set to '1.15' or earlier. Read more about the Python versions available for [each runtime version](/ml-engine/docs/runtime-version-list).
        :param str region: The region to run the training job in. See the [available regions](/ai-platform/training/docs/regions) for AI Platform Training.
        :param str runtime_version: Optional. The AI Platform runtime version to use for training. You must either specify this field or specify `masterConfig.imageUri`. For more information, see the [runtime version list](/ai-platform/training/docs/runtime-version-list) and learn [how to manage runtime versions](/ai-platform/training/docs/versioning).
        :param str scale_tier: Specifies the machine types, the number of replicas for workers and parameter servers.
        :param 'GoogleCloudMlV1__SchedulingResponse' scheduling: Optional. Scheduling options for a training job.
        :param str service_account: Optional. The email address of a service account to use when running the training appplication. You must have the `iam.serviceAccounts.actAs` permission for the specified service account. In addition, the AI Platform Training Google-managed service account must have the `roles/iam.serviceAccountAdmin` role for the specified service account. [Learn more about configuring a service account.](/ai-platform/training/docs/custom-service-account) If not specified, the AI Platform Training Google-managed service account is used by default.
        :param bool use_chief_in_tf_config: Optional. Use `chief` instead of `master` in the `TF_CONFIG` environment variable when training with a custom container. Defaults to `false`. [Learn more about this field.](/ai-platform/training/docs/distributed-training-details#chief-versus-master) This field has no effect for training jobs that don't use a custom container.
        :param 'GoogleCloudMlV1__ReplicaConfigResponse' worker_config: Optional. The configuration for workers. You should only set `workerConfig.acceleratorConfig` if `workerType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `workerConfig.imageUri` only if you build a custom image for your worker. If `workerConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        :param str worker_count: Optional. The number of worker replicas to use for the training job. Each replica in the cluster will be of the type specified in `worker_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `worker_type`. The default value is zero.
        :param str worker_type: Optional. Specifies the type of virtual machine to use for your training job's worker nodes. The supported values are the same as those described in the entry for `masterType`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. If you use `cloud_tpu` for this value, see special instructions for [configuring a custom TPU machine](/ml-engine/docs/tensorflow/using-tpus#configuring_a_custom_tpu_machine). This value must be present when `scaleTier` is set to `CUSTOM` and `workerCount` is greater than zero.
        """
        pulumi.set(__self__, "args", args)
        pulumi.set(__self__, "enable_web_access", enable_web_access)
        pulumi.set(__self__, "encryption_config", encryption_config)
        pulumi.set(__self__, "evaluator_config", evaluator_config)
        pulumi.set(__self__, "evaluator_count", evaluator_count)
        pulumi.set(__self__, "evaluator_type", evaluator_type)
        pulumi.set(__self__, "hyperparameters", hyperparameters)
        pulumi.set(__self__, "job_dir", job_dir)
        pulumi.set(__self__, "master_config", master_config)
        pulumi.set(__self__, "master_type", master_type)
        pulumi.set(__self__, "network", network)
        pulumi.set(__self__, "package_uris", package_uris)
        pulumi.set(__self__, "parameter_server_config", parameter_server_config)
        pulumi.set(__self__, "parameter_server_count", parameter_server_count)
        pulumi.set(__self__, "parameter_server_type", parameter_server_type)
        pulumi.set(__self__, "python_module", python_module)
        pulumi.set(__self__, "python_version", python_version)
        pulumi.set(__self__, "region", region)
        pulumi.set(__self__, "runtime_version", runtime_version)
        pulumi.set(__self__, "scale_tier", scale_tier)
        pulumi.set(__self__, "scheduling", scheduling)
        pulumi.set(__self__, "service_account", service_account)
        pulumi.set(__self__, "use_chief_in_tf_config", use_chief_in_tf_config)
        pulumi.set(__self__, "worker_config", worker_config)
        pulumi.set(__self__, "worker_count", worker_count)
        pulumi.set(__self__, "worker_type", worker_type)

    @property
    @pulumi.getter
    def args(self) -> Sequence[str]:
        """
        Optional. Command-line arguments passed to the training application when it starts. If your job uses a custom container, then the arguments are passed to the container's `ENTRYPOINT` command.
        """
        return pulumi.get(self, "args")

    @property
    @pulumi.getter(name="enableWebAccess")
    def enable_web_access(self) -> bool:
        """
        Optional. Whether you want AI Platform Training to enable [interactive shell access](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by TrainingOutput.web_access_uris or HyperparameterOutput.web_access_uris (within TrainingOutput.trials).
        """
        return pulumi.get(self, "enable_web_access")

    @property
    @pulumi.getter(name="encryptionConfig")
    def encryption_config(self) -> 'outputs.GoogleCloudMlV1__EncryptionConfigResponse':
        """
        Optional. Options for using customer-managed encryption keys (CMEK) to protect resources created by a training job, instead of using Google's default encryption. If this is set, then all resources created by the training job will be encrypted with the customer-managed encryption key that you specify. [Learn how and when to use CMEK with AI Platform Training](/ai-platform/training/docs/cmek).
        """
        return pulumi.get(self, "encryption_config")

    @property
    @pulumi.getter(name="evaluatorConfig")
    def evaluator_config(self) -> 'outputs.GoogleCloudMlV1__ReplicaConfigResponse':
        """
        Optional. The configuration for evaluators. You should only set `evaluatorConfig.acceleratorConfig` if `evaluatorType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `evaluatorConfig.imageUri` only if you build a custom image for your evaluator. If `evaluatorConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        """
        return pulumi.get(self, "evaluator_config")

    @property
    @pulumi.getter(name="evaluatorCount")
    def evaluator_count(self) -> str:
        """
        Optional. The number of evaluator replicas to use for the training job. Each replica in the cluster will be of the type specified in `evaluator_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `evaluator_type`. The default value is zero.
        """
        return pulumi.get(self, "evaluator_count")

    @property
    @pulumi.getter(name="evaluatorType")
    def evaluator_type(self) -> str:
        """
        Optional. Specifies the type of virtual machine to use for your training job's evaluator nodes. The supported values are the same as those described in the entry for `masterType`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. This value must be present when `scaleTier` is set to `CUSTOM` and `evaluatorCount` is greater than zero.
        """
        return pulumi.get(self, "evaluator_type")

    @property
    @pulumi.getter
    def hyperparameters(self) -> 'outputs.GoogleCloudMlV1__HyperparameterSpecResponse':
        """
        Optional. The set of Hyperparameters to tune.
        """
        return pulumi.get(self, "hyperparameters")

    @property
    @pulumi.getter(name="jobDir")
    def job_dir(self) -> str:
        """
        Optional. A Google Cloud Storage path in which to store training outputs and other data needed for training. This path is passed to your TensorFlow program as the '--job-dir' command-line argument. The benefit of specifying this field is that Cloud ML validates the path for use in training.
        """
        return pulumi.get(self, "job_dir")

    @property
    @pulumi.getter(name="masterConfig")
    def master_config(self) -> 'outputs.GoogleCloudMlV1__ReplicaConfigResponse':
        """
        Optional. The configuration for your master worker. You should only set `masterConfig.acceleratorConfig` if `masterType` is set to a Compute Engine machine type. Learn about [restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `masterConfig.imageUri` only if you build a custom image. Only one of `masterConfig.imageUri` and `runtimeVersion` should be set. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        """
        return pulumi.get(self, "master_config")

    @property
    @pulumi.getter(name="masterType")
    def master_type(self) -> str:
        """
        Optional. Specifies the type of virtual machine to use for your training job's master worker. You must specify this field when `scaleTier` is set to `CUSTOM`. You can use certain Compute Engine machine types directly in this field. See the [list of compatible Compute Engine machine types](/ai-platform/training/docs/machine-types#compute-engine-machine-types). Alternatively, you can use the certain legacy machine types in this field. See the [list of legacy machine types](/ai-platform/training/docs/machine-types#legacy-machine-types). Finally, if you want to use a TPU for training, specify `cloud_tpu` in this field. Learn more about the [special configuration options for training with TPUs](/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine).
        """
        return pulumi.get(self, "master_type")

    @property
    @pulumi.getter
    def network(self) -> str:
        """
        Optional. The full name of the [Compute Engine network](/vpc/docs/vpc) to which the Job is peered. For example, `projects/12345/global/networks/myVPC`. The format of this field is `projects/{project}/global/networks/{network}`, where {project} is a project number (like `12345`) and {network} is network name. Private services access must already be configured for the network. If left unspecified, the Job is not peered with any network. [Learn about using VPC Network Peering.](/ai-platform/training/docs/vpc-peering).
        """
        return pulumi.get(self, "network")

    @property
    @pulumi.getter(name="packageUris")
    def package_uris(self) -> Sequence[str]:
        """
        The Google Cloud Storage location of the packages with the training program and any additional dependencies. The maximum number of package URIs is 100.
        """
        return pulumi.get(self, "package_uris")

    @property
    @pulumi.getter(name="parameterServerConfig")
    def parameter_server_config(self) -> 'outputs.GoogleCloudMlV1__ReplicaConfigResponse':
        """
        Optional. The configuration for parameter servers. You should only set `parameterServerConfig.acceleratorConfig` if `parameterServerType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `parameterServerConfig.imageUri` only if you build a custom image for your parameter server. If `parameterServerConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        """
        return pulumi.get(self, "parameter_server_config")

    @property
    @pulumi.getter(name="parameterServerCount")
    def parameter_server_count(self) -> str:
        """
        Optional. The number of parameter server replicas to use for the training job. Each replica in the cluster will be of the type specified in `parameter_server_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `parameter_server_type`. The default value is zero.
        """
        return pulumi.get(self, "parameter_server_count")

    @property
    @pulumi.getter(name="parameterServerType")
    def parameter_server_type(self) -> str:
        """
        Optional. Specifies the type of virtual machine to use for your training job's parameter server. The supported values are the same as those described in the entry for `master_type`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. This value must be present when `scaleTier` is set to `CUSTOM` and `parameter_server_count` is greater than zero.
        """
        return pulumi.get(self, "parameter_server_type")

    @property
    @pulumi.getter(name="pythonModule")
    def python_module(self) -> str:
        """
        The Python module name to run after installing the packages.
        """
        return pulumi.get(self, "python_module")

    @property
    @pulumi.getter(name="pythonVersion")
    def python_version(self) -> str:
        """
        Optional. The version of Python used in training. You must either specify this field or specify `masterConfig.imageUri`. The following Python versions are available: * Python '3.7' is available when `runtime_version` is set to '1.15' or later. * Python '3.5' is available when `runtime_version` is set to a version from '1.4' to '1.14'. * Python '2.7' is available when `runtime_version` is set to '1.15' or earlier. Read more about the Python versions available for [each runtime version](/ml-engine/docs/runtime-version-list).
        """
        return pulumi.get(self, "python_version")

    @property
    @pulumi.getter
    def region(self) -> str:
        """
        The region to run the training job in. See the [available regions](/ai-platform/training/docs/regions) for AI Platform Training.
        """
        return pulumi.get(self, "region")

    @property
    @pulumi.getter(name="runtimeVersion")
    def runtime_version(self) -> str:
        """
        Optional. The AI Platform runtime version to use for training. You must either specify this field or specify `masterConfig.imageUri`. For more information, see the [runtime version list](/ai-platform/training/docs/runtime-version-list) and learn [how to manage runtime versions](/ai-platform/training/docs/versioning).
        """
        return pulumi.get(self, "runtime_version")

    @property
    @pulumi.getter(name="scaleTier")
    def scale_tier(self) -> str:
        """
        Specifies the machine types, the number of replicas for workers and parameter servers.
        """
        return pulumi.get(self, "scale_tier")

    @property
    @pulumi.getter
    def scheduling(self) -> 'outputs.GoogleCloudMlV1__SchedulingResponse':
        """
        Optional. Scheduling options for a training job.
        """
        return pulumi.get(self, "scheduling")

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> str:
        """
        Optional. The email address of a service account to use when running the training appplication. You must have the `iam.serviceAccounts.actAs` permission for the specified service account. In addition, the AI Platform Training Google-managed service account must have the `roles/iam.serviceAccountAdmin` role for the specified service account. [Learn more about configuring a service account.](/ai-platform/training/docs/custom-service-account) If not specified, the AI Platform Training Google-managed service account is used by default.
        """
        return pulumi.get(self, "service_account")

    @property
    @pulumi.getter(name="useChiefInTfConfig")
    def use_chief_in_tf_config(self) -> bool:
        """
        Optional. Use `chief` instead of `master` in the `TF_CONFIG` environment variable when training with a custom container. Defaults to `false`. [Learn more about this field.](/ai-platform/training/docs/distributed-training-details#chief-versus-master) This field has no effect for training jobs that don't use a custom container.
        """
        return pulumi.get(self, "use_chief_in_tf_config")

    @property
    @pulumi.getter(name="workerConfig")
    def worker_config(self) -> 'outputs.GoogleCloudMlV1__ReplicaConfigResponse':
        """
        Optional. The configuration for workers. You should only set `workerConfig.acceleratorConfig` if `workerType` is set to a Compute Engine machine type. [Learn about restrictions on accelerator configurations for training.](/ai-platform/training/docs/using-gpus#compute-engine-machine-types-with-gpu) Set `workerConfig.imageUri` only if you build a custom image for your worker. If `workerConfig.imageUri` has not been set, AI Platform uses the value of `masterConfig.imageUri`. Learn more about [configuring custom containers](/ai-platform/training/docs/distributed-training-containers).
        """
        return pulumi.get(self, "worker_config")

    @property
    @pulumi.getter(name="workerCount")
    def worker_count(self) -> str:
        """
        Optional. The number of worker replicas to use for the training job. Each replica in the cluster will be of the type specified in `worker_type`. This value can only be used when `scale_tier` is set to `CUSTOM`. If you set this value, you must also set `worker_type`. The default value is zero.
        """
        return pulumi.get(self, "worker_count")

    @property
    @pulumi.getter(name="workerType")
    def worker_type(self) -> str:
        """
        Optional. Specifies the type of virtual machine to use for your training job's worker nodes. The supported values are the same as those described in the entry for `masterType`. This value must be consistent with the category of machine type that `masterType` uses. In other words, both must be Compute Engine machine types or both must be legacy machine types. If you use `cloud_tpu` for this value, see special instructions for [configuring a custom TPU machine](/ml-engine/docs/tensorflow/using-tpus#configuring_a_custom_tpu_machine). This value must be present when `scaleTier` is set to `CUSTOM` and `workerCount` is greater than zero.
        """
        return pulumi.get(self, "worker_type")


@pulumi.output_type
class GoogleCloudMlV1__TrainingOutputResponse(dict):
    """
    Represents results of a training job. Output only.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "builtInAlgorithmOutput":
            suggest = "built_in_algorithm_output"
        elif key == "completedTrialCount":
            suggest = "completed_trial_count"
        elif key == "consumedMLUnits":
            suggest = "consumed_ml_units"
        elif key == "hyperparameterMetricTag":
            suggest = "hyperparameter_metric_tag"
        elif key == "isBuiltInAlgorithmJob":
            suggest = "is_built_in_algorithm_job"
        elif key == "isHyperparameterTuningJob":
            suggest = "is_hyperparameter_tuning_job"
        elif key == "webAccessUris":
            suggest = "web_access_uris"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__TrainingOutputResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__TrainingOutputResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__TrainingOutputResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 built_in_algorithm_output: 'outputs.GoogleCloudMlV1__BuiltInAlgorithmOutputResponse',
                 completed_trial_count: str,
                 consumed_ml_units: float,
                 hyperparameter_metric_tag: str,
                 is_built_in_algorithm_job: bool,
                 is_hyperparameter_tuning_job: bool,
                 trials: Sequence['outputs.GoogleCloudMlV1__HyperparameterOutputResponse'],
                 web_access_uris: Mapping[str, str]):
        """
        Represents results of a training job. Output only.
        :param 'GoogleCloudMlV1__BuiltInAlgorithmOutputResponse' built_in_algorithm_output: Details related to built-in algorithms jobs. Only set for built-in algorithms jobs.
        :param str completed_trial_count: The number of hyperparameter tuning trials that completed successfully. Only set for hyperparameter tuning jobs.
        :param float consumed_ml_units: The amount of ML units consumed by the job.
        :param str hyperparameter_metric_tag: The TensorFlow summary tag name used for optimizing hyperparameter tuning trials. See [`HyperparameterSpec.hyperparameterMetricTag`](#HyperparameterSpec.FIELDS.hyperparameter_metric_tag) for more information. Only set for hyperparameter tuning jobs.
        :param bool is_built_in_algorithm_job: Whether this job is a built-in Algorithm job.
        :param bool is_hyperparameter_tuning_job: Whether this job is a hyperparameter tuning job.
        :param Sequence['GoogleCloudMlV1__HyperparameterOutputResponse'] trials: Results for individual Hyperparameter trials. Only set for hyperparameter tuning jobs.
        :param Mapping[str, str] web_access_uris: URIs for accessing [interactive shells](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) (one URI for each training node). Only available if training_input.enable_web_access is `true`. The keys are names of each node in the training job; for example, `master-replica-0` for the master node, `worker-replica-0` for the first worker, and `ps-replica-0` for the first parameter server. The values are the URIs for each node's interactive shell.
        """
        pulumi.set(__self__, "built_in_algorithm_output", built_in_algorithm_output)
        pulumi.set(__self__, "completed_trial_count", completed_trial_count)
        pulumi.set(__self__, "consumed_ml_units", consumed_ml_units)
        pulumi.set(__self__, "hyperparameter_metric_tag", hyperparameter_metric_tag)
        pulumi.set(__self__, "is_built_in_algorithm_job", is_built_in_algorithm_job)
        pulumi.set(__self__, "is_hyperparameter_tuning_job", is_hyperparameter_tuning_job)
        pulumi.set(__self__, "trials", trials)
        pulumi.set(__self__, "web_access_uris", web_access_uris)

    @property
    @pulumi.getter(name="builtInAlgorithmOutput")
    def built_in_algorithm_output(self) -> 'outputs.GoogleCloudMlV1__BuiltInAlgorithmOutputResponse':
        """
        Details related to built-in algorithms jobs. Only set for built-in algorithms jobs.
        """
        return pulumi.get(self, "built_in_algorithm_output")

    @property
    @pulumi.getter(name="completedTrialCount")
    def completed_trial_count(self) -> str:
        """
        The number of hyperparameter tuning trials that completed successfully. Only set for hyperparameter tuning jobs.
        """
        return pulumi.get(self, "completed_trial_count")

    @property
    @pulumi.getter(name="consumedMLUnits")
    def consumed_ml_units(self) -> float:
        """
        The amount of ML units consumed by the job.
        """
        return pulumi.get(self, "consumed_ml_units")

    @property
    @pulumi.getter(name="hyperparameterMetricTag")
    def hyperparameter_metric_tag(self) -> str:
        """
        The TensorFlow summary tag name used for optimizing hyperparameter tuning trials. See [`HyperparameterSpec.hyperparameterMetricTag`](#HyperparameterSpec.FIELDS.hyperparameter_metric_tag) for more information. Only set for hyperparameter tuning jobs.
        """
        return pulumi.get(self, "hyperparameter_metric_tag")

    @property
    @pulumi.getter(name="isBuiltInAlgorithmJob")
    def is_built_in_algorithm_job(self) -> bool:
        """
        Whether this job is a built-in Algorithm job.
        """
        return pulumi.get(self, "is_built_in_algorithm_job")

    @property
    @pulumi.getter(name="isHyperparameterTuningJob")
    def is_hyperparameter_tuning_job(self) -> bool:
        """
        Whether this job is a hyperparameter tuning job.
        """
        return pulumi.get(self, "is_hyperparameter_tuning_job")

    @property
    @pulumi.getter
    def trials(self) -> Sequence['outputs.GoogleCloudMlV1__HyperparameterOutputResponse']:
        """
        Results for individual Hyperparameter trials. Only set for hyperparameter tuning jobs.
        """
        return pulumi.get(self, "trials")

    @property
    @pulumi.getter(name="webAccessUris")
    def web_access_uris(self) -> Mapping[str, str]:
        """
        URIs for accessing [interactive shells](https://cloud.google.com/ai-platform/training/docs/monitor-debug-interactive-shell) (one URI for each training node). Only available if training_input.enable_web_access is `true`. The keys are names of each node in the training job; for example, `master-replica-0` for the master node, `worker-replica-0` for the first worker, and `ps-replica-0` for the first parameter server. The values are the URIs for each node's interactive shell.
        """
        return pulumi.get(self, "web_access_uris")


@pulumi.output_type
class GoogleCloudMlV1__VersionResponse(dict):
    """
    Represents a version of the model. Each version is a trained model deployed in the cloud, ready to handle prediction requests. A model can have multiple versions. You can get information about all of the versions of a given model by calling projects.models.versions.list.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "acceleratorConfig":
            suggest = "accelerator_config"
        elif key == "autoScaling":
            suggest = "auto_scaling"
        elif key == "createTime":
            suggest = "create_time"
        elif key == "deploymentUri":
            suggest = "deployment_uri"
        elif key == "errorMessage":
            suggest = "error_message"
        elif key == "explanationConfig":
            suggest = "explanation_config"
        elif key == "isDefault":
            suggest = "is_default"
        elif key == "lastMigrationModelId":
            suggest = "last_migration_model_id"
        elif key == "lastMigrationTime":
            suggest = "last_migration_time"
        elif key == "lastUseTime":
            suggest = "last_use_time"
        elif key == "machineType":
            suggest = "machine_type"
        elif key == "manualScaling":
            suggest = "manual_scaling"
        elif key == "packageUris":
            suggest = "package_uris"
        elif key == "predictionClass":
            suggest = "prediction_class"
        elif key == "pythonVersion":
            suggest = "python_version"
        elif key == "requestLoggingConfig":
            suggest = "request_logging_config"
        elif key == "runtimeVersion":
            suggest = "runtime_version"
        elif key == "serviceAccount":
            suggest = "service_account"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__VersionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__VersionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__VersionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 accelerator_config: 'outputs.GoogleCloudMlV1__AcceleratorConfigResponse',
                 auto_scaling: 'outputs.GoogleCloudMlV1__AutoScalingResponse',
                 container: 'outputs.GoogleCloudMlV1__ContainerSpecResponse',
                 create_time: str,
                 deployment_uri: str,
                 description: str,
                 error_message: str,
                 etag: str,
                 explanation_config: 'outputs.GoogleCloudMlV1__ExplanationConfigResponse',
                 framework: str,
                 is_default: bool,
                 labels: Mapping[str, str],
                 last_migration_model_id: str,
                 last_migration_time: str,
                 last_use_time: str,
                 machine_type: str,
                 manual_scaling: 'outputs.GoogleCloudMlV1__ManualScalingResponse',
                 name: str,
                 package_uris: Sequence[str],
                 prediction_class: str,
                 python_version: str,
                 request_logging_config: 'outputs.GoogleCloudMlV1__RequestLoggingConfigResponse',
                 routes: 'outputs.GoogleCloudMlV1__RouteMapResponse',
                 runtime_version: str,
                 service_account: str,
                 state: str):
        """
        Represents a version of the model. Each version is a trained model deployed in the cloud, ready to handle prediction requests. A model can have multiple versions. You can get information about all of the versions of a given model by calling projects.models.versions.list.
        :param 'GoogleCloudMlV1__AcceleratorConfigResponse' accelerator_config: Optional. Accelerator config for using GPUs for online prediction (beta). Only specify this field if you have specified a Compute Engine (N1) machine type in the `machineType` field. Learn more about [using GPUs for online prediction](/ml-engine/docs/machine-types-online-prediction#gpus).
        :param 'GoogleCloudMlV1__AutoScalingResponse' auto_scaling: Automatically scale the number of nodes used to serve the model in response to increases and decreases in traffic. Care should be taken to ramp up traffic according to the model's ability to scale or you will start seeing increases in latency and 429 response codes.
        :param 'GoogleCloudMlV1__ContainerSpecResponse' container: Optional. Specifies a custom container to use for serving predictions. If you specify this field, then `machineType` is required. If you specify this field, then `deploymentUri` is optional. If you specify this field, then you must not specify `runtimeVersion`, `packageUris`, `framework`, `pythonVersion`, or `predictionClass`.
        :param str create_time: The time the version was created.
        :param str deployment_uri: The Cloud Storage URI of a directory containing trained model artifacts to be used to create the model version. See the [guide to deploying models](/ai-platform/prediction/docs/deploying-models) for more information. The total number of files under this directory must not exceed 1000. During projects.models.versions.create, AI Platform Prediction copies all files from the specified directory to a location managed by the service. From then on, AI Platform Prediction uses these copies of the model artifacts to serve predictions, not the original files in Cloud Storage, so this location is useful only as a historical record. If you specify container, then this field is optional. Otherwise, it is required. Learn [how to use this field with a custom container](/ai-platform/prediction/docs/custom-container-requirements#artifacts).
        :param str description: Optional. The description specified for the version when it was created.
        :param str error_message: The details of a failure or a cancellation.
        :param str etag: `etag` is used for optimistic concurrency control as a way to help prevent simultaneous updates of a model from overwriting each other. It is strongly suggested that systems make use of the `etag` in the read-modify-write cycle to perform model updates in order to avoid race conditions: An `etag` is returned in the response to `GetVersion`, and systems are expected to put that etag in the request to `UpdateVersion` to ensure that their change will be applied to the model as intended.
        :param 'GoogleCloudMlV1__ExplanationConfigResponse' explanation_config: Optional. Configures explainability features on the model's version. Some explanation features require additional metadata to be loaded as part of the model payload.
        :param str framework: Optional. The machine learning framework AI Platform uses to train this version of the model. Valid values are `TENSORFLOW`, `SCIKIT_LEARN`, `XGBOOST`. If you do not specify a framework, AI Platform will analyze files in the deployment_uri to determine a framework. If you choose `SCIKIT_LEARN` or `XGBOOST`, you must also set the runtime version of the model to 1.4 or greater. Do **not** specify a framework if you're deploying a [custom prediction routine](/ai-platform/prediction/docs/custom-prediction-routines) or if you're using a [custom container](/ai-platform/prediction/docs/use-custom-container).
        :param bool is_default: If true, this version will be used to handle prediction requests that do not specify a version. You can change the default version by calling projects.methods.versions.setDefault.
        :param Mapping[str, str] labels: Optional. One or more labels that you can add, to organize your model versions. Each label is a key-value pair, where both the key and the value are arbitrary strings that you supply. For more information, see the documentation on using labels. Note that this field is not updatable for mls1* models.
        :param str last_migration_model_id: The [AI Platform (Unified) `Model`](https://cloud.google.com/ai-platform-unified/docs/reference/rest/v1beta1/projects.locations.models) ID for the last [model migration](https://cloud.google.com/ai-platform-unified/docs/start/migrating-to-ai-platform-unified).
        :param str last_migration_time: The last time this version was successfully [migrated to AI Platform (Unified)](https://cloud.google.com/ai-platform-unified/docs/start/migrating-to-ai-platform-unified).
        :param str last_use_time: The time the version was last used for prediction.
        :param str machine_type: Optional. The type of machine on which to serve the model. Currently only applies to online prediction service. To learn about valid values for this field, read [Choosing a machine type for online prediction](/ai-platform/prediction/docs/machine-types-online-prediction). If this field is not specified and you are using a [regional endpoint](/ai-platform/prediction/docs/regional-endpoints), then the machine type defaults to `n1-standard-2`. If this field is not specified and you are using the global endpoint (`ml.googleapis.com`), then the machine type defaults to `mls1-c1-m2`.
        :param 'GoogleCloudMlV1__ManualScalingResponse' manual_scaling: Manually select the number of nodes to use for serving the model. You should generally use `auto_scaling` with an appropriate `min_nodes` instead, but this option is available if you want more predictable billing. Beware that latency and error rates will increase if the traffic exceeds that capability of the system to serve it based on the selected number of nodes.
        :param str name: The name specified for the version when it was created. The version name must be unique within the model it is created in.
        :param Sequence[str] package_uris: Optional. Cloud Storage paths (`gs://…`) of packages for [custom prediction routines](/ml-engine/docs/tensorflow/custom-prediction-routines) or [scikit-learn pipelines with custom code](/ml-engine/docs/scikit/exporting-for-prediction#custom-pipeline-code). For a custom prediction routine, one of these packages must contain your Predictor class (see [`predictionClass`](#Version.FIELDS.prediction_class)). Additionally, include any dependencies used by your Predictor or scikit-learn pipeline uses that are not already included in your selected [runtime version](/ml-engine/docs/tensorflow/runtime-version-list). If you specify this field, you must also set [`runtimeVersion`](#Version.FIELDS.runtime_version) to 1.4 or greater.
        :param str prediction_class: Optional. The fully qualified name (module_name.class_name) of a class that implements the Predictor interface described in this reference field. The module containing this class should be included in a package provided to the [`packageUris` field](#Version.FIELDS.package_uris). Specify this field if and only if you are deploying a [custom prediction routine (beta)](/ml-engine/docs/tensorflow/custom-prediction-routines). If you specify this field, you must set [`runtimeVersion`](#Version.FIELDS.runtime_version) to 1.4 or greater and you must set `machineType` to a [legacy (MLS1) machine type](/ml-engine/docs/machine-types-online-prediction). The following code sample provides the Predictor interface: class Predictor(object): \"\"\"Interface for constructing custom predictors.\"\"\" def predict(self, instances, **kwargs): \"\"\"Performs custom prediction. Instances are the decoded values from the request. They have already been deserialized from JSON. Args: instances: A list of prediction input instances. **kwargs: A dictionary of keyword args provided as additional fields on the predict request body. Returns: A list of outputs containing the prediction results. This list must be JSON serializable. \"\"\" raise NotImplementedError() @classmethod def from_path(cls, model_dir): \"\"\"Creates an instance of Predictor using the given path. Loading of the predictor should be done in this method. Args: model_dir: The local directory that contains the exported model file along with any additional files uploaded when creating the version resource. Returns: An instance implementing this Predictor class. \"\"\" raise NotImplementedError() Learn more about [the Predictor interface and custom prediction routines](/ml-engine/docs/tensorflow/custom-prediction-routines).
        :param str python_version: The version of Python used in prediction. The following Python versions are available: * Python '3.7' is available when `runtime_version` is set to '1.15' or later. * Python '3.5' is available when `runtime_version` is set to a version from '1.4' to '1.14'. * Python '2.7' is available when `runtime_version` is set to '1.15' or earlier. Read more about the Python versions available for [each runtime version](/ml-engine/docs/runtime-version-list).
        :param 'GoogleCloudMlV1__RequestLoggingConfigResponse' request_logging_config: Optional. *Only* specify this field in a projects.models.versions.patch request. Specifying it in a projects.models.versions.create request has no effect. Configures the request-response pair logging on predictions from this Version.
        :param 'GoogleCloudMlV1__RouteMapResponse' routes: Optional. Specifies paths on a custom container's HTTP server where AI Platform Prediction sends certain requests. If you specify this field, then you must also specify the `container` field. If you specify the `container` field and do not specify this field, it defaults to the following: ```json { "predict": "/v1/models/MODEL/versions/VERSION:predict", "health": "/v1/models/MODEL/versions/VERSION" } ``` See RouteMap for more details about these default values.
        :param str runtime_version: The AI Platform runtime version to use for this deployment. For more information, see the [runtime version list](/ml-engine/docs/runtime-version-list) and [how to manage runtime versions](/ml-engine/docs/versioning).
        :param str service_account: Optional. Specifies the service account for resource access control. If you specify this field, then you must also specify either the `containerSpec` or the `predictionClass` field. Learn more about [using a custom service account](/ai-platform/prediction/docs/custom-service-account).
        :param str state: The state of a version.
        """
        pulumi.set(__self__, "accelerator_config", accelerator_config)
        pulumi.set(__self__, "auto_scaling", auto_scaling)
        pulumi.set(__self__, "container", container)
        pulumi.set(__self__, "create_time", create_time)
        pulumi.set(__self__, "deployment_uri", deployment_uri)
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "error_message", error_message)
        pulumi.set(__self__, "etag", etag)
        pulumi.set(__self__, "explanation_config", explanation_config)
        pulumi.set(__self__, "framework", framework)
        pulumi.set(__self__, "is_default", is_default)
        pulumi.set(__self__, "labels", labels)
        pulumi.set(__self__, "last_migration_model_id", last_migration_model_id)
        pulumi.set(__self__, "last_migration_time", last_migration_time)
        pulumi.set(__self__, "last_use_time", last_use_time)
        pulumi.set(__self__, "machine_type", machine_type)
        pulumi.set(__self__, "manual_scaling", manual_scaling)
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "package_uris", package_uris)
        pulumi.set(__self__, "prediction_class", prediction_class)
        pulumi.set(__self__, "python_version", python_version)
        pulumi.set(__self__, "request_logging_config", request_logging_config)
        pulumi.set(__self__, "routes", routes)
        pulumi.set(__self__, "runtime_version", runtime_version)
        pulumi.set(__self__, "service_account", service_account)
        pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="acceleratorConfig")
    def accelerator_config(self) -> 'outputs.GoogleCloudMlV1__AcceleratorConfigResponse':
        """
        Optional. Accelerator config for using GPUs for online prediction (beta). Only specify this field if you have specified a Compute Engine (N1) machine type in the `machineType` field. Learn more about [using GPUs for online prediction](/ml-engine/docs/machine-types-online-prediction#gpus).
        """
        return pulumi.get(self, "accelerator_config")

    @property
    @pulumi.getter(name="autoScaling")
    def auto_scaling(self) -> 'outputs.GoogleCloudMlV1__AutoScalingResponse':
        """
        Automatically scale the number of nodes used to serve the model in response to increases and decreases in traffic. Care should be taken to ramp up traffic according to the model's ability to scale or you will start seeing increases in latency and 429 response codes.
        """
        return pulumi.get(self, "auto_scaling")

    @property
    @pulumi.getter
    def container(self) -> 'outputs.GoogleCloudMlV1__ContainerSpecResponse':
        """
        Optional. Specifies a custom container to use for serving predictions. If you specify this field, then `machineType` is required. If you specify this field, then `deploymentUri` is optional. If you specify this field, then you must not specify `runtimeVersion`, `packageUris`, `framework`, `pythonVersion`, or `predictionClass`.
        """
        return pulumi.get(self, "container")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        The time the version was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="deploymentUri")
    def deployment_uri(self) -> str:
        """
        The Cloud Storage URI of a directory containing trained model artifacts to be used to create the model version. See the [guide to deploying models](/ai-platform/prediction/docs/deploying-models) for more information. The total number of files under this directory must not exceed 1000. During projects.models.versions.create, AI Platform Prediction copies all files from the specified directory to a location managed by the service. From then on, AI Platform Prediction uses these copies of the model artifacts to serve predictions, not the original files in Cloud Storage, so this location is useful only as a historical record. If you specify container, then this field is optional. Otherwise, it is required. Learn [how to use this field with a custom container](/ai-platform/prediction/docs/custom-container-requirements#artifacts).
        """
        return pulumi.get(self, "deployment_uri")

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Optional. The description specified for the version when it was created.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter(name="errorMessage")
    def error_message(self) -> str:
        """
        The details of a failure or a cancellation.
        """
        return pulumi.get(self, "error_message")

    @property
    @pulumi.getter
    def etag(self) -> str:
        """
        `etag` is used for optimistic concurrency control as a way to help prevent simultaneous updates of a model from overwriting each other. It is strongly suggested that systems make use of the `etag` in the read-modify-write cycle to perform model updates in order to avoid race conditions: An `etag` is returned in the response to `GetVersion`, and systems are expected to put that etag in the request to `UpdateVersion` to ensure that their change will be applied to the model as intended.
        """
        return pulumi.get(self, "etag")

    @property
    @pulumi.getter(name="explanationConfig")
    def explanation_config(self) -> 'outputs.GoogleCloudMlV1__ExplanationConfigResponse':
        """
        Optional. Configures explainability features on the model's version. Some explanation features require additional metadata to be loaded as part of the model payload.
        """
        return pulumi.get(self, "explanation_config")

    @property
    @pulumi.getter
    def framework(self) -> str:
        """
        Optional. The machine learning framework AI Platform uses to train this version of the model. Valid values are `TENSORFLOW`, `SCIKIT_LEARN`, `XGBOOST`. If you do not specify a framework, AI Platform will analyze files in the deployment_uri to determine a framework. If you choose `SCIKIT_LEARN` or `XGBOOST`, you must also set the runtime version of the model to 1.4 or greater. Do **not** specify a framework if you're deploying a [custom prediction routine](/ai-platform/prediction/docs/custom-prediction-routines) or if you're using a [custom container](/ai-platform/prediction/docs/use-custom-container).
        """
        return pulumi.get(self, "framework")

    @property
    @pulumi.getter(name="isDefault")
    def is_default(self) -> bool:
        """
        If true, this version will be used to handle prediction requests that do not specify a version. You can change the default version by calling projects.methods.versions.setDefault.
        """
        return pulumi.get(self, "is_default")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        Optional. One or more labels that you can add, to organize your model versions. Each label is a key-value pair, where both the key and the value are arbitrary strings that you supply. For more information, see the documentation on using labels. Note that this field is not updatable for mls1* models.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter(name="lastMigrationModelId")
    def last_migration_model_id(self) -> str:
        """
        The [AI Platform (Unified) `Model`](https://cloud.google.com/ai-platform-unified/docs/reference/rest/v1beta1/projects.locations.models) ID for the last [model migration](https://cloud.google.com/ai-platform-unified/docs/start/migrating-to-ai-platform-unified).
        """
        return pulumi.get(self, "last_migration_model_id")

    @property
    @pulumi.getter(name="lastMigrationTime")
    def last_migration_time(self) -> str:
        """
        The last time this version was successfully [migrated to AI Platform (Unified)](https://cloud.google.com/ai-platform-unified/docs/start/migrating-to-ai-platform-unified).
        """
        return pulumi.get(self, "last_migration_time")

    @property
    @pulumi.getter(name="lastUseTime")
    def last_use_time(self) -> str:
        """
        The time the version was last used for prediction.
        """
        return pulumi.get(self, "last_use_time")

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> str:
        """
        Optional. The type of machine on which to serve the model. Currently only applies to online prediction service. To learn about valid values for this field, read [Choosing a machine type for online prediction](/ai-platform/prediction/docs/machine-types-online-prediction). If this field is not specified and you are using a [regional endpoint](/ai-platform/prediction/docs/regional-endpoints), then the machine type defaults to `n1-standard-2`. If this field is not specified and you are using the global endpoint (`ml.googleapis.com`), then the machine type defaults to `mls1-c1-m2`.
        """
        return pulumi.get(self, "machine_type")

    @property
    @pulumi.getter(name="manualScaling")
    def manual_scaling(self) -> 'outputs.GoogleCloudMlV1__ManualScalingResponse':
        """
        Manually select the number of nodes to use for serving the model. You should generally use `auto_scaling` with an appropriate `min_nodes` instead, but this option is available if you want more predictable billing. Beware that latency and error rates will increase if the traffic exceeds that capability of the system to serve it based on the selected number of nodes.
        """
        return pulumi.get(self, "manual_scaling")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        The name specified for the version when it was created. The version name must be unique within the model it is created in.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="packageUris")
    def package_uris(self) -> Sequence[str]:
        """
        Optional. Cloud Storage paths (`gs://…`) of packages for [custom prediction routines](/ml-engine/docs/tensorflow/custom-prediction-routines) or [scikit-learn pipelines with custom code](/ml-engine/docs/scikit/exporting-for-prediction#custom-pipeline-code). For a custom prediction routine, one of these packages must contain your Predictor class (see [`predictionClass`](#Version.FIELDS.prediction_class)). Additionally, include any dependencies used by your Predictor or scikit-learn pipeline uses that are not already included in your selected [runtime version](/ml-engine/docs/tensorflow/runtime-version-list). If you specify this field, you must also set [`runtimeVersion`](#Version.FIELDS.runtime_version) to 1.4 or greater.
        """
        return pulumi.get(self, "package_uris")

    @property
    @pulumi.getter(name="predictionClass")
    def prediction_class(self) -> str:
        """
        Optional. The fully qualified name (module_name.class_name) of a class that implements the Predictor interface described in this reference field. The module containing this class should be included in a package provided to the [`packageUris` field](#Version.FIELDS.package_uris). Specify this field if and only if you are deploying a [custom prediction routine (beta)](/ml-engine/docs/tensorflow/custom-prediction-routines). If you specify this field, you must set [`runtimeVersion`](#Version.FIELDS.runtime_version) to 1.4 or greater and you must set `machineType` to a [legacy (MLS1) machine type](/ml-engine/docs/machine-types-online-prediction). The following code sample provides the Predictor interface: class Predictor(object): \"\"\"Interface for constructing custom predictors.\"\"\" def predict(self, instances, **kwargs): \"\"\"Performs custom prediction. Instances are the decoded values from the request. They have already been deserialized from JSON. Args: instances: A list of prediction input instances. **kwargs: A dictionary of keyword args provided as additional fields on the predict request body. Returns: A list of outputs containing the prediction results. This list must be JSON serializable. \"\"\" raise NotImplementedError() @classmethod def from_path(cls, model_dir): \"\"\"Creates an instance of Predictor using the given path. Loading of the predictor should be done in this method. Args: model_dir: The local directory that contains the exported model file along with any additional files uploaded when creating the version resource. Returns: An instance implementing this Predictor class. \"\"\" raise NotImplementedError() Learn more about [the Predictor interface and custom prediction routines](/ml-engine/docs/tensorflow/custom-prediction-routines).
        """
        return pulumi.get(self, "prediction_class")

    @property
    @pulumi.getter(name="pythonVersion")
    def python_version(self) -> str:
        """
        The version of Python used in prediction. The following Python versions are available: * Python '3.7' is available when `runtime_version` is set to '1.15' or later. * Python '3.5' is available when `runtime_version` is set to a version from '1.4' to '1.14'. * Python '2.7' is available when `runtime_version` is set to '1.15' or earlier. Read more about the Python versions available for [each runtime version](/ml-engine/docs/runtime-version-list).
        """
        return pulumi.get(self, "python_version")

    @property
    @pulumi.getter(name="requestLoggingConfig")
    def request_logging_config(self) -> 'outputs.GoogleCloudMlV1__RequestLoggingConfigResponse':
        """
        Optional. *Only* specify this field in a projects.models.versions.patch request. Specifying it in a projects.models.versions.create request has no effect. Configures the request-response pair logging on predictions from this Version.
        """
        return pulumi.get(self, "request_logging_config")

    @property
    @pulumi.getter
    def routes(self) -> 'outputs.GoogleCloudMlV1__RouteMapResponse':
        """
        Optional. Specifies paths on a custom container's HTTP server where AI Platform Prediction sends certain requests. If you specify this field, then you must also specify the `container` field. If you specify the `container` field and do not specify this field, it defaults to the following: ```json { "predict": "/v1/models/MODEL/versions/VERSION:predict", "health": "/v1/models/MODEL/versions/VERSION" } ``` See RouteMap for more details about these default values.
        """
        return pulumi.get(self, "routes")

    @property
    @pulumi.getter(name="runtimeVersion")
    def runtime_version(self) -> str:
        """
        The AI Platform runtime version to use for this deployment. For more information, see the [runtime version list](/ml-engine/docs/runtime-version-list) and [how to manage runtime versions](/ml-engine/docs/versioning).
        """
        return pulumi.get(self, "runtime_version")

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> str:
        """
        Optional. Specifies the service account for resource access control. If you specify this field, then you must also specify either the `containerSpec` or the `predictionClass` field. Learn more about [using a custom service account](/ai-platform/prediction/docs/custom-service-account).
        """
        return pulumi.get(self, "service_account")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The state of a version.
        """
        return pulumi.get(self, "state")


@pulumi.output_type
class GoogleCloudMlV1__XraiAttributionResponse(dict):
    """
    Attributes credit by computing the XRAI taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Currently only implemented for models with natural image inputs.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "numIntegralSteps":
            suggest = "num_integral_steps"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleCloudMlV1__XraiAttributionResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleCloudMlV1__XraiAttributionResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleCloudMlV1__XraiAttributionResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 num_integral_steps: int):
        """
        Attributes credit by computing the XRAI taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Currently only implemented for models with natural image inputs.
        :param int num_integral_steps: Number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range.
        """
        pulumi.set(__self__, "num_integral_steps", num_integral_steps)

    @property
    @pulumi.getter(name="numIntegralSteps")
    def num_integral_steps(self) -> int:
        """
        Number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range.
        """
        return pulumi.get(self, "num_integral_steps")


@pulumi.output_type
class GoogleIamV1__AuditConfigResponse(dict):
    """
    Specifies the audit configuration for a service. The configuration determines which permission types are logged, and what identities, if any, are exempted from logging. An AuditConfig must have one or more AuditLogConfigs. If there are AuditConfigs for both `allServices` and a specific service, the union of the two AuditConfigs is used for that service: the log_types specified in each AuditConfig are enabled, and the exempted_members in each AuditLogConfig are exempted. Example Policy with multiple AuditConfigs: { "audit_configs": [ { "service": "allServices", "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" }, { "log_type": "ADMIN_READ" } ] }, { "service": "sampleservice.googleapis.com", "audit_log_configs": [ { "log_type": "DATA_READ" }, { "log_type": "DATA_WRITE", "exempted_members": [ "user:aliya@example.com" ] } ] } ] } For sampleservice, this policy enables DATA_READ, DATA_WRITE and ADMIN_READ logging. It also exempts `jose@example.com` from DATA_READ logging, and `aliya@example.com` from DATA_WRITE logging.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "auditLogConfigs":
            suggest = "audit_log_configs"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleIamV1__AuditConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleIamV1__AuditConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleIamV1__AuditConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 audit_log_configs: Sequence['outputs.GoogleIamV1__AuditLogConfigResponse'],
                 service: str):
        """
        Specifies the audit configuration for a service. The configuration determines which permission types are logged, and what identities, if any, are exempted from logging. An AuditConfig must have one or more AuditLogConfigs. If there are AuditConfigs for both `allServices` and a specific service, the union of the two AuditConfigs is used for that service: the log_types specified in each AuditConfig are enabled, and the exempted_members in each AuditLogConfig are exempted. Example Policy with multiple AuditConfigs: { "audit_configs": [ { "service": "allServices", "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" }, { "log_type": "ADMIN_READ" } ] }, { "service": "sampleservice.googleapis.com", "audit_log_configs": [ { "log_type": "DATA_READ" }, { "log_type": "DATA_WRITE", "exempted_members": [ "user:aliya@example.com" ] } ] } ] } For sampleservice, this policy enables DATA_READ, DATA_WRITE and ADMIN_READ logging. It also exempts `jose@example.com` from DATA_READ logging, and `aliya@example.com` from DATA_WRITE logging.
        :param Sequence['GoogleIamV1__AuditLogConfigResponse'] audit_log_configs: The configuration for logging of each type of permission.
        :param str service: Specifies a service that will be enabled for audit logging. For example, `storage.googleapis.com`, `cloudsql.googleapis.com`. `allServices` is a special value that covers all services.
        """
        pulumi.set(__self__, "audit_log_configs", audit_log_configs)
        pulumi.set(__self__, "service", service)

    @property
    @pulumi.getter(name="auditLogConfigs")
    def audit_log_configs(self) -> Sequence['outputs.GoogleIamV1__AuditLogConfigResponse']:
        """
        The configuration for logging of each type of permission.
        """
        return pulumi.get(self, "audit_log_configs")

    @property
    @pulumi.getter
    def service(self) -> str:
        """
        Specifies a service that will be enabled for audit logging. For example, `storage.googleapis.com`, `cloudsql.googleapis.com`. `allServices` is a special value that covers all services.
        """
        return pulumi.get(self, "service")


@pulumi.output_type
class GoogleIamV1__AuditLogConfigResponse(dict):
    """
    Provides the configuration for logging a type of permissions. Example: { "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" } ] } This enables 'DATA_READ' and 'DATA_WRITE' logging, while exempting jose@example.com from DATA_READ logging.
    """
    @staticmethod
    def __key_warning(key: str):
        suggest = None
        if key == "exemptedMembers":
            suggest = "exempted_members"
        elif key == "logType":
            suggest = "log_type"

        if suggest:
            pulumi.log.warn(f"Key '{key}' not found in GoogleIamV1__AuditLogConfigResponse. Access the value via the '{suggest}' property getter instead.")

    def __getitem__(self, key: str) -> Any:
        GoogleIamV1__AuditLogConfigResponse.__key_warning(key)
        return super().__getitem__(key)

    def get(self, key: str, default = None) -> Any:
        GoogleIamV1__AuditLogConfigResponse.__key_warning(key)
        return super().get(key, default)

    def __init__(__self__, *,
                 exempted_members: Sequence[str],
                 log_type: str):
        """
        Provides the configuration for logging a type of permissions. Example: { "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" } ] } This enables 'DATA_READ' and 'DATA_WRITE' logging, while exempting jose@example.com from DATA_READ logging.
        :param Sequence[str] exempted_members: Specifies the identities that do not cause logging for this type of permission. Follows the same format of Binding.members.
        :param str log_type: The log type that this config enables.
        """
        pulumi.set(__self__, "exempted_members", exempted_members)
        pulumi.set(__self__, "log_type", log_type)

    @property
    @pulumi.getter(name="exemptedMembers")
    def exempted_members(self) -> Sequence[str]:
        """
        Specifies the identities that do not cause logging for this type of permission. Follows the same format of Binding.members.
        """
        return pulumi.get(self, "exempted_members")

    @property
    @pulumi.getter(name="logType")
    def log_type(self) -> str:
        """
        The log type that this config enables.
        """
        return pulumi.get(self, "log_type")


@pulumi.output_type
class GoogleIamV1__BindingResponse(dict):
    """
    Associates `members`, or principals, with a `role`.
    """
    def __init__(__self__, *,
                 condition: 'outputs.GoogleType__ExprResponse',
                 members: Sequence[str],
                 role: str):
        """
        Associates `members`, or principals, with a `role`.
        :param 'GoogleType__ExprResponse' condition: The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        :param Sequence[str] members: Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        :param str role: Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        pulumi.set(__self__, "condition", condition)
        pulumi.set(__self__, "members", members)
        pulumi.set(__self__, "role", role)

    @property
    @pulumi.getter
    def condition(self) -> 'outputs.GoogleType__ExprResponse':
        """
        The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        """
        return pulumi.get(self, "condition")

    @property
    @pulumi.getter
    def members(self) -> Sequence[str]:
        """
        Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        """
        return pulumi.get(self, "members")

    @property
    @pulumi.getter
    def role(self) -> str:
        """
        Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        return pulumi.get(self, "role")


@pulumi.output_type
class GoogleType__ExprResponse(dict):
    """
    Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec. Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
    """
    def __init__(__self__, *,
                 description: str,
                 expression: str,
                 location: str,
                 title: str):
        """
        Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec. Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
        :param str description: Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        :param str expression: Textual representation of an expression in Common Expression Language syntax.
        :param str location: Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        :param str title: Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        pulumi.set(__self__, "description", description)
        pulumi.set(__self__, "expression", expression)
        pulumi.set(__self__, "location", location)
        pulumi.set(__self__, "title", title)

    @property
    @pulumi.getter
    def description(self) -> str:
        """
        Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        """
        return pulumi.get(self, "description")

    @property
    @pulumi.getter
    def expression(self) -> str:
        """
        Textual representation of an expression in Common Expression Language syntax.
        """
        return pulumi.get(self, "expression")

    @property
    @pulumi.getter
    def location(self) -> str:
        """
        Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        """
        return pulumi.get(self, "location")

    @property
    @pulumi.getter
    def title(self) -> str:
        """
        Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        return pulumi.get(self, "title")


