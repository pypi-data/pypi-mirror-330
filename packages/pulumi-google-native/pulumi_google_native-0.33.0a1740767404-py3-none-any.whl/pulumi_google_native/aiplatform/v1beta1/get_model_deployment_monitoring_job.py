# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from ... import _utilities
from . import outputs

__all__ = [
    'GetModelDeploymentMonitoringJobResult',
    'AwaitableGetModelDeploymentMonitoringJobResult',
    'get_model_deployment_monitoring_job',
    'get_model_deployment_monitoring_job_output',
]

@pulumi.output_type
class GetModelDeploymentMonitoringJobResult:
    def __init__(__self__, analysis_instance_schema_uri=None, bigquery_tables=None, create_time=None, display_name=None, enable_monitoring_pipeline_logs=None, encryption_spec=None, endpoint=None, error=None, labels=None, latest_monitoring_pipeline_metadata=None, log_ttl=None, logging_sampling_strategy=None, model_deployment_monitoring_objective_configs=None, model_deployment_monitoring_schedule_config=None, model_monitoring_alert_config=None, name=None, next_schedule_time=None, predict_instance_schema_uri=None, sample_predict_instance=None, schedule_state=None, state=None, stats_anomalies_base_directory=None, update_time=None):
        if analysis_instance_schema_uri and not isinstance(analysis_instance_schema_uri, str):
            raise TypeError("Expected argument 'analysis_instance_schema_uri' to be a str")
        pulumi.set(__self__, "analysis_instance_schema_uri", analysis_instance_schema_uri)
        if bigquery_tables and not isinstance(bigquery_tables, list):
            raise TypeError("Expected argument 'bigquery_tables' to be a list")
        pulumi.set(__self__, "bigquery_tables", bigquery_tables)
        if create_time and not isinstance(create_time, str):
            raise TypeError("Expected argument 'create_time' to be a str")
        pulumi.set(__self__, "create_time", create_time)
        if display_name and not isinstance(display_name, str):
            raise TypeError("Expected argument 'display_name' to be a str")
        pulumi.set(__self__, "display_name", display_name)
        if enable_monitoring_pipeline_logs and not isinstance(enable_monitoring_pipeline_logs, bool):
            raise TypeError("Expected argument 'enable_monitoring_pipeline_logs' to be a bool")
        pulumi.set(__self__, "enable_monitoring_pipeline_logs", enable_monitoring_pipeline_logs)
        if encryption_spec and not isinstance(encryption_spec, dict):
            raise TypeError("Expected argument 'encryption_spec' to be a dict")
        pulumi.set(__self__, "encryption_spec", encryption_spec)
        if endpoint and not isinstance(endpoint, str):
            raise TypeError("Expected argument 'endpoint' to be a str")
        pulumi.set(__self__, "endpoint", endpoint)
        if error and not isinstance(error, dict):
            raise TypeError("Expected argument 'error' to be a dict")
        pulumi.set(__self__, "error", error)
        if labels and not isinstance(labels, dict):
            raise TypeError("Expected argument 'labels' to be a dict")
        pulumi.set(__self__, "labels", labels)
        if latest_monitoring_pipeline_metadata and not isinstance(latest_monitoring_pipeline_metadata, dict):
            raise TypeError("Expected argument 'latest_monitoring_pipeline_metadata' to be a dict")
        pulumi.set(__self__, "latest_monitoring_pipeline_metadata", latest_monitoring_pipeline_metadata)
        if log_ttl and not isinstance(log_ttl, str):
            raise TypeError("Expected argument 'log_ttl' to be a str")
        pulumi.set(__self__, "log_ttl", log_ttl)
        if logging_sampling_strategy and not isinstance(logging_sampling_strategy, dict):
            raise TypeError("Expected argument 'logging_sampling_strategy' to be a dict")
        pulumi.set(__self__, "logging_sampling_strategy", logging_sampling_strategy)
        if model_deployment_monitoring_objective_configs and not isinstance(model_deployment_monitoring_objective_configs, list):
            raise TypeError("Expected argument 'model_deployment_monitoring_objective_configs' to be a list")
        pulumi.set(__self__, "model_deployment_monitoring_objective_configs", model_deployment_monitoring_objective_configs)
        if model_deployment_monitoring_schedule_config and not isinstance(model_deployment_monitoring_schedule_config, dict):
            raise TypeError("Expected argument 'model_deployment_monitoring_schedule_config' to be a dict")
        pulumi.set(__self__, "model_deployment_monitoring_schedule_config", model_deployment_monitoring_schedule_config)
        if model_monitoring_alert_config and not isinstance(model_monitoring_alert_config, dict):
            raise TypeError("Expected argument 'model_monitoring_alert_config' to be a dict")
        pulumi.set(__self__, "model_monitoring_alert_config", model_monitoring_alert_config)
        if name and not isinstance(name, str):
            raise TypeError("Expected argument 'name' to be a str")
        pulumi.set(__self__, "name", name)
        if next_schedule_time and not isinstance(next_schedule_time, str):
            raise TypeError("Expected argument 'next_schedule_time' to be a str")
        pulumi.set(__self__, "next_schedule_time", next_schedule_time)
        if predict_instance_schema_uri and not isinstance(predict_instance_schema_uri, str):
            raise TypeError("Expected argument 'predict_instance_schema_uri' to be a str")
        pulumi.set(__self__, "predict_instance_schema_uri", predict_instance_schema_uri)
        if sample_predict_instance and not isinstance(sample_predict_instance, dict):
            raise TypeError("Expected argument 'sample_predict_instance' to be a dict")
        pulumi.set(__self__, "sample_predict_instance", sample_predict_instance)
        if schedule_state and not isinstance(schedule_state, str):
            raise TypeError("Expected argument 'schedule_state' to be a str")
        pulumi.set(__self__, "schedule_state", schedule_state)
        if state and not isinstance(state, str):
            raise TypeError("Expected argument 'state' to be a str")
        pulumi.set(__self__, "state", state)
        if stats_anomalies_base_directory and not isinstance(stats_anomalies_base_directory, dict):
            raise TypeError("Expected argument 'stats_anomalies_base_directory' to be a dict")
        pulumi.set(__self__, "stats_anomalies_base_directory", stats_anomalies_base_directory)
        if update_time and not isinstance(update_time, str):
            raise TypeError("Expected argument 'update_time' to be a str")
        pulumi.set(__self__, "update_time", update_time)

    @property
    @pulumi.getter(name="analysisInstanceSchemaUri")
    def analysis_instance_schema_uri(self) -> str:
        """
        YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
        """
        return pulumi.get(self, "analysis_instance_schema_uri")

    @property
    @pulumi.getter(name="bigqueryTables")
    def bigquery_tables(self) -> Sequence['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTableResponse']:
        """
        The created bigquery tables for the job under customer project. Customer could do their own query & analysis. There could be 4 log tables in maximum: 1. Training data logging predict request/response 2. Serving data logging predict request/response
        """
        return pulumi.get(self, "bigquery_tables")

    @property
    @pulumi.getter(name="createTime")
    def create_time(self) -> str:
        """
        Timestamp when this ModelDeploymentMonitoringJob was created.
        """
        return pulumi.get(self, "create_time")

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> str:
        """
        The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
        """
        return pulumi.get(self, "display_name")

    @property
    @pulumi.getter(name="enableMonitoringPipelineLogs")
    def enable_monitoring_pipeline_logs(self) -> bool:
        """
        If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
        """
        return pulumi.get(self, "enable_monitoring_pipeline_logs")

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> 'outputs.GoogleCloudAiplatformV1beta1EncryptionSpecResponse':
        """
        Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @property
    @pulumi.getter
    def endpoint(self) -> str:
        """
        Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
        """
        return pulumi.get(self, "endpoint")

    @property
    @pulumi.getter
    def error(self) -> 'outputs.GoogleRpcStatusResponse':
        """
        Only populated when the job's state is `JOB_STATE_FAILED` or `JOB_STATE_CANCELLED`.
        """
        return pulumi.get(self, "error")

    @property
    @pulumi.getter
    def labels(self) -> Mapping[str, str]:
        """
        The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        return pulumi.get(self, "labels")

    @property
    @pulumi.getter(name="latestMonitoringPipelineMetadata")
    def latest_monitoring_pipeline_metadata(self) -> 'outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse':
        """
        Latest triggered monitoring pipeline metadata.
        """
        return pulumi.get(self, "latest_monitoring_pipeline_metadata")

    @property
    @pulumi.getter(name="logTtl")
    def log_ttl(self) -> str:
        """
        The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
        """
        return pulumi.get(self, "log_ttl")

    @property
    @pulumi.getter(name="loggingSamplingStrategy")
    def logging_sampling_strategy(self) -> 'outputs.GoogleCloudAiplatformV1beta1SamplingStrategyResponse':
        """
        Sample Strategy for logging.
        """
        return pulumi.get(self, "logging_sampling_strategy")

    @property
    @pulumi.getter(name="modelDeploymentMonitoringObjectiveConfigs")
    def model_deployment_monitoring_objective_configs(self) -> Sequence['outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigResponse']:
        """
        The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
        """
        return pulumi.get(self, "model_deployment_monitoring_objective_configs")

    @property
    @pulumi.getter(name="modelDeploymentMonitoringScheduleConfig")
    def model_deployment_monitoring_schedule_config(self) -> 'outputs.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigResponse':
        """
        Schedule config for running the monitoring job.
        """
        return pulumi.get(self, "model_deployment_monitoring_schedule_config")

    @property
    @pulumi.getter(name="modelMonitoringAlertConfig")
    def model_monitoring_alert_config(self) -> 'outputs.GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigResponse':
        """
        Alert config for model monitoring.
        """
        return pulumi.get(self, "model_monitoring_alert_config")

    @property
    @pulumi.getter
    def name(self) -> str:
        """
        Resource name of a ModelDeploymentMonitoringJob.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="nextScheduleTime")
    def next_schedule_time(self) -> str:
        """
        Timestamp when this monitoring pipeline will be scheduled to run for the next round.
        """
        return pulumi.get(self, "next_schedule_time")

    @property
    @pulumi.getter(name="predictInstanceSchemaUri")
    def predict_instance_schema_uri(self) -> str:
        """
        YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "predict_instance_schema_uri")

    @property
    @pulumi.getter(name="samplePredictInstance")
    def sample_predict_instance(self) -> Any:
        """
        Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
        """
        return pulumi.get(self, "sample_predict_instance")

    @property
    @pulumi.getter(name="scheduleState")
    def schedule_state(self) -> str:
        """
        Schedule state when the monitoring job is in Running state.
        """
        return pulumi.get(self, "schedule_state")

    @property
    @pulumi.getter
    def state(self) -> str:
        """
        The detailed state of the monitoring job. When the job is still creating, the state will be 'PENDING'. Once the job is successfully created, the state will be 'RUNNING'. Pause the job, the state will be 'PAUSED'. Resume the job, the state will return to 'RUNNING'.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter(name="statsAnomaliesBaseDirectory")
    def stats_anomalies_base_directory(self) -> 'outputs.GoogleCloudAiplatformV1beta1GcsDestinationResponse':
        """
        Stats anomalies base folder path.
        """
        return pulumi.get(self, "stats_anomalies_base_directory")

    @property
    @pulumi.getter(name="updateTime")
    def update_time(self) -> str:
        """
        Timestamp when this ModelDeploymentMonitoringJob was updated most recently.
        """
        return pulumi.get(self, "update_time")


class AwaitableGetModelDeploymentMonitoringJobResult(GetModelDeploymentMonitoringJobResult):
    # pylint: disable=using-constant-test
    def __await__(self):
        if False:
            yield self
        return GetModelDeploymentMonitoringJobResult(
            analysis_instance_schema_uri=self.analysis_instance_schema_uri,
            bigquery_tables=self.bigquery_tables,
            create_time=self.create_time,
            display_name=self.display_name,
            enable_monitoring_pipeline_logs=self.enable_monitoring_pipeline_logs,
            encryption_spec=self.encryption_spec,
            endpoint=self.endpoint,
            error=self.error,
            labels=self.labels,
            latest_monitoring_pipeline_metadata=self.latest_monitoring_pipeline_metadata,
            log_ttl=self.log_ttl,
            logging_sampling_strategy=self.logging_sampling_strategy,
            model_deployment_monitoring_objective_configs=self.model_deployment_monitoring_objective_configs,
            model_deployment_monitoring_schedule_config=self.model_deployment_monitoring_schedule_config,
            model_monitoring_alert_config=self.model_monitoring_alert_config,
            name=self.name,
            next_schedule_time=self.next_schedule_time,
            predict_instance_schema_uri=self.predict_instance_schema_uri,
            sample_predict_instance=self.sample_predict_instance,
            schedule_state=self.schedule_state,
            state=self.state,
            stats_anomalies_base_directory=self.stats_anomalies_base_directory,
            update_time=self.update_time)


def get_model_deployment_monitoring_job(location: Optional[str] = None,
                                        model_deployment_monitoring_job_id: Optional[str] = None,
                                        project: Optional[str] = None,
                                        opts: Optional[pulumi.InvokeOptions] = None) -> AwaitableGetModelDeploymentMonitoringJobResult:
    """
    Gets a ModelDeploymentMonitoringJob.
    """
    __args__ = dict()
    __args__['location'] = location
    __args__['modelDeploymentMonitoringJobId'] = model_deployment_monitoring_job_id
    __args__['project'] = project
    opts = pulumi.InvokeOptions.merge(_utilities.get_invoke_opts_defaults(), opts)
    __ret__ = pulumi.runtime.invoke('google-native:aiplatform/v1beta1:getModelDeploymentMonitoringJob', __args__, opts=opts, typ=GetModelDeploymentMonitoringJobResult).value

    return AwaitableGetModelDeploymentMonitoringJobResult(
        analysis_instance_schema_uri=pulumi.get(__ret__, 'analysis_instance_schema_uri'),
        bigquery_tables=pulumi.get(__ret__, 'bigquery_tables'),
        create_time=pulumi.get(__ret__, 'create_time'),
        display_name=pulumi.get(__ret__, 'display_name'),
        enable_monitoring_pipeline_logs=pulumi.get(__ret__, 'enable_monitoring_pipeline_logs'),
        encryption_spec=pulumi.get(__ret__, 'encryption_spec'),
        endpoint=pulumi.get(__ret__, 'endpoint'),
        error=pulumi.get(__ret__, 'error'),
        labels=pulumi.get(__ret__, 'labels'),
        latest_monitoring_pipeline_metadata=pulumi.get(__ret__, 'latest_monitoring_pipeline_metadata'),
        log_ttl=pulumi.get(__ret__, 'log_ttl'),
        logging_sampling_strategy=pulumi.get(__ret__, 'logging_sampling_strategy'),
        model_deployment_monitoring_objective_configs=pulumi.get(__ret__, 'model_deployment_monitoring_objective_configs'),
        model_deployment_monitoring_schedule_config=pulumi.get(__ret__, 'model_deployment_monitoring_schedule_config'),
        model_monitoring_alert_config=pulumi.get(__ret__, 'model_monitoring_alert_config'),
        name=pulumi.get(__ret__, 'name'),
        next_schedule_time=pulumi.get(__ret__, 'next_schedule_time'),
        predict_instance_schema_uri=pulumi.get(__ret__, 'predict_instance_schema_uri'),
        sample_predict_instance=pulumi.get(__ret__, 'sample_predict_instance'),
        schedule_state=pulumi.get(__ret__, 'schedule_state'),
        state=pulumi.get(__ret__, 'state'),
        stats_anomalies_base_directory=pulumi.get(__ret__, 'stats_anomalies_base_directory'),
        update_time=pulumi.get(__ret__, 'update_time'))


@_utilities.lift_output_func(get_model_deployment_monitoring_job)
def get_model_deployment_monitoring_job_output(location: Optional[pulumi.Input[str]] = None,
                                               model_deployment_monitoring_job_id: Optional[pulumi.Input[str]] = None,
                                               project: Optional[pulumi.Input[Optional[str]]] = None,
                                               opts: Optional[pulumi.InvokeOptions] = None) -> pulumi.Output[GetModelDeploymentMonitoringJobResult]:
    """
    Gets a ModelDeploymentMonitoringJob.
    """
    ...
