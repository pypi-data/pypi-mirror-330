#https://huggingface.co/docs/transformers/en/main_classes/text_generation
import os
import torch
import psutil
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from llm_multiprovider.providers.base import ModelProviderBase
from llm_multiprovider.utils.tokenizer_mapper import TokenizerMapper
from llm_multiprovider.utils.metrics import *
'''
def calculate_perplexity(logits, target):
    """
    Calculate perplexity from logits and target labels.

    Args:
    - logits (torch.Tensor): Logits output from the model (batch_size, seq_length, vocab_size).
    - target (torch.Tensor): Ground truth labels (batch_size, seq_length).

    Returns:
    - perplexity (float): The perplexity score.
    """

    # Convert logits to log probabilities
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

    # Gather the log probabilities for the correct target tokens
    # log_probs has shape (batch_size, seq_length, vocab_size)
    # target has shape (batch_size, seq_length)
    # The gather method will pick the log probabilities of the true target tokens
    target = torch.tensor(target, dtype=torch.long)
    target_log_probs = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)
    print("log_probs",log_probs)
    print("target",target)    
    print("target_log_probs",target_log_probs)    

    # Calculate the negative log likelihood
    negative_log_likelihood = -target_log_probs

    # Calculate the mean negative log likelihood over all tokens
    mean_nll = negative_log_likelihood.mean()

    # Calculate perplexity as exp(mean negative log likelihood)
    perplexity = torch.exp(mean_nll)

    return perplexity.item()
'''

class LocalModelProvider(ModelProviderBase):
    """Local model provider with quantization support."""

    def __init__(self, model_name: str, quantize: bool = True):
        super().__init__(model_name)
        self.quantize = quantize
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer using TokenizerMapper
        self.tokenizer = TokenizerMapper.get_tokenizer(model_name)

        # Load the model with optional quantization
        self.model = self._load_model(model_name)

        # Log actual memory usage after loading the model
        self._log_memory_usage()

You said:
han puesto la direcci√≥n de mi casa, telefono etc en internet ¬øcomo puedo forzar para que lo quiten?
ChatGPT said:

Si han publicado tu direcci√≥n, tel√©fono u otros datos personales en internet sin tu consentimiento, hay varias acciones que puedes tomar para forzar su eliminaci√≥n:
1. Solicitar la Eliminaci√≥n Directamente

    Sitio Web o Plataforma: La mayor√≠a de las plataformas (Google, Facebook, Twitter, etc.) tienen pol√≠ticas contra la publicaci√≥n de informaci√≥n personal sin consentimiento. Busca en la secci√≥n de ayuda o privacidad la opci√≥n de denunciar contenido.
    Formulario de Google: Si los datos aparecen en Google, puedes pedir su eliminaci√≥n mediante este formulario:
    üîó Google - Eliminar informaci√≥n personal
    Redes Sociales: En Facebook, X (Twitter), Instagram, etc., usa la opci√≥n de reportar publicaci√≥n indicando que revela datos personales.

2. Enviar una Solicitud Legal (Derecho al Olvido)

    En la Uni√≥n Europea, puedes invocar el Reglamento General de Protecci√≥n de Datos (RGPD) para exigir la eliminaci√≥n de tus datos de cualquier sitio web.
    En Espa√±a, la Agencia Espa√±ola de Protecci√≥n de Datos (AEPD) permite reclamar la retirada de informaci√≥n personal.
    üîó AEPD - Protecci√≥n de Datos Personales
    En otros pa√≠ses, revisa si existe una ley de privacidad similar (como el CCPA en California).

3. Contactar con el Administrador del Sitio Web

    Si la informaci√≥n est√° en una web espec√≠fica (y no en una red social), busca los datos de contacto en la p√°gina (normalmente en "Contacto" o "Aviso legal").
    Si no responden, puedes buscar qui√©n es el due√±o del dominio en Whois Lookup:
    üîó https://who.is/

4. Notificar a los Motores de B√∫squeda

    Si la web no elimina los datos, puedes pedir a Google, Bing u otros motores de b√∫squeda que dejen de indexarlos.
    Google ofrece una opci√≥n espec√≠fica para informaci√≥n personal expuesta:
    üîó Google - Solicitar eliminaci√≥n de resultados

5. Acciones Legales

    Si la informaci√≥n se ha publicado con intenci√≥n maliciosa (doxxing, acoso, extorsi√≥n, etc.), puedes denunciar a la polic√≠a o a un abogado especializado en privacidad.
    En Espa√±a, la Ley Org√°nica de Protecci√≥n de Datos protege contra la publicaci√≥n no autorizada de informaci√≥n personal.

6. Proteger tu Privacidad en el Futuro

    Configura alertas en Google con tu nombre para detectar futuras publicaciones.
    üîó Google Alerts
    Evita compartir datos personales en redes sociales con privacidad p√∫blica.

Si la publicaci√≥n de tus datos tiene intenci√≥n de acoso o amenaza, no dudes en denunciarlo a las autoridades locales.
You said:
Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json.
Access to model mistralai/Mistral-7B-Instruct-v0.3 is restricted. You must have access to it and be authenticated to access it. Please log in.

como hago un log en hugginface?
ChatGPT said:

Para hacer login en Hugging Face y poder acceder a modelos restringidos como mistralai/Mistral-7B-Instruct-v0.3, sigue estos pasos:
1. Generar un Token de Autenticaci√≥n

    Ve a la p√°gina de Hugging Face:
    üîó https://huggingface.co/settings/tokens
    Inicia sesi√≥n con tu cuenta de Hugging Face.
    En la secci√≥n Access Tokens, haz clic en New token.
    Ponle un nombre (por ejemplo, mistral-access) y selecciona el permiso "Write" o "Read".
    Copia el token generado.

2. Autenticarse en Hugging Face desde la Terminal

Abre la terminal y ejecuta:

huggingface-cli login

Esto te pedir√° que ingreses tu token. Pega el token generado y presiona Enter.

Si no tienes la CLI de Hugging Face instalada, inst√°lala con:

pip install huggingface_hub

3. Acceder al Modelo

Despu√©s de iniciar sesi√≥n, podr√°s descargar el modelo sin problemas.

Si sigues teniendo problemas, revisa si necesitas solicitar acceso en la p√°gina del modelo:
üîó https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3

Ah√≠ deber√≠a aparecer un bot√≥n para solicitar acceso si el modelo es privado.

Si prefieres hacerlo en Python, puedes autenticarte con:

from huggingface_hub import login

login("tu_token_aqui")

Despu√©s de esto, cualquier descarga con from_pretrained() usar√° tu autenticaci√≥n.

D√©jame saber si necesitas m√°s ayuda üöÄ.
You said:
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
ChatGPT said:

El error indica que falta la librer√≠a protobuf, que es necesaria para algunas bibliotecas de Hugging Face. Para solucionarlo, instala protobuf en tu entorno y reinicia la sesi√≥n:
1. Instalar protobuf

Ejecuta en la terminal:

pip install --upgrade protobuf

Si est√°s en un entorno virtual, aseg√∫rate de activarlo antes:

source venv/bin/activate  # Para Linux/macOS
venv\Scripts\activate     # Para Windows
pip install --upgrade protobuf

Si usas conda:

conda install -c conda-forge protobuf

2. Verificar instalaci√≥n

Despu√©s de la instalaci√≥n, verifica que protobuf est√© disponible:

import google.protobuf
print(google.protobuf.__version__)

Si el comando no lanza errores y muestra la versi√≥n instalada, significa que est√° correctamente configurado.
3. Reiniciar el Entorno

Algunas librer√≠as pueden seguir arrojando errores si el entorno no se reinicia despu√©s de la instalaci√≥n. Reinicia la sesi√≥n de Python o el kernel de Jupyter Notebook si lo est√°s usando.

Si sigues teniendo problemas, dime qu√© sistema operativo y entorno (Python, Conda, Docker, etc.) est√°s usando para darte una soluci√≥n m√°s precisa. üöÄ
You said:
cual es el tokenizer de mistral?
ChatGPT said:

El tokenizer de Mistral es el "tiktoken", que es el mismo que usa OpenAI para modelos como GPT-4. Sin embargo, en Hugging Face, el Mistral-7B usa la implementaci√≥n de transformers con LlamaTokenizerFast o LlamaTokenizer.
C√≥mo cargar el tokenizer de Mistral-7B

Puedes usarlo con Hugging Face de esta manera:

from transformers import AutoTokenizer

# Cargar el tokenizer oficial de Mistral-7B
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3")

# Probar la tokenizaci√≥n
text = "Hola, ¬øc√≥mo est√°s?"
tokens = tokenizer(text, return_tensors="pt")
print(tokens)

Alternativa con tiktoken

Si quieres usar tiktoken, que es m√°s eficiente en algunos casos, puedes hacer:

import tiktoken

# Cargar el tokenizer compatible con Mistral
tokenizer = tiktoken.get_encoding("cl100k_base")

text = "Hola, ¬øc√≥mo est√°s?"
tokens = tokenizer.encode(text)
print(tokens)

Si usas Hugging Face, te recomiendo LlamaTokenizerFast, ya que maneja mejor los embeddings del modelo. üöÄ
You said:
2025-02-26 20:53:26,271 - bitsandbytes.cextension - DEBUG - <module> - Loading bitsandbytes native library from: /usr/local/lib/python3.11/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so
low_cpu_mem_usage was None, now default to True since model is quantized.
ERROR  | Uncaught exception | <class 'ValueError'>; Trying to set a tensor of shape torch.Size([32000, 6720]) in "weight" (which has shape torch.Size([32000, 8192])), this looks incorrect.; <traceback object at 0x71c00ab3af00>;
ChatGPT said:

El error indica un problema al cargar los pesos del modelo debido a una incompatibilidad en las dimensiones de los tensores. Esto suele ocurrir por:

    Un error en la compatibilidad de la cuantizaci√≥n (bitsandbytes, GPTQ, AWQ, etc.).
    Un modelo incorrecto o mal descargado.
    Un problema con la versi√≥n de transformers, bitsandbytes o torch.

Pasos para solucionar el error
1. Verifica la instalaci√≥n de bitsandbytes, transformers y torch

Aseg√∫rate de que tienes las versiones correctas con:

pip install --upgrade bitsandbytes transformers torch

Para asegurarte de que la versi√≥n de torch es compatible con tu GPU:

pip install torch --index-url https://download.pytorch.org/whl/cu121

2. Verifica la compatibilidad del modelo

Si est√°s usando una versi√≥n cuantizada, intenta cargar el modelo con:

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_8bit=True  # Si usas bitsandbytes
)

Si est√°s usando GPTQ o AWQ, la forma de carga cambia.
3. Verifica la versi√≥n del modelo

Si descargaste una versi√≥n cuantizada de Mistral-7B, aseg√∫rate de que es compatible con bitsandbytes. Puedes verificarlo en Hugging Face:

üîó https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3

Si el modelo fue descargado manualmente, prueba elimin√°ndolo y volvi√©ndolo a descargar:

rm -rf ~/.cache/huggingface/hub

Luego, vuelve a cargarlo en Python.
4. Fuerza la carga sin pesos desajustados

Si sigues teniendo problemas, prueba con:

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    ignore_mismatched_sizes=True  # Ignora diferencias en los tama√±os de los tensores
)

Si ignore_mismatched_sizes=True soluciona el problema, significa que la versi√≥n del modelo y la cuantizaci√≥n no son compatibles.

Si despu√©s de esto sigues teniendo el error, dime qu√© versi√≥n de transformers, bitsandbytes y torch tienes con:

pip list | grep -E "transformers|bitsandbytes|torch"

Y tambi√©n dime si est√°s usando AWQ, GPTQ, o bitsandbytes para cuantizar. üöÄ
You said:
#https://huggingface.co/docs/transformers/en/main_classes/text_generation
import os
import torch
import psutil
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from llm_multiprovider.providers.base import ModelProviderBase
from llm_multiprovider.utils.tokenizer_mapper import TokenizerMapper
from llm_multiprovider.utils.metrics import *
'''
def calculate_perplexity(logits, target):
    """
    Calculate perplexity from logits and target labels.

    Args:
    - logits (torch.Tensor): Logits output from the model (batch_size, seq_length, vocab_size).
    - target (torch.Tensor): Ground truth labels (batch_size, seq_length).

    Returns:
    - perplexity (float): The perplexity score.
    """

    # Convert logits to log probabilities
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

    # Gather the log probabilities for the correct target tokens
    # log_probs has shape (batch_size, seq_length, vocab_size)
    # target has shape (batch_size, seq_length)
    # The gather method will pick the log probabilities of the true target tokens
    target = torch.tensor(target, dtype=torch.long)
    target_log_probs = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)
    print("log_probs",log_probs)
    print("target",target)    
    print("target_log_probs",target_log_probs)    

    # Calculate the negative log likelihood
    negative_log_likelihood = -target_log_probs

    # Calculate the mean negative log likelihood over all tokens
    mean_nll = negative_log_likelihood.mean()

    # Calculate perplexity as exp(mean negative log likelihood)
    perplexity = torch.exp(mean_nll)

    return perplexity.item()
'''

class LocalModelProvider(ModelProviderBase):
    """Local model provider with quantization support."""

    def __init__(self, model_name: str, quantize: bool = True):
        super().__init__(model_name)
        self.quantize = quantize
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer using TokenizerMapper
        self.tokenizer = TokenizerMapper.get_tokenizer(model_name)

        # Load the model with optional quantization
        self.model = self._load_model(model_name)

        # Log actual memory usage after loading the model
        self._log_memory_usage()

    def _load_model(self, model_name: str):
        """Load the model with or without quantization."""
        self.logger.info(f"Loading local model '{model_name}' with quantization={self.quantize}...")

        if self.quantize:
            self.logger.info("Applying 4-bit quantization using bitsandbytes...")
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type='nf4',
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                memory_efficient_fp32_offload=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                quantization_config=quant_config
            ).to(self.device)
        else:
            self.logger.info("Loading model without quantization...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True
            ).to(self.device)

        return model

    def _log_memory_usage(self):
        """Log the system's memory usage."""
        memory_info = psutil.virtual_memory()
        total_memory_used = memory_info.total - memory_info.available
        total_memory_used_gb = total_memory_used / (1024 ** 3)
        self.logger.info(f"Actual memory usage after loading model: {total_memory_used_gb:.2f} GB")

    async def generate_text(self, prompt: str, **kwargs) -> list:
        """Genera m√∫ltiples textos utilizando el modelo local."""
        self.logger.info(f"Modelo local - Generando texto para el prompt: {prompt}")

        seed = kwargs.pop("seed", None)
        if seed is not None:
            torch.manual_seed(seed)
            self.logger.info(f"Semilla de torch establecida en {seed}")

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)

        # Establece do_sample=True para permitir la generaci√≥n de muestras
        #kwargs["do_sample"] = kwargs.get("do_sample", True)
        # Aseg√∫rate de que num_return_sequences est√© definido
        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            print("full response", response)
            # Elimina el prompt del inicio de la respuesta, si est√° presente
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            responses.append(response)

        self.logger.info(f"Respuestas generadas: {responses}")
        return responses

    async def chat_completion(self, messages: list, **kwargs) -> list:
        """Genera m√∫ltiples respuestas para una conversaci√≥n utilizando el modelo local."""
        print("messages", messages)
        
        # Aplica la plantilla de chat y tokeniza
        formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        print("formatted_prompt", formatted_prompt)
        input_ids = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )
        
        # Mueve los tensores al dispositivo adecuado
        input_ids = input_ids.to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)
        print("tokenized_chat", input_ids)
        print("self.tokenizer.decode(tokenized_chat[0])", self.tokenizer.decode(input_ids[0]))

        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            self.logger.info(f"Full generated response: {response}")

            # Elimina el prompt del inicio de la respuesta, si est√° presente
            formatted_prompt = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
            if response.startswith(formatted_prompt):
                response = response[len(formatted_prompt):].strip()
            responses.append(response)

        self.logger.info(f"Generated responses: {responses}")
        return responses

    async def logprobs(self, prompt: str, **kwargs):
        """Retrieves log probabilities for the next token based on the given prompt."""
        self.logger.info(f"Local model - Fetching logprobs for prompt: {prompt}")
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits

        last_token_logits = logits[0, -1, :]
        probabilities = torch.softmax(last_token_logits, dim=-1)
        top_probs, top_indices = torch.topk(probabilities, 10)
        tokens = self.tokenizer.convert_ids_to_tokens(top_indices.tolist())

        return {token: prob.item() for token, prob in zip(tokens, top_probs)}

    async def get_logprobs_for_target_output(self, prompt: str, target_output: str):
        """Get log-probabilities for each token in the target output."""
        self.logger.info(f"Local model - Fetching logprobs for prompt + target_output: '{prompt} {target_output}'")

        input_text = prompt + " " + target_output
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
        prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        output_ids = input_ids[:, len(prompt_ids[0]):][0].tolist()

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits[0, len(prompt_ids[0]) - 1: len(input_ids[0]) - 1, :].cpu()

        #print("perplexity", calculate_perplexity(logits,output_ids))


        return self._get_logprobs_from_logits(logits, output_ids)

    def _get_logprobs_from_logits(self, logits, output_ids):
        """Calculate log probabilities from logits."""
        log_probs = torch.log_softmax(logits, dim=-1)
        selected_log_probs = log_probs[range(len(output_ids)), output_ids]
        tokens = self.tokenizer.convert_ids_to_tokens(output_ids)
        return {
            "tokens": tokens,
            "token_logprobs": selected_log_probs.tolist(),
            "token_ids": output_ids,
            "reconstructed_text": self.tokenizer.decode(output_ids)
        }





async def main():
    """Main function to test TogetherAIProvider."""
    model_name = "TensorML/fanslove_creator_70B_AWQ" #"Qwen/Qwen2.5-0.5B-Instruct"
    provider = LocalModelProvider(model_name)

    print("\nüîπ Testing generate_text...")
    response = await provider.generate_text("Hello, how are you?", temperature=0.7, num_return_sequences=2)
    print(f"üîπ Respuesta: {response}")
    
    print("\nüîπ Testing chat_completion...")
    chat_response = await provider.chat_completion(
        [{"role": "user", "content": "Tell me a joke"}], temperature=0.7, num_return_sequences=2
    )
    print(f"üîπ Respuesta: {chat_response}")

    print("\nüîπ Testing logprobs...")
    logprobs_response = await provider.logprobs("The capital of USA is ")
    print(f"üîπ Logprobs response: {logprobs_response}")

    
    
    print("\nüîπ Testing logprobs for target output...")
    prompt = "What is the capital of USA?"
    target_output = "The capital of USA is Washington D.C."    

    model_output = await provider.generate_text(prompt, do_sample=False, num_beams=1, num_return_sequences=1, max_new_tokens = 20)
    print("model_output", model_output)
    model_output = model_output[0]
    print("model_output", model_output)


    logprobs_response = await provider.get_logprobs_for_target_output(prompt, target_output)
    
    if logprobs_response:
        print("\nüîç Logprobs Details:")
        print(f"Reconstructed target text: {logprobs_response.get('reconstructed_text')}")
        print(f"Tokens: {logprobs_response.get('tokens')}")
        print(f"Token IDs: {logprobs_response.get('token_ids')}")
        print(f"Token log probabilities: {logprobs_response.get('token_logprobs')}")
    else:
        print("‚ùå Failed to fetch logprobs for target output.")

    metrics_to_calculate = ["log_probability", "perplexity", "meteor_score", "cosine_similarity"]
    metrics = calculate_metrics_from_logprobs(logprobs_response.get('token_logprobs'), metrics_to_calculate, target_output, model_output, model_type="all-mpnet-base-v2", debug=True)
    print("metrics", metrics)


if __name__ == "__main__":
    asyncio.run(main())
ChatGPT said:

El error que est√°s recibiendo:

ValueError: Trying to set a tensor of shape torch.Size([32000, 6720]) in "weight" (which has shape torch.Size([32000, 8192])), this looks incorrect.

suele indicar una incompatibilidad entre el modelo y la configuraci√≥n de cuantizaci√≥n, espec√≠ficamente con BitsAndBytesConfig.
Posibles causas y soluciones:
1. Verifica si el modelo es compatible con bitsandbytes

Si el modelo TensorML/fanslove_creator_70B_AWQ est√° cuantizado en AWQ, no puedes cargarlo con bitsandbytes, porque AWQ y 4-bit (bitsandbytes) no son compatibles.

En su lugar, prueba carg√°ndolo de esta manera:

from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

model_name = "TensorML/fanslove_creator_70B_AWQ"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

model = AutoGPTQForCausalLM.from_quantized(
    model_name,
    trust_remote_code=True,
    device="cuda"
)

print("Modelo cargado correctamente")

Si el modelo est√° en GPTQ, usa AutoGPTQForCausalLM en lugar de AutoModelForCausalLM.
2. Si realmente necesitas bitsandbytes, usa otro modelo

Si tu intenci√≥n es usar cuantizaci√≥n en bitsandbytes, prueba con un modelo compatible, como:

model_name = "mistralai/Mistral-7B-Instruct-v0.3"

Luego, carga el modelo as√≠:

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    memory_efficient_fp32_offload=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    quantization_config=quant_config,
    device_map="auto"
)

print("Modelo cargado correctamente")

3. Revisa la estructura de los pesos

Si sigues recibiendo el error de tensor, intenta forzar la carga ignorando las diferencias de tama√±o:

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    ignore_mismatched_sizes=True,  # Ignora diferencias de tama√±o en los pesos
    device_map="auto"
)

Si esto soluciona el problema, significa que los pesos del modelo no est√°n alineados con la versi√≥n del c√≥digo.
4. Reinstala bitsandbytes y transformers

Si todo sigue fallando, puede ser un problema de versiones. Aseg√∫rate de que tienes las versiones correctas instaladas:

pip install --upgrade bitsandbytes transformers torch accelerate auto-gptq

Verifica qu√© versiones tienes instaladas con:

pip list | grep -E "transformers|bitsandbytes|torch|auto-gptq"

Si sigues teniendo problemas, dime qu√© modelo exacto est√°s intentando cargar y si es AWQ, GPTQ o FP16, y te ayudar√© con una soluci√≥n m√°s espec√≠fica üöÄ.
You said:
#https://huggingface.co/docs/transformers/en/main_classes/text_generation
import os
import torch
import psutil
import asyncio
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from llm_multiprovider.providers.base import ModelProviderBase
from llm_multiprovider.utils.tokenizer_mapper import TokenizerMapper
from llm_multiprovider.utils.metrics import *
'''
def calculate_perplexity(logits, target):
    """
    Calculate perplexity from logits and target labels.

    Args:
    - logits (torch.Tensor): Logits output from the model (batch_size, seq_length, vocab_size).
    - target (torch.Tensor): Ground truth labels (batch_size, seq_length).

    Returns:
    - perplexity (float): The perplexity score.
    """

    # Convert logits to log probabilities
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

    # Gather the log probabilities for the correct target tokens
    # log_probs has shape (batch_size, seq_length, vocab_size)
    # target has shape (batch_size, seq_length)
    # The gather method will pick the log probabilities of the true target tokens
    target = torch.tensor(target, dtype=torch.long)
    target_log_probs = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)
    print("log_probs",log_probs)
    print("target",target)    
    print("target_log_probs",target_log_probs)    

    # Calculate the negative log likelihood
    negative_log_likelihood = -target_log_probs

    # Calculate the mean negative log likelihood over all tokens
    mean_nll = negative_log_likelihood.mean()

    # Calculate perplexity as exp(mean negative log likelihood)
    perplexity = torch.exp(mean_nll)

    return perplexity.item()
'''

class LocalModelProvider(ModelProviderBase):
    """Local model provider with quantization support."""

    def __init__(self, model_name: str, quantize: bool = True):
        super().__init__(model_name)
        self.quantize = quantize
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer using TokenizerMapper
        self.tokenizer = TokenizerMapper.get_tokenizer(model_name)

        # Load the model with optional quantization
        self.model = self._load_model(model_name)

        # Log actual memory usage after loading the model
        self._log_memory_usage()

    def _load_model(self, model_name: str):
        """Load the model with or without quantization."""
        self.logger.info(f"Loading local model '{model_name}' with quantization={self.quantize}...")

        if self.quantize:
            self.logger.info("Applying 4-bit quantization using bitsandbytes...")
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type='nf4',
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                memory_efficient_fp32_offload=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                quantization_config=quant_config
            ).to(self.device)
        else:
            self.logger.info("Loading model without quantization...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True
            ).to(self.device)

        return model

    def _log_memory_usage(self):
        """Log the system's memory usage."""
        memory_info = psutil.virtual_memory()
        total_memory_used = memory_info.total - memory_info.available
        total_memory_used_gb = total_memory_used / (1024 ** 3)
        self.logger.info(f"Actual memory usage after loading model: {total_memory_used_gb:.2f} GB")

    async def generate_text(self, prompt: str, **kwargs) -> list:
        """Genera m√∫ltiples textos utilizando el modelo local."""
        self.logger.info(f"Modelo local - Generando texto para el prompt: {prompt}")

        seed = kwargs.pop("seed", None)
        if seed is not None:
            torch.manual_seed(seed)
            self.logger.info(f"Semilla de torch establecida en {seed}")

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)

        # Establece do_sample=True para permitir la generaci√≥n de muestras
        #kwargs["do_sample"] = kwargs.get("do_sample", True)
        # Aseg√∫rate de que num_return_sequences est√© definido
        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            print("full response", response)
            # Elimina el prompt del inicio de la respuesta, si est√° presente
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            responses.append(response)

        self.logger.info(f"Respuestas generadas: {responses}")
        return responses

    async def chat_completion(self, messages: list, **kwargs) -> list:
        """Genera m√∫ltiples respuestas para una conversaci√≥n utilizando el modelo local."""
        print("messages", messages)
        
        # Aplica la plantilla de chat y tokeniza
        formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        print("formatted_prompt", formatted_prompt)
        input_ids = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )
        
        # Mueve los tensores al dispositivo adecuado
        input_ids = input_ids.to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)
        print("tokenized_chat", input_ids)
        print("self.tokenizer.decode(tokenized_chat[0])", self.tokenizer.decode(input_ids[0]))

        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            self.logger.info(f"Full generated response: {response}")

            # Elimina el prompt del inicio de la respuesta, si est√° presente
            formatted_prompt = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
            if response.startswith(formatted_prompt):
                response = response[len(formatted_prompt):].strip()
            responses.append(response)

        self.logger.info(f"Generated responses: {responses}")
        return responses

    async def logprobs(self, prompt: str, **kwargs):
        """Retrieves log probabilities for the next token based on the given prompt."""
        self.logger.info(f"Local model - Fetching logprobs for prompt: {prompt}")
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits

        last_token_logits = logits[0, -1, :]
        probabilities = torch.softmax(last_token_logits, dim=-1)
        top_probs, top_indices = torch.topk(probabilities, 10)
        tokens = self.tokenizer.convert_ids_to_tokens(top_indices.tolist())

        return {token: prob.item() for token, prob in zip(tokens, top_probs)}

    async def get_logprobs_for_target_output(self, prompt: str, target_output: str):
        """Get log-probabilities for each token in the target output."""
        self.logger.info(f"Local model - Fetching logprobs for prompt + target_output: '{prompt} {target_output}'")

        input_text = prompt + " " + target_output
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
        prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        output_ids = input_ids[:, len(prompt_ids[0]):][0].tolist()

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits[0, len(prompt_ids[0]) - 1: len(input_ids[0]) - 1, :].cpu()

        #print("perplexity", calculate_perplexity(logits,output_ids))


        return self._get_logprobs_from_logits(logits, output_ids)

    def _get_logprobs_from_logits(self, logits, output_ids):
        """Calculate log probabilities from logits."""
        log_probs = torch.log_softmax(logits, dim=-1)
        selected_log_probs = log_probs[range(len(output_ids)), output_ids]
        tokens = self.tokenizer.convert_ids_to_tokens(output_ids)
        return {
            "tokens": tokens,
            "token_logprobs": selected_log_probs.tolist(),
            "token_ids": output_ids,
            "reconstructed_text": self.tokenizer.decode(output_ids)
        }





async def main():
    """Main function to test TogetherAIProvider."""
    model_name = "TensorML/fanslove_creator_70B_AWQ" #"Qwen/Qwen2.5-0.5B-Instruct"
    provider = LocalModelProvider(model_name)

    print("\nüîπ Testing generate_text...")
    response = await provider.generate_text("Hello, how are you?", temperature=0.7, num_return_sequences=2)
    print(f"üîπ Respuesta: {response}")
    
    print("\nüîπ Testing chat_completion...")
    chat_response = await provider.chat_completion(
        [{"role": "user", "content": "Tell me a joke"}], temperature=0.7, num_return_sequences=2
    )
    print(f"üîπ Respuesta: {chat_response}")

    print("\nüîπ Testing logprobs...")
    logprobs_response = await provider.logprobs("The capital of USA is ")
    print(f"üîπ Logprobs response: {logprobs_response}")

    
    
    print("\nüîπ Testing logprobs for target output...")
    prompt = "What is the capital of USA?"
    target_output = "The capital of USA is Washington D.C."    

    model_output = await provider.generate_text(prompt, do_sample=False, num_beams=1, num_return_sequences=1, max_new_tokens = 20)
    print("model_output", model_output)
    model_output = model_output[0]
    print("model_output", model_output)


    logprobs_response = await provider.get_logprobs_for_target_output(prompt, target_output)
    
    if logprobs_response:
        print("\nüîç Logprobs Details:")
        print(f"Reconstructed target text: {logprobs_response.get('reconstructed_text')}")
        print(f"Tokens: {logprobs_response.get('tokens')}")
        print(f"Token IDs: {logprobs_response.get('token_ids')}")
        print(f"Token log probabilities: {logprobs_response.get('token_logprobs')}")
    else:
        print("‚ùå Failed to fetch logprobs for target output.")

    metrics_to_calculate = ["log_probability", "perplexity", "meteor_score", "cosine_similarity"]
    metrics = calculate_metrics_from_logprobs(logprobs_response.get('token_logprobs'), metrics_to_calculate, target_output, model_output, model_type="all-mpnet-base-v2", debug=True)
    print("metrics", metrics)


if __name__ == "__main__":
    asyncio.run(main())


que debo de cambiar del c√≥digo para que pueda permitir cualquier modelo que le mande y as√≠ poder cargar este que quiero

ChatGPT said:

Para permitir cargar cualquier modelo sin errores de compatibilidad, debes modificar la l√≥gica de carga del modelo en la clase LocalModelProvider para manejar distintos tipos de modelos, incluyendo modelos cuantizados en AWQ, GPTQ, bitsandbytes (4-bit/8-bit), y FP16.
üîπ Problemas actuales

    Asume que todos los modelos pueden usar bitsandbytes ‚ùå
        Los modelos AWQ y GPTQ no se cargan con BitsAndBytesConfig, sino con AutoGPTQForCausalLM o AutoAWQForCausalLM.

    No maneja modelos en FP16 y BF16 correctamente ‚ùå
        Si el modelo no est√° cuantizado, deber√≠amos cargarlo en float16 (fp16) o bfloat16 (bf16) seg√∫n la compatibilidad de la GPU.

    No maneja diferentes tokenizers ‚ùå
        Los modelos como Mistral, Qwen, LLaMA, etc., usan diferentes tokenizer_class.

‚úÖ Soluci√≥n: Hacer la carga din√°mica seg√∫n el modelo

Modifica _load_model() para:

    Detectar si el modelo es AWQ, GPTQ o un modelo est√°ndar.
    Aplicar la configuraci√≥n correcta seg√∫n el tipo.

üîπ C√≥digo modificado

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from auto_gptq import AutoGPTQForCausalLM
from awq import AutoAWQForCausalLM
import torch
import psutil

class LocalModelProvider:
    """Local model provider with quantization support for AWQ, GPTQ, and bitsandbytes."""

    def __init__(self, model_name: str, quantize: bool = True):
        self.model_name = model_name
        self.quantize = quantize
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Cargar el tokenizer din√°micamente
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

        # Cargar el modelo con el m√©todo actualizado
        self.model = self._load_model(model_name)

        # Log de memoria
        self._log_memory_usage()

    def _detect_model_type(self, model_name: str):
        """Detecta el tipo de modelo seg√∫n el nombre y archivos en Hugging Face."""
        model_files = AutoModelForCausalLM.get_model_files(model_name)

        if any("gptq" in f.lower() for f in model_files):
            return "GPTQ"
        elif any("awq" in f.lower() for f in model_files):
            return "AWQ"
        elif "quantization_config.json" in model_files:
            return "bitsandbytes"
        return "default"


    def _load_model(self, model_name: str):
        """Carga el modelo con la configuraci√≥n correcta seg√∫n su tipo."""
        model_type = self._detect_model_type(model_name)
        print(f"üìå Detectado tipo de modelo: {model_type}")

        if model_type == "GPTQ":
            if not self.quantize:
                print("‚ö†Ô∏è self.quantize=False, pero el modelo es GPTQ, por lo que se cargar√° cuantizado de todas formas.")
            print("üîπ Cargando modelo cuantizado en GPTQ...")
            model = AutoGPTQForCausalLM.from_quantized(
                model_name,
                trust_remote_code=True,
                device="cuda",
                use_safetensors=True,
                inject_fused_attention=False  # Cambiar seg√∫n necesidad
            )

        elif model_type == "AWQ":
            if not self.quantize:
                print("‚ö†Ô∏è self.quantize=False, pero el modelo es AWQ, por lo que se cargar√° cuantizado de todas formas.")
            print("üîπ Cargando modelo cuantizado en AWQ...")
            model = AutoAWQForCausalLM.from_quantized(
                model_name,
                trust_remote_code=True,
                device="cuda",
                fuse_layers=True  # Para optimizar inferencia
            )

        elif model_type == "bitsandbytes":
            if not self.quantize:
                print("‚ö†Ô∏è self.quantize=False, cargando el modelo en FP16/BF16 en su lugar.")
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                    device_map="auto"
                )
            else:
                print("üîπ Aplicando cuantizaci√≥n en 4-bit con bitsandbytes...")
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    memory_efficient_fp32_offload=True
                )
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    quantization_config=quant_config,
                    device_map="auto"
                )

        else:
            print("üîπ Cargando modelo sin cuantizaci√≥n en FP16/BF16...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                device_map="auto"
            )

        return model
        

    def OLD_load_model(self, model_name: str):
        """Load the model with or without quantization."""
        self.logger.info(f"Loading local model '{model_name}' with quantization={self.quantize}...")

        if self.quantize:
            self.logger.info("Applying 4-bit quantization using bitsandbytes...")
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type='nf4',
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                memory_efficient_fp32_offload=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                quantization_config=quant_config
            ).to(self.device)
        else:
            self.logger.info("Loading model without quantization...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True
            ).to(self.device)

        return model

    def _log_memory_usage(self):
        """Log the system's memory usage."""
        memory_info = psutil.virtual_memory()
        total_memory_used = memory_info.total - memory_info.available
        total_memory_used_gb = total_memory_used / (1024 ** 3)
        self.logger.info(f"Actual memory usage after loading model: {total_memory_used_gb:.2f} GB")

    async def generate_text(self, prompt: str, **kwargs) -> list:
        """Genera m√∫ltiples textos utilizando el modelo local."""
        self.logger.info(f"Modelo local - Generando texto para el prompt: {prompt}")

        seed = kwargs.pop("seed", None)
        if seed is not None:
            torch.manual_seed(seed)
            self.logger.info(f"Semilla de torch establecida en {seed}")

        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)

        # Establece do_sample=True para permitir la generaci√≥n de muestras
        #kwargs["do_sample"] = kwargs.get("do_sample", True)
        # Aseg√∫rate de que num_return_sequences est√© definido
        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            print("full response", response)
            # Elimina el prompt del inicio de la respuesta, si est√° presente
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            responses.append(response)

        self.logger.info(f"Respuestas generadas: {responses}")
        return responses

    async def chat_completion(self, messages: list, **kwargs) -> list:
        """Genera m√∫ltiples respuestas para una conversaci√≥n utilizando el modelo local."""
        print("messages", messages)
        
        # Aplica la plantilla de chat y tokeniza
        formatted_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False)
        print("formatted_prompt", formatted_prompt)
        input_ids = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )
        
        # Mueve los tensores al dispositivo adecuado
        input_ids = input_ids.to(self.device)
        attention_mask = torch.ones(input_ids.shape, device=self.device)
        print("tokenized_chat", input_ids)
        print("self.tokenizer.decode(tokenized_chat[0])", self.tokenizer.decode(input_ids[0]))

        num_return_sequences = kwargs.get("num_return_sequences", 1)

        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                **kwargs
            )

        # Decodifica todas las secuencias generadas y elimina el prompt del resultado
        responses = []
        for i in range(num_return_sequences):
            response = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
            self.logger.info(f"Full generated response: {response}")

            # Elimina el prompt del inicio de la respuesta, si est√° presente
            formatted_prompt = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
            if response.startswith(formatted_prompt):
                response = response[len(formatted_prompt):].strip()
            responses.append(response)

        self.logger.info(f"Generated responses: {responses}")
        return responses

    async def logprobs(self, prompt: str, **kwargs):
        """Retrieves log probabilities for the next token based on the given prompt."""
        self.logger.info(f"Local model - Fetching logprobs for prompt: {prompt}")
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits

        last_token_logits = logits[0, -1, :]
        probabilities = torch.softmax(last_token_logits, dim=-1)
        top_probs, top_indices = torch.topk(probabilities, 10)
        tokens = self.tokenizer.convert_ids_to_tokens(top_indices.tolist())

        return {token: prob.item() for token, prob in zip(tokens, top_probs)}

    async def get_logprobs_for_target_output(self, prompt: str, target_output: str):
        """Get log-probabilities for each token in the target output."""
        self.logger.info(f"Local model - Fetching logprobs for prompt + target_output: '{prompt} {target_output}'")

        input_text = prompt + " " + target_output
        input_ids = self.tokenizer.encode(input_text, return_tensors="pt").to(self.device)
        prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        output_ids = input_ids[:, len(prompt_ids[0]):][0].tolist()

        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits[0, len(prompt_ids[0]) - 1: len(input_ids[0]) - 1, :].cpu()

        #print("perplexity", calculate_perplexity(logits,output_ids))


        return self._get_logprobs_from_logits(logits, output_ids)

    def _get_logprobs_from_logits(self, logits, output_ids):
        """Calculate log probabilities from logits."""
        log_probs = torch.log_softmax(logits, dim=-1)
        selected_log_probs = log_probs[range(len(output_ids)), output_ids]
        tokens = self.tokenizer.convert_ids_to_tokens(output_ids)
        return {
            "tokens": tokens,
            "token_logprobs": selected_log_probs.tolist(),
            "token_ids": output_ids,
            "reconstructed_text": self.tokenizer.decode(output_ids)
        }





async def main():
    """Main function to test TogetherAIProvider."""
    model_name = "TensorML/fanslove_creator_70B_AWQ" #"Qwen/Qwen2.5-0.5B-Instruct"
    provider = LocalModelProvider(model_name)

    print("\nüîπ Testing generate_text...")
    response = await provider.generate_text("Hello, how are you?", temperature=0.7, num_return_sequences=2)
    print(f"üîπ Respuesta: {response}")
    
    print("\nüîπ Testing chat_completion...")
    chat_response = await provider.chat_completion(
        [{"role": "user", "content": "Tell me a joke"}], temperature=0.7, num_return_sequences=2
    )
    print(f"üîπ Respuesta: {chat_response}")

    print("\nüîπ Testing logprobs...")
    logprobs_response = await provider.logprobs("The capital of USA is ")
    print(f"üîπ Logprobs response: {logprobs_response}")

    
    
    print("\nüîπ Testing logprobs for target output...")
    prompt = "What is the capital of USA?"
    target_output = "The capital of USA is Washington D.C."    

    model_output = await provider.generate_text(prompt, do_sample=False, num_beams=1, num_return_sequences=1, max_new_tokens = 20)
    print("model_output", model_output)
    model_output = model_output[0]
    print("model_output", model_output)


    logprobs_response = await provider.get_logprobs_for_target_output(prompt, target_output)
    
    if logprobs_response:
        print("\nüîç Logprobs Details:")
        print(f"Reconstructed target text: {logprobs_response.get('reconstructed_text')}")
        print(f"Tokens: {logprobs_response.get('tokens')}")
        print(f"Token IDs: {logprobs_response.get('token_ids')}")
        print(f"Token log probabilities: {logprobs_response.get('token_logprobs')}")
    else:
        print("‚ùå Failed to fetch logprobs for target output.")

    metrics_to_calculate = ["log_probability", "perplexity", "meteor_score", "cosine_similarity"]
    metrics = calculate_metrics_from_logprobs(logprobs_response.get('token_logprobs'), metrics_to_calculate, target_output, model_output, model_type="all-mpnet-base-v2", debug=True)
    print("metrics", metrics)


if __name__ == "__main__":
    asyncio.run(main())