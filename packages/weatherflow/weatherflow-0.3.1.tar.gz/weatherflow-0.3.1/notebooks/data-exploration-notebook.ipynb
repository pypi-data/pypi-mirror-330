{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERA5 Data Exploration for Weather Flow Matching\n",
    "\n",
    "This notebook demonstrates how to load, explore, and visualize ERA5 data for weather prediction using the WeatherFlow library. We'll cover:\n",
    "\n",
    "1. Loading data from WeatherBench2\n",
    "2. Exploring the data structure\n",
    "3. Visualizing different variables\n",
    "4. Preparing data for model training\n",
    "5. Computing statistics and climatology\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's make sure we have WeatherFlow and all dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install WeatherFlow if needed\n",
    "try:\n",
    "    import weatherflow\n",
    "    print(f\"WeatherFlow version: {weatherflow.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install -e ..\n",
    "    import weatherflow\n",
    "    print(f\"WeatherFlow installed, version: {weatherflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress some warnings for cleaner output\n",
    "\n",
    "# Import WeatherFlow modules\n",
    "from weatherflow.data import ERA5Dataset, create_data_loaders\n",
    "from weatherflow.utils import WeatherVisualizer\n",
    "\n",
    "# Set up matplotlib larger figures\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading ERA5 Data\n",
    "\n",
    "WeatherFlow supports loading ERA5 data from multiple sources:\n",
    "\n",
    "1. WeatherBench2 on Google Cloud Storage\n",
    "2. Local NetCDF files\n",
    "3. Custom Zarr datasets\n",
    "\n",
    "Let's use the WeatherBench2 dataset which contains preprocessed global ERA5 reanalysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and pressure levels we're interested in\n",
    "variables = ['z', 't', 'u', 'v']  # Geopotential, temperature, u-wind, v-wind\n",
    "pressure_levels = [500]  # 500 hPa level\n",
    "years = ('2016', '2016')  # Load just one year for faster exploration\n",
    "\n",
    "# Detailed explanation of variables:\n",
    "variable_details = {\n",
    "    'z': 'Geopotential (m²/s²) - Represents atmospheric pressure levels',\n",
    "    't': 'Temperature (K) - Air temperature',\n",
    "    'u': 'U-component of wind (m/s) - Eastward wind',\n",
    "    'v': 'V-component of wind (m/s) - Northward wind',\n",
    "    'q': 'Specific humidity (kg/kg) - Mass of water vapor per unit mass of air',\n",
    "    'r': 'Relative humidity (%) - Amount of water vapor relative to maximum possible'\n",
    "}\n",
    "\n",
    "# Print selected variables and their descriptions\n",
    "print(\"Selected variables:\")\n",
    "for var in variables:\n",
    "    print(f\"  - {var}: {variable_details.get(var, 'Unknown variable')}\")\n",
    "print(f\"\\nPressure level: {pressure_levels[0]} hPa\")\n",
    "print(f\"Time period: {years[0]} to {years[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ERA5 data with progress information\n",
    "print(\"Loading ERA5 data from WeatherBench2...\")\n",
    "try:\n",
    "    # Try loading with default settings\n",
    "    era5_data = ERA5Dataset(\n",
    "        variables=variables,\n",
    "        pressure_levels=pressure_levels,\n",
    "        time_slice=years,\n",
    "        normalize=False,  # Keep original values for exploration\n",
    "        verbose=True\n",
    "    )\n",
    "    print(f\"Successfully loaded data with {len(era5_data)} time steps\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    print(\"\\nTrying alternative loading method...\")\n",
    "    \n",
    "    # If default method fails, try with explicit storage options\n",
    "    era5_data = ERA5Dataset(\n",
    "        variables=variables,\n",
    "        pressure_levels=pressure_levels,\n",
    "        time_slice=years,\n",
    "        normalize=False,\n",
    "        verbose=True,\n",
    "        add_physics_features=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring Data Structure\n",
    "\n",
    "Let's examine the structure of the loaded data to better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic dataset information\n",
    "print(f\"Dataset shape information:\")\n",
    "print(f\"  - Number of time steps: {len(era5_data)}\")\n",
    "print(f\"  - Variables: {era5_data.variables}\")\n",
    "print(f\"  - Pressure levels: {era5_data.pressure_levels}\")\n",
    "print(f\"  - Spatial grid size: {era5_data.ds.latitude.size} × {era5_data.ds.longitude.size}\")\n",
    "\n",
    "# Look at the first sample to understand its structure\n",
    "sample = era5_data[0]\n",
    "print(\"\\nSample data structure:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  - {key}: {type(value)}\")\n",
    "        for subkey, subvalue in value.items():\n",
    "            print(f\"      {subkey}: {type(subvalue)}\")\n",
    "    else:\n",
    "        print(f\"  - {key}: {type(value)}, shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinate information\n",
    "coords = era5_data.get_coords()\n",
    "lats = coords['latitude']\n",
    "lons = coords['longitude']\n",
    "\n",
    "print(f\"Latitude range: {lats.min():.2f}° to {lats.max():.2f}°, {len(lats)} points\")\n",
    "print(f\"Longitude range: {lons.min():.2f}° to {lons.max():.2f}°, {len(lons)} points\")\n",
    "\n",
    "# Show coordinate spacing (important for certain physical calculations)\n",
    "lat_spacing = np.mean(np.diff(lats))\n",
    "lon_spacing = np.mean(np.diff(lons))\n",
    "print(f\"Grid resolution: {lat_spacing:.2f}° latitude × {lon_spacing:.2f}° longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Weather Variables\n",
    "\n",
    "Now let's visualize each of our variables to get a feel for the data. We'll use the WeatherVisualizer class from WeatherFlow for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = WeatherVisualizer(figsize=(14, 8))\n",
    "\n",
    "# Extract first sample (current state)\n",
    "sample_data = era5_data[0]['input']\n",
    "\n",
    "# Create a dictionary for visualization\n",
    "data_dict = {}\n",
    "for i, var in enumerate(variables):\n",
    "    # Each variable has shape [levels, lat, lon], select first level\n",
    "    data_dict[var] = sample_data[i, 0].numpy()  # Convert tensor to numpy\n",
    "\n",
    "# Plot each variable\n",
    "for i, var_name in enumerate(variables):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    fig, ax = visualizer.plot_field(\n",
    "        data_dict[var_name],\n",
    "        title=f\"{var_name} at {pressure_levels[0]} hPa\",\n",
    "        var_name=var_name,\n",
    "        coastlines=True,\n",
    "        grid=True\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Wind Vector Visualization\n",
    "\n",
    "Since we have both U and V wind components, we can visualize the vector field to see wind patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract U and V wind components\n",
    "u_index = variables.index('u')\n",
    "v_index = variables.index('v')\n",
    "u_wind = sample_data[u_index, 0].numpy()\n",
    "v_wind = sample_data[v_index, 0].numpy()\n",
    "\n",
    "# For background, use geopotential height\n",
    "z_index = variables.index('z')\n",
    "geopotential = sample_data[z_index, 0].numpy()\n",
    "\n",
    "# Calculate wind speed (magnitude)\n",
    "wind_speed = np.sqrt(u_wind**2 + v_wind**2)\n",
    "\n",
    "# Plot wind field with geopotential height as background\n",
    "fig, ax = visualizer.plot_flow_vectors(\n",
    "    u_wind, v_wind, \n",
    "    background=geopotential, \n",
    "    var_name='z',\n",
    "    title=f\"Wind Field at {pressure_levels[0]} hPa\",\n",
    "    scale=1.0, \n",
    "    density=1.0\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot wind speed\n",
    "plt.figure(figsize=(14, 8))\n",
    "fig, ax = visualizer.plot_field(\n",
    "    wind_speed,\n",
    "    title=f\"Wind Speed at {pressure_levels[0]} hPa\",\n",
    "    cmap='YlOrRd',\n",
    "    coastlines=True,\n",
    "    grid=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Evolution\n",
    "\n",
    "Let's look at how variables change over time by extracting and visualizing a sequence of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps to visualize\n",
    "n_steps = 5\n",
    "\n",
    "# Extract a sequence of states for one variable (geopotential)\n",
    "var_index = 0  # Index of variable to visualize (geopotential)\n",
    "level_index = 0  # First pressure level\n",
    "\n",
    "# Collect time sequence\n",
    "time_sequence = []\n",
    "time_stamps = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    if i < len(era5_data):\n",
    "        sample = era5_data[i]\n",
    "        # Extract the variable\n",
    "        time_sequence.append(sample['input'][var_index, level_index].numpy())\n",
    "        # Extract timestamp from metadata\n",
    "        time_stamps.append(sample['metadata']['t0'])\n",
    "\n",
    "# Create animation\n",
    "print(f\"Creating animation for {variables[var_index]} at {pressure_levels[0]} hPa...\")\n",
    "anim = visualizer.create_prediction_animation(\n",
    "    time_sequence,\n",
    "    var_name=variables[var_index],\n",
    "    title=f\"{variables[var_index]} Evolution\",\n",
    "    interval=800  # Slower animation for better viewing\n",
    ")\n",
    "\n",
    "# Display animation\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Statistics and Climatology\n",
    "\n",
    "Understanding the statistical properties of each variable is important for normalization and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for each variable\n",
    "stats = {}\n",
    "\n",
    "# Number of samples to use for statistics (limit for memory efficiency)\n",
    "n_samples = min(50, len(era5_data))\n",
    "print(f\"Computing statistics from {n_samples} samples...\")\n",
    "\n",
    "# Initialize arrays to collect data\n",
    "var_data = {var: [] for var in variables}\n",
    "\n",
    "# Collect data\n",
    "for i in tqdm(range(n_samples)):\n",
    "    sample = era5_data[i]\n",
    "    for j, var in enumerate(variables):\n",
    "        var_data[var].append(sample['input'][j].numpy().flatten())\n",
    "\n",
    "# Compute statistics\n",
    "for var in variables:\n",
    "    # Concatenate all samples for this variable\n",
    "    all_data = np.concatenate(var_data[var])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats[var] = {\n",
    "        'mean': np.mean(all_data),\n",
    "        'std': np.std(all_data),\n",
    "        'min': np.min(all_data),\n",
    "        'max': np.max(all_data),\n",
    "        '5th_percentile': np.percentile(all_data, 5),\n",
    "        '95th_percentile': np.percentile(all_data, 95)\n",
    "    }\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nVariable Statistics:\")\n",
    "for var in variables:\n",
    "    print(f\"\\n{var} ({variable_details.get(var, '')})\")\n",
    "    for stat_name, stat_value in stats[var].items():\n",
    "        print(f\"  - {stat_name}: {stat_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    # Get data for histograms\n",
    "    all_data = np.concatenate(var_data[var])\n",
    "    \n",
    "    # Plot histogram\n",
    "    axes[i].hist(all_data, bins=50, alpha=0.7, density=True)\n",
    "    axes[i].set_title(f\"{var} Distribution\")\n",
    "    axes[i].set_xlabel(variable_details.get(var, var))\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    \n",
    "    # Add vertical lines for mean and std range\n",
    "    mean = stats[var]['mean']\n",
    "    std = stats[var]['std']\n",
    "    axes[i].axvline(mean, color='r', linestyle='--', label=f\"Mean: {mean:.2f}\")\n",
    "    axes[i].axvline(mean + std, color='g', linestyle=':', label=f\"±1 Std: {std:.2f}\")\n",
    "    axes[i].axvline(mean - std, color='g', linestyle=':')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Normalization and Preparation for Training\n",
    "\n",
    "Based on the statistics we calculated, let's create properly normalized data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized data loaders for training\n",
    "print(\"Creating data loaders with normalization...\")\n",
    "\n",
    "# Split data into training and validation\n",
    "train_years = ('2016', '2016-06')  # First half of 2016\n",
    "val_years = ('2016-07', '2016-12')  # Second half of 2016\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    variables=variables,\n",
    "    pressure_levels=pressure_levels,\n",
    "    train_slice=train_years,\n",
    "    val_slice=val_years,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    normalize=True  # Apply normalization\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normalized data\n",
    "# Get a batch from the training loader\n",
    "sample_batch = next(iter(train_loader))\n",
    "\n",
    "# Plot normalized fields for each variable\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    # Extract normalized field\n",
    "    normalized_field = sample_batch['input'][0, i, 0].numpy()\n",
    "    \n",
    "    # Plot\n",
    "    im = axes[i].imshow(normalized_field, cmap=visualizer.VAR_CMAPS.get(var, 'viridis'))\n",
    "    axes[i].set_title(f\"Normalized {var}\")\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Temporal Patterns and Lag Correlation\n",
    "\n",
    "Understanding the temporal correlation in weather data is crucial for flow matching. Let's examine how variables evolve over short time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific location (grid point) to examine\n",
    "lat_idx = len(lats) // 2  # Middle latitude (roughly equator)\n",
    "lon_idx = len(lons) // 2  # Middle longitude\n",
    "\n",
    "print(f\"Selected location: Latitude {lats[lat_idx]:.2f}°, Longitude {lons[lon_idx]:.2f}°\")\n",
    "\n",
    "# Extract time series for each variable at this location\n",
    "n_samples = min(100, len(era5_data))\n",
    "time_series = {var: [] for var in variables}\n",
    "timestamps = []\n",
    "\n",
    "for i in tqdm(range(n_samples)):\n",
    "    sample = era5_data[i]\n",
    "    timestamps.append(sample['metadata']['t0'])\n",
    "    \n",
    "    for j, var in enumerate(variables):\n",
    "        # Extract value at the selected location\n",
    "        value = sample['input'][j, 0, lat_idx, lon_idx].item()\n",
    "        time_series[var].append(value)\n",
    "\n",
    "# Convert timestamps to datetime objects for better plotting\n",
    "import pandas as pd\n",
    "datetimes = pd.to_datetime(timestamps)\n",
    "\n",
    "# Plot time series\n",
    "fig, axes = plt.subplots(len(variables), 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    axes[i].plot(datetimes, time_series[var], '-o', markersize=4)\n",
    "    axes[i].set_title(f\"{var} - {variable_details.get(var, '')}\")\n",
    "    axes[i].set_ylabel(var)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lag correlations to understand predictability\n",
    "max_lag = 10  # Maximum lag in time steps\n",
    "lag_corrs = {var: [] for var in variables}\n",
    "\n",
    "for var in variables:\n",
    "